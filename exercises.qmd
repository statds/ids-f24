# Exercises {#sec-ex}

1. **Quarto and Git setup**
  Quarto and Git are two important tools for data science. Get
  familiar with them through the following tasks. Please use
  the [`templates/hw.qmd`](https://github.com/statds/ids-f24/blob/main/templates/hw.qmd)
  template.
    1. Install Quarto onto your computer following the instructions
    of [Get Started](https://quarto.org/docs/get-started/). Document
    the obstacles you encountered and how you overcame them.
    1. Pick a tool of your choice (e.g., VS Code, Jupyter Notebook,
    Emacs, etc.), follow the instructions to reproduce the example
    of line plot on polar axis.
    1. Render the homework into a pdf file and put the file into
    a release in your GitHub repo. Document any obstacles you have
    and how you overcome them.
    

1. **Git basics and GitHub setup**
  Learn the Git basics and set up an account on GitHub if you do not
  already have one. Practice the tips on Git in the notes. By going
  through the following tasks, ensure your repo has at least 10
  commits, each with an informative message. Regularly check the
  status of your repo using `git status`. The specific tasks are:
    1. Clone the class notes repo to an appropriate folder on your computer.
    1. Add all the files to your designated homework repo from
    GitHub Classroom and work on that repo for the rest of the problem.
    1. Add your name and wishes to the Wishlist; commit.
    1. Remove the `Last, First` entry from the list; commit.
    1. Create a new file called `add.qmd` containing a few lines of
    texts; commit.
    1. Remove `add.qmd` (pretending that this is by accident); commit.
    1. Recover the accidentally removed file `add.qmd`; add a long line
  (a paragraph without a hard break); add a short line (under 80
  characters); commit.
    1. Change one word in the long line and one word in the short
  line; use `git diff` to see the difference from the last commit;
  commit.
    1. Play with other git operations and commit.

1. **Contributing to the Class Notes**
  To contribute to the classnotes, you need to have a working
  copy of the sources on your computer. Document the following
  steps in a `qmd` file as if you are explaining them to someone
  who want to contribute too.
    1. Create a fork of the notes repo into your own GitHub account.
    1. Clone it to an appropriate folder on your computer.
    1. Render the classnotes on your computer; document the obstacles
    and solutions.
    1. Make a new branch (and name it appropriately) to experiment
    with your changes.
    1. Checkout your branch and add your wishes to the wish list; 
    commit with an informative message; and
    push the changes to your GitHub account.
    1. Make a pull request to class notes repo from your fork at
    GitHub. Make sure you have clear messages to document the changes.

1. **Monty Hall**
   Write a function to demonstrate the Monty Hall problem through
   simulation. The function takes two arguments `ndoors` and
   `ntrials`, representing the number of doors in the experiment and
   the number of trails in a simulation, respectively. The function
   should return the proportion of wins for both the switch and
   no-switch strategy. Apply your function with 3 doors and 5 doors,
   both with 1000 trials. Include sufficient text around the code to
   explain your them.

1. **Approximating $\pi$**
   Write a function to do a Monte Carlo approximation of $\pi$. The
   function takes a Monte Carlo sample size `n` as input, and returns
   a point estimate of $\pi$ and a 95% confidence interval. Apply your
   function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 
   1000 times for each sample size and check the empirical probability that the
   confidence intervals cover the true value of $\pi$. Comment on
   the results.

1. **Google Billboard Ad**
   Find the first 10-digit prime number occurring in consecutive
   digits of $e$. This was a [Google recruiting
   ad](http://mathworld.wolfram.com/news/2004-10-13/google/).

1. **Game 24**
   The [math game 24](http://en.wikipedia.org/wiki/24_Game) is one of the
   addictive games among number lovers. With four randomly selected cards form
   a deck of poker cards, use all four values and elementary arithmetic
   operations ($+-\times /$) to come up with 24. Let $\square$ be one of
   the four numbers. Let $\bigcirc$ represent one of the four operators.
   For example, 
   \begin{equation*}
    (\square \bigcirc \square) \bigcirc (\square \bigcirc \square)
   \end{equation*}
   is one way to group the the operations.
   1. List all the possible ways to group the four numbers.
   1. How many possibly ways are there to check for a solution?
   1. Write a function to solve the problem in a brutal force way. The
   inputs of the function are four numbers. The function returns a list
   of solutions. Some of the solutions will be equivalent, but let us
   not worry about that for now.

1. **NYC Crash Data Cleaning**
   The NYC motor vehicle collisions data with documentation is available from
   [NYC Open
   Data](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95).
   The raw data needs some cleaning.
    1. Use the filter from the website to download the crash
    data of the week of June 30, 2024 in CSV format;
    save it under a directory `data` with an informative
    name (e.g., `nyccrashes_2024w0630_by20240916.csv`);
    read the data into a Panda data frame with careful
    handling of the date time variables.
    1. Clean up the variable names. Use lower cases and replace
    spaces with underscores.
    1. Get the basic summaries of each variables:
    missing percentage; descriptive statistics for continuous variables;
    frequency tables for discrete variables.
    1. Are their invalid `longitude` and `latitude` in the data?
    If so, replace them with `NA`.
    1. Are there `zip_code` values that are not legit NYC zip codes?
    If so, replace them with `NA`.
    1. Are there missing in `zip_code` and `borough`? Do they
    always co-occur?
    1. Are there cases where `zip_code` and `borough` are missing
    but the geo codes are not missing? If so, fill in `zip_code`
    and `borough` using the geo codes.
    1. Is it redundant to keep both `location` and the
    `longitude`/`latitude` at the NYC Open Data server?
    1. Check the frequency of `crash_time` by hour. Is there a matter
    of bad luck at exactly midnight? How would you interpret this?
    1. Are the number of persons killed/injured the summation of
    the numbers of pedestrians, cyclist, and motorists killed/injured?
    If so, is it redundant to keep these two columns at the NYC
    Open Data server?
    1. Print the whole frequency table of
    `contributing_factor_vehicle_1`. 
    Convert lower cases to uppercases and check the frequencies again.
    1. Provided an opportunity to meet the data provider, what
    suggestions would you make based on your data exploration
    experience?

1. **NYC Crash Data Exploration** Except for the first question, use
   the cleaned crash data in feather format.
    1. Construct a contigency table for missing in geocode (latitude and
     longitude) by borough. Is the missing pattern the same across boroughs?
     Formulate a hypothesis and test it. 
    1. Construct a `hour` variable with integer values from 0 to 23. Plot the
     histogram of the number of crashes by `hour`. Plot it by borough.
    1. Overlay the locations of the crashes on a map of NYC. The map could be a
     static map or Google map.
    1. Create a new variable `severe` which is one if the number of persons
     injured of deaths is 1 or more; and zero otherwise. Construct a cross
     table for `severe` versus borough. Is the severity of the crashes the
     same across boroughs? Test the null hypothesis that the two variables
     are not associated with an appropriate test.
    1. Merge the crash data with the zip code database.
    1. Fit a logistic model with `severe` as the outcome variable and covariates
     that are available in the data or can be engineered from the data. For
     example, zip code level covariates can be obtained by merging with the
     zip code database.

1. Using the cleaned NYC crash data, perform classification of `severe` with
   support vector machine and compare the results with the benchmark from
   regularized logistic regression. Use the last week's data as testing data.
    1. Explain the parameters you used in your fitting for each method.
    1. Explain the confusion matrix result from each fit.
    1. Compare the performance of the two approaches in terms of accuracy,
       precision, recall, F1-score, and AUC.


1. The [NYC Open Data of 311 Service Requests](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9) contains
   all requests from 2010 to present. We consider a subset of it with request
   time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is
   available in CSV format as `data/nyc311_011523-012123_by022023.csv`. Read the
   data dictionary to understand the meaning of the variables,
    1. Clean the data: fill missing fields as much as possible; check for
       obvious data entry errors (e.g., can `Closed Date` be earlier than
       `Created Date`?); summarize your suggestions to the data curator in
       several bullet points.
    1. Remove requests that are not made to NYPD and create a new variable
       `duration`, which represents the time period from the `Created Date` to
       `Closed Date`. Note that `duration` may be censored for some
       requests. Visualize the distribution of uncensored `duration` by
       weekdays/weekend and by borough, and test whether the distributions
       are the same across weekdays/weekends of their creation
	   and across boroughs.
    1. Define a binary variable `over3h` which is 1 if `duration` is greater
       than 3 hours. Note that it can be obtained even for censored `duration`.
	   Build a model to predict `over3h`.
	   If your model has tuning parameters, justify their choices. Apply
       this model to the 311 requests of NYPD in the week of 01/22/2023. Assess
       the performance of your model.
    1. Now you know the data quite well. Come up with a research question of
       interest that can be answered by the data, which could be analytics or
       visualizations. Perform the needed analyses and answer your question.

1. **NYC Rodents**
Rats in NYC are widespread, as they are in many densely populated
areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of 
October 2023, NYC dropped from the 2nd to the 3rd places in the 
annual ["rattiest city" list](https://www.orkin.com/press-room/top-rodent-infested-cities-2023#)
released by a pest control company. In 
the 311 Service Request Data, there is one complain type `Rodent`. 
Extract all the requests with complain type `Rodent`, created 
between January 1, 2022 and December 31, 2023. Save them into a 
csv file named `rodent_2022-2023.csv`.
    1. Are there any complains that are not closed yet?
    1. Are there any complains with a closed data before the
    created date?
    1. How many agencies were this complain type reported to?
    1. Summarize the missingess for each variable.
    1. Summarize a frequency table for the `descriptor` variable, 
    and summarize a cross table by year.
    1. Which types of 'DESCRIPTOR' do you think should be included
    if our interest is rodent sighting?
    1. Take a subset of the data with the descriptors you chose and
    summarize the response time by borough.


1. **NYC rodent sightings data cleaning**
The data appears to need some cleaning before any further analysis.
Some missing values could be filled based on other columns.
    1. Checking all 47 column names suggests that some columns 
    might be redundant. Identify them and demonstrate the redundancy.
    2. Are zip code and borough always missing together? If geocodes
    are available, use reverse geocoding to fill the zip code.
    3. Export the cleaned data in both csv and feather format. 
    Comment on the file sizes. 


1. **SQL Practice on NYC rodent sightings**
The NYC rodent sightings data that we prepared could be stored
more efficiently using a database. Let us start from the csv file
you exported from the last problem.
    1. Create a table called `rodent` from the csv file.
    1. The `agency` and `agency_name` columns are redundant in
    the table. Create a table called `agency`, which contains
    only these two columns, one agency a row.
    1. Drop the `agency_name` name from the `rodent` table.
    Justify why we do not need it here.
    1. Comment on the sizes of the table (or exported csv file)
    of `rodent` before and after dropping the `agency_name`
    column.
    1. Come up with a scheme for the two tables that allows
    even more efficient storage of the `agency` column
    in the `rodent` table. _Hint: use an integer to code the
    agencies.

1. **Logistic Modeling**
The response time to 311 service requests is a measure of civic
service quality. Let us model the response time to 311 requests
with complain type `Rodent`.
    1. Compute the response time in hours. Note that some response
    will be missing because of unavailable closed date.
    1. Compute a binary variable `over3d`, which is one if the
    response time is greater than 3 days, and zero otherwise. Note
    that this variable should have no missing values.
    1. Use the package `uszipcode` to obtain the zip code level
    covariates such as median house income and median home value.
    Merge these variables to the rodent data.
    1. Split the data at random into training (80%) and testing (20%).
    Build a logistic model to predict `over3d` on the training data,
    and validate the performance on the testing data.
    1. Build a lasso logistic model to predict `over3d`, and justify
    your choice of the tuning parameter. Validate on the testing data.

1. **Midterm Project: Rodents in NYC**
[Rodents in NYC](https://en.wikipedia.org/wiki/Rats_in_New_York_City)
are widespread, as they are in many densely populated areas. As of
October 2023, NYC dropped from the 2nd to the 3rd places in the annual
["rattiest city"
list](https://www.orkin.com/press-room/top-rodent-infested-cities-2023#)
released by a pest control company. Rat sightings in NYC was analyzed
by Dr. Michael Walsh in a [2014 PeerJ
article](https://peerj.com/articles/533/). We investigate this problem
from a different angle with the [NYC Rodent Inspection
data](https://data.cityofnewyork.us/Health/Rodent-Inspection/p937-wjvj/about_data),
provided by the Department of Health and Mental Hygiene (DOHMH).
Download the 2022-2023 data by filtering the `INSPECTION_DATE` to
between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024
and `INSPECTION_TYPE` is either `Initial` or `Compliance` (which
should be about 108 MB). Read the meta data information
to understand the data.
    1. Data cleaning.
        + There are two zipcode columns: `ZIP_CODE` and `Zipcodes`.
        Which one represent the zipcode of the inspection site? 
        Comment on the data dictionary.
        + Summarize the missing information. Are their missing values
        that can be filled using other columns? Fill them if yes.
        + Are their redundant information in the data? Try storing
        the data using `arrow` and comment on the efficiency gain.
        + Are there invalid zipcode or borough? Justify and clean
        them up if yes.
    1. Data exploration.
        + Create binary variable `passing` indicating passing or
        not for the inspection result. Does `passing` depend on
        whether the inspection is initial or compliance? State
        your hypothesis and summarize your test result.
        + Are the passing pattern different across different
        boroughs for initial inspections? How about compliance
        inspections? State your hypothesis and summarize your
        test results.
        + If we suspect that the passing rate may depends on
        the time of a day of the inspection, we may compare
        the passting rates for inspections done in the mornings
        and inspections one in the afternoons. Visualize the
        comparison by borough and inspection type.
        + Perform a formal hypothesis test to confirm the
        observations from your visualization.
    1. Data analytics.
        + Aggregate the inspections by zip code to create
        a dataset with five columns. The first three columns are
         `zipcode`; `n_initial`, the
        count of the initial inspections in that zipcode; and 
        `n_initpass`, the number of initial inspections with a
        passing result in that zipcode. The other two variables
        are `n_compliance` and `n_comppass`, the counterpart
        for compliance inspections.
        + Add a variable to your dataset, `n_sighting`, which
        represent the number of rodent sightings from the 311
        service request data in the same 2022-2023 period.
        + Merge your dataset with the simple zipcode table in
        package `uszipcode` by zipcode to obtain demographic
        and socioeconomic variables at the zipcode level.
        + Build a binomial regression for the passing rate of initial
        inspections at the zipcode level. Assess the goodness-of-fit
        of your model. Summarize your results to a New Yorker
        who is not data science savvy.
    1. Now you know the data quite well. Come up with a research
    question of interest that can be answered by the data, which
    could be analytics or visualizations. Perform the needed
    analyses and answer your question.
	
	




