[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Fall 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-f24.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2024, Spring 2023 and Spring 2022 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nTBA",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nAkach, Suha\n\nChallenge and push myself to be better at python and all its libraries.\nBe confident in my abilities of programming and making statistical inferences that are correct.\nBe able to create my own personal project in class on time.\n\nAstle, Jaden\n\nI’ve used Git before, but I’d like to become more comfortable using it and get more used to different issues that arise.\nI’d like to learn more effective ways to “tell the story” of data analysis and show empowering visualizations.\nI’d like to explore more methods that professional data scientists use in their model trainings to share with UConn’s Data Science Club.\n\nBabiec, Owen\n\nBecome more comfortable with Git and Github and their applications\nBetter understand the Data Science pipeline and workflow\nLearn how to show my skills I have learned in this class during interviews\n\nBaptista, Stef\n\nDevelop a project/presentation suitible enough for industry\nImprove on my data science skills regarding pandas and numpy\nUnderstanding the scope of packages in python as a language\n\nBienvenue, Jack\n\nLearn professional visualization techniques, particularly for geospatial data\nFoster a high level of working knowledge of Git\nCreate a small portfolio of examples and projects for later reference\n\nBlanchard, Zachary\n\nGain experience working and collaborating on projects in Git\nImprove computer programming skills and familiarity with Python\nTeach other students about creating presentations using Quarto\n\nBorowski, Emily\n\nGain a greater understanding of Quarto and GitHub\nBecome more comfortable with my coding abilities\nAcquire a deeper understanding of data science\n\nClokey, Sara\n\nBecome more familiar with GitHub and Quarto\nExecute a data science project from start to finish\n\nDesroches, Melanie\n\nExplore the field of data science as a possible future career\nDevelope data science and machine learning skills\nBecome better at programming with Python and using Git/GitHub\n\nFebles, Xavier\n\nGain a further understanding of GitHub\nDevelop data visualization skills\nLearn applications of skills learned in previous courses\n\nJha, Aansh\n\nBe a better student of the data science field\nHone git to work in colabarative workspaces\nLearn better methods in data visualization\n\nJohnson, Dorothea\n\nEnter data science contests\nFamiliarize myself with using Python for data Science\nDevelop a proficiency in Github\n\nKashalapov, Olivia\n\nBetter understand neural networks\nMachine learning utilizing Python\nCreating and analyzing predictive models for informed decision making\n\nManna, Rahul\n\nUse knowledge gained and skills developed in class to study real-world problems such as climate change.\nObtain a basic understanding of machine learning\n\nMazzola, Julia\n\nBecome proficient in Git and Github.\nHave a better understanding of data science best practices and techniques.\nDeepen my knowledge of Python programming concepts and libraries.\n\nParicharak, Aditya\n\nMaster Commandline Interface\nApply my statistical knowladge and skills to course work\nUnderstand how to work with datasets\n\nParvez, Mohammad Shahriyar\n\nFamiliarizing myself with GitHub to effectively track and manage the entire data analysis process.\nAdopting Quarto for improved documentation of my data workflows.\nExploring advanced techniques for data analysis and visualization.\nDeveloping my personal Git repository and publishing data projects as a professional website.\n\nTan, Qianruo\n\nLearn how to use GitHub, and create my own page\nGet a good grade on this class\nLearn more about how to processing data\n\nXu, Deyu",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\ntarget = \"Blanchard\"  # pre-arranged 1st presenter\npresenters = [name for name in presenters if target not in name]\n\nimport random\n## seed jointly set by the class\nrandom.seed(5347 + 2896 + 9050 + 1687 + 63)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Xu,Deyu',\n 'Clokey,Sara Karen',\n 'Johnson,Dorothea Trixie',\n 'Febles,Xavier Milan',\n 'Cai,Yizhan',\n 'Bienvenue,Jack Noel',\n 'Mazzola,Julia Cecelia',\n 'Akach,Suha',\n 'Manna,Rahul',\n 'Astle,Jaden Bryce',\n 'Kashalapov,Olivia',\n 'Borowski,Emily Helen',\n 'Tan,Qianruo',\n 'Desroches,Melanie',\n 'Paricharak,Aditya Sushant',\n 'Jha,Aansh',\n 'Babiec,Owen Thomas',\n 'Baptista,Stef Clare',\n 'Parvez,Mohammad Shahriyar']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nMaking presentations with Quarto\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/11\nZachary Blanchard\nMaking presentation with Quarto\n\n\n09/16\nDeyu Xu\nImport/Export data\n\n\n09/18\nSara Clokey\nCommunications in data science\n\n\n09/23\nDorathea Johnson\nDatabase with SQL\n\n\n09/25\nXavier Febles\n\n\n\n09/30\nJack Bienvenue\nVisualizing spatial data in a Google Map\n\n\n10/02\nJulia Mazzola\nData Visualization with Plotnine\n\n\n10/07\nSuha Akach\n\n\n\n10/09\nRahul Manna\n\n\n\n10/14\nKaitlyn Anderson\nCareer readiness in data science\n\n\n10/16\nTBA\n\n\n\n10/21\nJaden Astle\nClassification and regression trees\n\n\n10/23\nOlivia Kashalapov\n\n\n\n10/28\nEmily Borowski\nRandom Forest\n\n\n10/30\nQianruo Tan\nReinforcement Learning\n\n\n11/04\nMelanie Desroches\n\n\n\n11/06\nAditya Paricharak\n\n\n\n11/11\nAansh Jha\n\n\n\n11/11\nOwen Babiec\nCalling R from Python and vice versa\n\n\n11/13\nStef Baptista\n\n\n\n11/13\nMohammad Parvez\nDeveloping a Python package\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nSara Clokey; Dorothea Johnson; Xavier Febles; Jack Bienvenue\n\n\n11/20\nJulia Mazzola; Suha Akach; Rahul Manna; Jaden Astle\n\n\n12/02\nOlivia Kashalapov; Emily Borowski；Qianruo Tan; Melanie Desroches\n\n\n12/04\nAditya Paricharak; Aansh Jha; Owen Babiec; Stef Baptista\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#compiling-the-classnotes",
    "href": "quarto.html#compiling-the-classnotes",
    "title": "3  Reproducibile Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-f24. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-f24-venv\nHere .ids-f24-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-f24-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-f24-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (floder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-f24.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-f24\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([ 5.38636047,  0.22069893,  1.67560202, -0.4838066 ,  0.99107176,\n        3.09440432, -1.24662609,  3.20163589,  5.01351933,  1.21536211])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.06969754, 0.09034066, 0.09940812, 0.08224741, 0.09661286,\n       0.09607159, 0.07174579, 0.09533525, 0.07509333, 0.09783507])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n9.56 μs ± 478 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n1.99 μs ± 20.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n68.7 μs ± 736 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': np.int64(258), 'switch': np.int64(364)}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4468852416\n4468852416\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4433490168\n4499260544\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "5  Communication in Data Science",
    "section": "",
    "text": "5.1 Introduction\nHi! My name is Sara, and I am a junior double majoring in Applied Data Analysis and Communication. The topic of my presentation today, Communication in Data Science, combines my academic and professional interests while underscoring the importance of ‘soft skills’ like public speaking, for example, within STEM fields like data science.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#importance-application-of-communication",
    "href": "communication.html#importance-application-of-communication",
    "title": "5  Communication in Data Science",
    "section": "5.2 Importance & Application of Communication",
    "text": "5.2 Importance & Application of Communication\nData science as a career path has exploded within the last decade. Some fields that offer data science positions include:\n\nFinance\nHealthcare\nMedia production\nSports\nBanking\nInsurance\nE-Commerce\nEnergy\nManufacturing\nTransportation\nConstruction\n\nBecause data science is applicable in so many industries, it is essential that data scientists have the skills and experience to communicate their work with others who do not have the same technical education. As analyzed by Radovilsky et al. (2018), job listings within the field of data science often include qualifications like “strong interpersonal skills” and “demonstrated presentation and communication ability,” highlighting the pervasive need for this skill set.\nWithin these industries, collaboration and teamwork are often at the forefront. Inexperience with data should not prevent your colleagues from being able to contribute to shared projects, and strong communication skills can mitigate this challenge!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#general-communication-skills",
    "href": "communication.html#general-communication-skills",
    "title": "5  Communication in Data Science",
    "section": "5.3 General Communication Skills",
    "text": "5.3 General Communication Skills\n\n5.3.1 Verbal Communication Skills\nVerbal Communication: “The use of sounds and language to relay a message” (Yer (2018))\nVerbal Communication Tips:\n\nMake a good first impression\nUse appropriate language (jargon, metaphors)\nPrioritize brevity\nPractice beforehand\nAllow room for questions\n\n\n\n5.3.2 Non-Verbal Communication Skills\nNon-Verbal Communication: “Information, emotion, a movement that is expressed without words and without the help of language” (Grillo and Enesi (2022))\nNon-Verbal Communication Tips:\n\nUtilize vocal variety (pitch, rate, volume)\nAvoid distracting hand and body movements\nMake eye contact\nPay attention to proxemics\n\n\n\n5.3.3 Visual Communication Skills\nVisual Communication: “Any communication that employs one’s sense of sight to deliver a message without the usage of any verbal cues” (Fayaz (2022))\nVisual Communication Tips:\n\nPrioritize clarity\nUse proper labeling and scaling\nCreate visual contrast (colors, shapes, fonts)\nChoose the most appropriate visual representation\n\n\n\n5.3.4 Written Communication Skills\nWritten Communication: “Any form of communication which is written and documented from the sender to the receiver” (Prabavathi and Nagasubramani (2018))\nWritten Communication Tips:\n\nClearly state your goal with a thesis statement\nMaintain professionalism (contractions, slang)\nProofread and utilize peer editing\nFollow a specific structure\nBalance concision with analysis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#communication-in-data-science",
    "href": "communication.html#communication-in-data-science",
    "title": "5  Communication in Data Science",
    "section": "5.4 Communication in Data Science",
    "text": "5.4 Communication in Data Science\nOften, data scientists must communicate “technical conclusions to non-technical members” (Pragmatic (2024)); this may be colleagues in other departments, like marketing, or supervisors at the managerial level. Here are some tips for effectively communicating project results specifically in the field of data science.\n\n5.4.1 Identify your Audience\nWho are you sharing information with? Is it a room of data scientists like this one? Is it full of students who want to learn about data science? Is it a group of executives looking to make a funding decision?\n\nConsider the context and prior knowledge (technical jargon)\nConsider the motivation for listening\n\n\n\n5.4.2 Utilize Data Visualization\nOne of the most effective methods of communicating results in data science, especially to those without technical coding knowledge, is data visualization techniques (Vandemeulebroecke et al. (2019)). Python uses the package ‘matplotlib’ to produce these visualizations, including:\n\nline plots\nbar plots\nbox plots\nhistograms\nheat maps\npie charts\n\nThese visualizations allow complex statistical projects to be simplified into a single graphic, focusing on project results and implications rather than methodology. Ensure that data visualization techniques are free of technical jargon and clearly label all visual aspects.\n\n\n5.4.3 Focus on Data Communication Skills\nThe following skill sets highlight technical data communication that will be more common in projects with other data scientists to communicate about your data.\n\nCoding communication: Python, R, Julia, JavaScript, SQL, etc.\nAnalysis communication: creating a storyline, descriptive versus diagnostic versus predictive analytics, problem identification\nData management: collection, cleaning/transformation, storage\nData visualization\n\n\n\n5.4.4 Give Space for Questions and Feedback\nWithin professional spaces, data scientists should provide time for their clients, supervisors, and colleagues to ask questions about their work and subsequent findings.\n\nPause for questions throughout the presentation\nOffer contact information for continued collaboration\nProvide a structure for anonymous feedback\nSchedule follow-ups if necessary\n\nTeamwork is often at the heart of data science projects within industries, and open communication makes this teamwork run much more smoothly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#further-learning",
    "href": "communication.html#further-learning",
    "title": "5  Communication in Data Science",
    "section": "5.5 Further Learning",
    "text": "5.5 Further Learning\nWhile pursuing degrees in the data science field, consider taking Communication courses at UConn that can bolster your understanding and skill set. Some applicable communication courses include:\n\nCOMM 2100: Professional Communication\nCOMM 3110: Organizational Communication\nCOMM 3430: Science Communication\nCOMM 5655: Human-Computer Interaction\nCOMM 5900: Professional Communication\n\nEffective communication also requires practice. Here are some ways to practice these skills while earning your degree:\n\nFully participate in group projects\nSeek presentation opportunities (class, conferences, etc.)\nExplain data science coursework to peers outside of your program\nExplore internship opportunities that involve collaboration with other departments\n\n\n\n\n\nFayaz, Omer. 2022. “Principles of Visual Communication.”\n\n\nGrillo, H. M., and M. Enesi. 2022. “The Impact, Importance, Types, and Use of Non-Verbal Communication in Social Relations.” Linguistics and Culture Review 6: 291–307.\n\n\nPrabavathi, R., and P. C. Nagasubramani. 2018. “Effective Oral and Written Communication.” Journal of Applied and Advanced Research 10: 29–32.\n\n\nPragmatic. 2024. “Communication Skills for Data Science.” https://www.pragmaticinstitute.com\n  /resources/articles/data/communication-skills-for-data-science/.\n\n\nRadovilsky, Z., V. Hegde, A. Acharya, and U. Uma. 2018. “Skills Requirements of Business Data Analytics and Data Science Jobs: A Comparative Analysis.” Journal of Supply Chain and Operations Management 16: 82–101.\n\n\nVandemeulebroecke, Marc, Mark Baillie, Alison Margolskee, and Baldur Magnusson. 2019. “Effective Visual Communication for the Quantitative Scientist.” CPT Pharmacometrics Syst Pharmacol 10: 705–19.\n\n\nYer, Simona. 2018. “Verbal Communication as a Two-Way Process in Connecting People.” Social Science Research Network.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#importexport-data",
    "href": "manipulation.html#importexport-data",
    "title": "6  Data Manipulation",
    "section": "6.2 Import/Export Data",
    "text": "6.2 Import/Export Data\nThis section was written by Deyu Xu, a MS student in Statistics at the time.\n\n6.2.1 Subsection 1\n\n\n6.2.2 Subsection 2\n\n\n6.2.3 Extended Reading\nGive some references here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#sql",
    "href": "manipulation.html#sql",
    "title": "6  Data Manipulation",
    "section": "6.3 SQL",
    "text": "6.3 SQL\nThis section was written by Thea Johnson, a senior in Statistics at the time.\n\n6.3.1 Subsection 1\n\n\n6.3.2 Subsection 2\n\n\n6.3.3 Extended Reading\nGive some references here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#nyc-crash-data",
    "href": "manipulation.html#nyc-crash-data",
    "title": "6  Data Manipulation",
    "section": "6.4 NYC Crash Data",
    "text": "6.4 NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on September 16, 2024, in CSC format.\n\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20240916.csv'\ndf = pd.read_csv(file_path)\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Display the first few rows of the dataset to understand its structure\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n17:30\nNaN\nNaN\n0.00000\n0.00000\n(0.0, 0.0)\nNaN\nNaN\nGOLD STREET\n...\nUnspecified\nNaN\nNaN\nNaN\n4736746\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n0:32\nNaN\nNaN\nNaN\nNaN\nNaN\nBELT PARKWAY RAMP\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n7:05\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNaN\nNaN\n2797 OCEAN PARKWAY\n...\nNaN\nNaN\nNaN\nNaN\n4737060\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n20:47\nNaN\nNaN\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n10:14\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736759\nBus\nBox Truck\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Longitude/latitude don't need double precision\ndf['latitude'] = df['latitude'].astype('float32', errors='ignore')\ndf['longitude'] = df['longitude'].astype('float32', errors='ignore')\n\n# Drop the redundant 'location' column\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nmissing_cooccur, total_missing_zip_code, total_missing_borough\n\n(np.int64(541), np.int64(541), np.int64(541))\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nzip_code = get_zip_code(latitude, longitude)\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "6  Data Manipulation",
    "section": "6.5 Cross-platform Data Format Arrow",
    "text": "6.5 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\n\ndf.to_feather('data/nyccrashes_cleaned.feather')\n\n# Compare the file sizes of the feather format and the CSV format\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20240916.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#using-the-census-data",
    "href": "manipulation.html#using-the-census-data",
    "title": "6  Data Manipulation",
    "section": "6.6 Using the Census Data",
    "text": "6.6 Using the Census Data\nThe US Census provides a lot of useful data that could be merged with the NYC crash data for further analytics.\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the zip_code column is clean and consistent.and consistent.\n\nimport pandas as pd\ndf = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\nvalid_zip_df = df.dropna(subset=['zip_code']).copy()\nvalid_zip_df['zip_code'] = valid_zip_df['zip_code'].astype(int).astype(str).str.zfill(5)\nunique_zips = valid_zip_df['zip_code'].unique()\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nDemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment, if it has not been installd yet.\npip install uszipcode\n\n\n``\nWe will first clean the `zip_code` column to ensure it only\ncontains valid ZIP codes. Then, we will use a vectorized\napproach to fetch the required data for each unique zip code\nand merge this information back into the original `DataFrame`.\n`{python}\n# Remove rows where 'zip_code' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['zip_code']).copy()\nvalid_zip_df['zip_code'] = valid_zip_df['zip_code'].astype(str).str.zfill(5)\nSince uszipcode doesn’t valid_zip_dfrently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple\ntimes.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'zip_code'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"zip_code\")\n\nmerged_df.head()\n\n\n\n\n\n\n\n\ncrash_datetime\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\n...\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\nmedian_home_value\nmedian_household_income\n\n\n\n\n0\n2024-06-30 07:05:00\nBROOKLYN\n11235\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n...\nNone\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n531000.0\n41639.0\n\n\n1\n2024-06-30 20:47:00\nMANHATTAN\n10021\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n...\nNone\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n1000001.0\n107907.0\n\n\n2\n2024-06-30 10:14:00\nBROOKLYN\n11222\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n...\nNone\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n726500.0\n63739.0\n\n\n3\n2024-06-30 15:52:00\nBRONX\n10468\n40.86685\n-73.89597\n(40.86685, -73.89597)\nNone\nNone\n60 EAST KINGSBRIDGE ROAD\n1\n...\nNone\nNone\n4736781\nStation Wagon/Sport Utility Vehicle\nMoped\nNone\nNone\nNone\n171200.0\n33776.0\n\n\n4\n2024-06-30 16:30:00\nBROOKLYN\n11226\n40.63969\n-73.95321\n(40.63969, -73.95321)\nNEWKIRK AVENUE\nEAST 25 STREET\nNone\n1\n...\nNone\nNone\n4737299\nSedan\nMoped\nNone\nNone\nNone\n480500.0\n40734.0\n\n\n\n\n5 rows × 30 columns",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1 Data Visualization with Plotnine\nThis section was written by Julia Mazzola",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#data-visualization-with-plotnine",
    "href": "visualization.html#data-visualization-with-plotnine",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1.1 Introduction\nHi! My name is Julia, and I am a Senior double majoring in Economics and Statistical Data Science. I’m excited to show you the power of data visualization with Plotnine, a Python library inspired by R’s ggplot2. Visualization is a crucial tool to effectively communicate your findings to your audience and Plotnine is a useful library to use.\n\n\n7.1.2 What is Plotnine?\nPlotnine uses grammer of graphics to create layered, customizable visualizations. Grammar of graphics is a framework that provides a systematic approach to creating visual representations of data by breaking down the plot into its fundamental components. To understand this better, think about how sentences have grammer, we can layer our graphics to create complex and detailed visulizations.\nComponents of the layered grammar of graphics:\n\nLayer: used to create the objects on a plot\n\nData: defines the source of the information to be visualized\nMapping: defines how the variables are represented in the plot\nStatistical transformation (stat): transforms the data, generally by summarizing the information\nGeometric object (geom): determines the type of plot type (e.g., points, lines, bars)\nPosition adjustment (position): adjusts the display of overlapping points to improve clarity\n\nScale: controls how values are mapped to aesthetic attributes (e.g., color, size)\nCoordinate system (coord): maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn\nFaceting (facet): used to split the data up into subsets of the entire dataset\n\nYou can make a wide array of different graphics with Plotnine. Some common examples are:\n\nScatterplot, geom_point()\nBar Chart geom_bar()\nHistogram geom_histogram()\nLine Chart geom_line()\n\n\n\n7.1.3 Installing Plotnine\nTo use Plotnine you must install it into your venv first. The instructions are as follows:\nType this command into either conda, your terminal, gitbash, or whatever you use for package install for your venv.\nFor pip:\npip install plotnine\nFor conda:\nconda install -c conda-forge plotnine\nYou can import Plotnine without a prefix:\n\nfrom plotnine import *\n\nOr with with a prefix to access each component such as:\n\nimport plotnine as p9\n\nThis way is generally recommended for larger projects or when collaborating with others for better code maintainability. But for simplicity in this section I will use the first method.\nFor the examples we will be using NYC open data to visualize motor vehicle crashes from the week of June 30, 2024.\n\nimport pandas as pd\n\nnyc_crash = pd.read_feather('nyccrashes_cleaned.feather').dropna(subset=['borough'])\n\n\n\n7.1.4 Scatterplot\nFirstly, we will be creating a scatterplot. This can be done with geom_point(). Our scatterplot will be displaying Crash Locations based on the longitude and latitude of the crash sites.\nCreating a Basic Scatterplot\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n(ggplot(nyc_crash, aes(x='longitude', y='latitude')) +\n    #specifies graph type\n    geom_point() +\n     #creates labels for graphic\n    labs(title='Crash Locations',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\nCustomizing a Scatterplot\nYou can customize your plot further by changing the color, edge color, transparency, size, or shape of your points. This is done in geom_point().\n\n(ggplot(nyc_crash, aes(x='longitude', y='latitude')) + \n    geom_point(color = 'black', fill = 'purple', \n                alpha = 0.5, size = 2, shape = 's') + \n    labs(title='Crash Locations by Contributing Factor',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\nThis scatterplot provides a lot of information, yet there are ways we can customize our plot to be more informative for our audience. We can create a scatterplot that differentiates by contributing factor.\nChanging Shape of Variables\nAdding shapes by the contributing factor for vehicle 1:\n\n#List of top 5 reasons for the contributing facor since it is difficult to display each reason\nfactor1 = [\"Driver Inattention/Distraction\", \n           \"Failure to Yield Right-of-Way\", \n           \"Following Too Closely\", \n           \"Unsafe Speed\", \n           \"Passing or Lane Usage Improper\"]\n\n# Filter the data to only include valid contributing factors\nconfact = nyc_crash[nyc_crash['contributing_factor_vehicle_1'].isin(factor1)]\n\n\n#changes shape of point according to 'contributing_factor_vehicle_1'\n(ggplot(confact, aes(x='longitude', y='latitude',\n    shape ='contributing_factor_vehicle_1')) + \n    geom_point() + \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\nChanging Color of Variables\nTo add color coordination to your plot in Plotnine, you can specify the variable you want to use for coloring by including color='variable' within the aes() function. This allows you to visually distinguish different categories in your dataset.\n\n#color= changhes color according to 'borough'\n(ggplot(nyc_crash, aes(x='longitude', y='latitude', color = 'borough')) +\n    geom_point() +\n    labs(title='Crash Locations',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\nAs you can see each borough has its own color and the audience can easily see which borough the crash occured in.\n\n#color= changes color according to 'contributing_factor_vehicle_1'\n(ggplot(confact, aes(x='longitude', y='latitude',\n    color ='contributing_factor_vehicle_1')) + \n    geom_point() + \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\nThis graph uses color to distinguish what contributing factor caused the crash.\nScatterplots per Facet\nTo organize your data in a way that enhances interpretability, you can utilize facet_grid() or facet_wrap(). This approach allows for the creation of separate plots based on categorical variables, making it easier to identify trends and patterns.\nScatterplot of Crash Locations by Contributing Factor with facet_wrap():\n\n(ggplot(confact, aes(x='longitude', y='latitude')) +  \n    geom_point() +\n    #create separate plots for each contributing factor\n    facet_wrap('contributing_factor_vehicle_1') +  \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n         x='Longitude',\n         y='Latitude') +\n    #changes the size of the graphic\n    theme(figure_size=(6, 8)))\n\n\n\n\n\n\n\n\nScatterplot of Crash Locations by Contributing Factor with facet_grid():\n\n(ggplot(confact, aes(x='longitude', y='latitude')) +  \n    geom_point() +\n    #create separate plots for each contributing factor\n    facet_grid('contributing_factor_vehicle_1') +  \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n         x='Longitude',\n         y='Latitude') +\n    #changes the size of the graphic\n    theme(figure_size=(6, 10)))\n\n\n\n\n\n\n\n\nScatterplot of Crash Locations by Contributing Factor and Borough:\n\n(ggplot(confact, aes(x='longitude', y='latitude')) +  \n    geom_point() +\n    #create a grid of subplots based on the values of two categorical variables\n    facet_grid('contributing_factor_vehicle_1 ~ borough') +  \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n         x='Longitude',\n         y='Latitude') +\n    #changes angle of text and size of the graphic\n    theme(\n        axis_text_x=element_text(angle=90), \n        figure_size=(7, 12)))\n\n\n\n\n\n\n\n\nAdding linear regression line to scatterplot\nFitting a regression line to your plot can be really helpful to visualize trends to your data easier. To add a linear regression line to your scatterplot, you would include the following line of code:\n\ngeom_smooth(method='lm', se=False, color=' ')\n\n&lt;plotnine.geoms.geom_smooth.geom_smooth at 0x118bd8320&gt;\n\n\nAlthough this data is not one that makes sense to fit a regression line, the scatterplot with it is as follows:\n\n(ggplot(nyc_crash, aes(x='longitude', y='latitude')) +\n    geom_point() +\n    #adds a linear regression line\n    geom_smooth(method='lm', se=False, color='red') + \n    labs(title='Crash Locations',\n         x='Longitude',\n         y='Latitude'))\n\n\n\n\n\n\n\n\n\n\n7.1.5 Bar Chart\nAnother common use for displaying data is a bar chart. You can create one with geom_bar(). We will start with a simple chart of crashes by borough.\nCreating a Basic Bar Chart\n\n(ggplot(nyc_crash, aes(x='borough'))  # Use 'borough' for the x-axis\n + geom_bar(fill='purple') \n + labs(title='Number of Crashes by Borough', \n        x='Borough',\n        y='Count'))\n\n\n\n\n\n\n\n\nCustomizing your Bar Chart\nYou can change up your bar chart a couple of different ways. You can handpick colors you want, designate it to variables, and flip orientation, etc:\n\n#designate your preffered colors (pastel color codes)\ncolors = ['#B3FFBA', '#E1C6FF', '#FFB3BA', '#BAE1FF', '#FFD5BA']\n\n#adding fill= changes the color of bar according to variable\n(ggplot(nyc_crash, aes(x='borough', fill = 'borough')) \n#assigns your preffered colors\n + geom_bar(fill = colors) \n #flips orientation of the chart\n + coord_flip()\n + labs(title='Number of Crashes by Borough', \n        x='Borough',\n        y='Count'))\n\n\n\n\n\n\n\n\nMultivariable Bar Chart\nYou can also split up a bar chart to make it visually easier to understand.\n\n#using 'confact' dataset again for better visualization\n(ggplot(confact, aes(x='contributing_factor_vehicle_1', fill='borough')) \n    + geom_bar()\n    + ggtitle('Top 5 Contributing Factors by Borough')  \n    + xlab(\"Top 5 Contributing Factor Vehicle 1\")\n    + ylab(\"Number of Crashes\")\n    #creates smaller text\n    #rotates x-axis text for readability\n    #creates a bigger image\n    + theme(axis_text_x=element_text(size=9, angle=65), figure_size= (7,7)))\n\n\n\n\n\n\n\n\n\n\n7.1.6 Histogram\nAnother useful way to display data is a histogram. You can create one with geom_hisogram(). Using a histogram is very useful when displaying continuous data.\nBasic Histogram\n\n(ggplot(nyc_crash, aes(x='number_of_persons_injured')) +\n    #bins= sets the amount of bars in your histogram\n    geom_histogram(bins=10, alpha=0.8, fill='green') + \n    labs(title='Distribution of Persons Injured',\n         x='Number of Persons Injured',\n         y='Count of Crashes'))\n\n\n\n\n\n\n\n\nWith a histogram it is very easy to understand trends for a dataset and you can see that our NYC crash data is positively skewed.\nMultivariable Histogram\nSimilar to bar charts, we can make Histograms that display more than one variable. We can also add some more customizations such as adding an outline color for our sections, adding width of the bins and angle of .\n\n(ggplot(confact, aes(x='number_of_persons_injured', fill = 'borough')) \n    #changing width of the bins and adding outline color with color= to make it easier to read\n    + geom_histogram(binwidth=1, color = 'black'))\n\n\n\n\n\n\n\n\nOverlapping Histogram\nHistograms can also be useful when comparing multiple categories. Here we are comparing Manhattan and Brooklyn’s number of persons injured with an overlapping histogram.\n\n(ggplot(nyc_crash[nyc_crash['borough'].isin(['MANHATTAN', 'BROOKLYN'])], aes(x='number_of_persons_injured', fill='borough')) +\n    geom_histogram(bins=10) +\n    labs(title='Persons Injured: Manhattan vs Brooklyn',\n         x='Number of Persons Injured',\n         y='Count'))\n\n\n\n\n\n\n\n\n\n\n7.1.7 Line Chart\nLine charts are great for time-series data and can be created with geom_line(). This type of chart is particularly useful for identifying patterns, fluctuations, and trends, making it easier to understand how a variable changes over a specified period. We will create one analyzing Number of Crashes by Hour.\nBasic Line Chart\n\nnyc_crash['crash_datetime'] = pd.to_datetime(nyc_crash['crash_datetime'])\n\n# Extract hour\nnyc_crash['crash_hour'] = nyc_crash['crash_datetime'].dt.hour\n\n# Count crashes per hour\ncrash_counts = nyc_crash.groupby(['crash_hour']).size().reset_index(name='crash_count')\n\n\n#plot crashes by hour\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count')) +\n    #creates the line chart\n    geom_line() +\n    #adds points for better visibility\n    geom_point() +\n    labs(title='Number of Crashes by Hour',\n         x='Hour',\n         y='Crashes') +\n    #formats the x-axis to display ticks by every 2 hours\n    scale_x_continuous(breaks=range(0, 24, 2)))\n\n\n\n\n\n\n\n\nThis example is excellent for understanding the grammar of graphics. As you can see, we use geom_line() to create the line chart, while also adding geom_point(), which is typically used for scatterplots, to make the figure clearer by layering additional details.”\nMultivariable Line Chart\nSimilarly to the other figures you can create a line chart with multiple variables. Now we will create a chart with number of crashes by borough.\n\n#setting crash counts to also include borough\ncrash_counts = nyc_crash.groupby(['crash_hour', 'borough']).size().reset_index(name='crash_count')\n\n#plots crashes by hour with different lines for each borough\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count', color='borough')) +\n    #size= changes the thinkness of the lines\n    geom_line(size=1) +\n    labs(title='Number of Crashes by Hour and Borough',\n         x='Hour of the Day',\n         y='Number of Crashes') +\n    scale_x_continuous(breaks=range(0, 24, 2)))\n\n\n\n\n\n\n\n\nLine Chart per Facet\nYou can use plot each variable by on separate panels with facet_wrap().\n\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count')) +\n    geom_line(size=1) +\n    #breaks the figure up by borough\n    facet_wrap(\"borough\") + \n    labs(title='Number of Crashes by Hour and Borough',\n         x='Hour of the Day',\n         y='Number of Crashes'))\n\n\n\n\n\n\n\n\n\n\n7.1.8 Conclusion\nPlotnine is a very powerful tool to make impactful and detailed graphics. The flexibility of its grammar of graphics approach means there are endless ways to modify, enhance, and be creative with your plots. You can layer geoms, adjust aesthetics, and apply scales, facets, and themes.\nBasic things to remember:\nCreating Specific Plots\n\nScatterplot, geom_point()\nBoxplot geom_box()\nHistogram geom_histogram()\nLine Chart geom_line()\nBar Chart geom_bar()\nDensity Plot geom_denisty()\n\nFormatting and Customizing Your Figure\n\nfill: to change the color of the data\ncolor: to change the color of the borders\nalpha: to change the transparency\nbins: to change the number of bins\nfigure_size: to change size of graphic\ngeom_smooth: to add a smoothed line\nfacet: plot each group on a separate panel\n\nfacet_wrap(): creates a series of plots arranged in a grid, wrapping into new rows or columns as needed\nfacet_grid(): allows you to create a grid layout based on two categorical variables, organizing plots in a matrix format\n\ntheme: change overall theme\n\nThere are many other features and customizations you can do with Plotnine. For more information on how to leverage the full potential of this package for your data visualization needs check out Plotnine’s Graph Gallery.\nHappy plotting!\n\n\n7.1.9 Sources\nSarker, D. (2018). A comprehensive guide to the grammar of graphics for effective visualization of multi-dimensional data. Towards Data Science. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149\nPython Graph Gallery. (2024). Plotnine: ggplot in python. Python Graph Gallery. https://python-graph-gallery.com/plotnine/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "8  Statistical Tests and Models",
    "section": "",
    "text": "8.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.test() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.02949\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "8  Statistical Tests and Models",
    "section": "",
    "text": "Comparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "8  Statistical Tests and Models",
    "section": "8.2 Statistical Modeling",
    "text": "8.2 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n8.2.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n8.2.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nTue, 08 Oct 2024\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n11:52:01\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nTue, 08 Oct 2024\n\n\n\n\nTime:\n11:52:01\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n8.2.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function (\\(g(\\mu) = \\mu\\)) is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity (\\(g(\\mu) = \\mu\\))\nDistribution: Normal\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit (\\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\))\nDistribution: Binomial\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log (\\(g(\\mu) = \\log(\\mu)\\))\nDistribution: Poisson\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse (\\(g(\\mu) = \\frac{1}{\\mu}\\))\nDistribution: Gamma\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution.\nA logistic regression can be fit with statsmodels.api.glm.\nTo demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a DataFrame with random features named `simdat`\nsimdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\n\n# Calculating the linear combination of inputs plus an intercept\neta = simdat.dot([2, 2, 2, 2, 2]) - 5\n\n# Applying the logistic function to get probabilities using statsmodels' logit link\np = sm.families.links.Logit().inverse(eta)\n\n# Generating binary outcomes based on these probabilities and adding them to `simdat`\nsimdat['yb'] = np.random.binomial(1, p, p.size)\n\n# Display the first few rows of the dataframe\nprint(simdat.head())\n\n         x1        x2        x3        x4        x5  yb\n0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0\n1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0\n2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0\n3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0\n4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0\n\n\nFit a logistic regression for y1b with the formula interface.\n\nimport statsmodels.formula.api as smf\n\n# Specify the model formula\nformula = 'yb ~ x1 + x2 + x3 + x4 + x5'\n\n# Fit the logistic regression model using glm and a formula\nfit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     yb   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      994\nModel Family:                Binomial   Df Model:                            5\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -159.17\nDate:                Tue, 08 Oct 2024   Deviance:                       318.34\nTime:                        11:52:02   Pearson chi2:                 1.47e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.4197\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.0186      0.392    -12.796      0.000      -5.787      -4.250\nx1             1.9990      0.211      9.471      0.000       1.585       2.413\nx2             2.1058      0.214      9.853      0.000       1.687       2.525\nx3             1.9421      0.210      9.260      0.000       1.531       2.353\nx4             2.1504      0.232      9.260      0.000       1.695       2.606\nx5             2.0603      0.221      9.313      0.000       1.627       2.494\n==============================================================================",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#validating-the-results-of-logistic-regression",
    "href": "stats.html#validating-the-results-of-logistic-regression",
    "title": "8  Statistical Tests and Models",
    "section": "8.3 Validating the Results of Logistic Regression",
    "text": "8.3 Validating the Results of Logistic Regression\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\n\n8.3.1 Confusion Matrix\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n8.3.2 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n8.3.3 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n8.3.4 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n8.3.5 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n8.3.6 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n8.3.7 Demonstration\nLet’s apply these metrics to the simdat dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[104  11]\n [ 26 109]]\nAccuracy: 0.85\nPrecision: 0.91\nRecall: 0.81\n\n\nBy varying threshold, one can plot the whole ROC curve.\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Print AUC\nprint(f\"AUC: {roc_auc:.2f}\")\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nAUC: 0.92\n\n\n\n\n\n\n\n\n\nWe could pick the best threshold that optmizes F1-score/\n\n# Compute F1 score for each threshold\nf1_scores = []\nfor thresh in thresholds:\n    y_pred_thresh = (y_scores &gt;= thresh).astype(int)  # Apply threshold to get binary predictions\n    f1 = f1_score(y_test, y_pred_thresh)\n    f1_scores.append(f1)\n\n# Find the best threshold (the one that maximizes F1 score)\nbest_thresh = thresholds[np.argmax(f1_scores)]\nbest_f1 = max(f1_scores)\n\n# Print the best threshold and corresponding F1 score\nprint(f\"Best threshold: {best_thresh:.4f}\")\nprint(f\"Best F1 score: {best_f1:.2f}\")\n\nBest threshold: 0.3960\nBest F1 score: 0.89",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#lasso-logistic-models",
    "href": "stats.html#lasso-logistic-models",
    "title": "8  Statistical Tests and Models",
    "section": "8.4 LASSO Logistic Models",
    "text": "8.4 LASSO Logistic Models\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani 1996), is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.\n\n8.4.1 Theoretical Formulation of the Problem\nThe objective function for LASSO logistic regression can be expressed as,\n\\[\n\\min_{\\beta}\n\\left\\{ -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nwhere:\n\n\\(\\hat{p}_i = \\frac{1}{1 + e^{-X_i\\beta}}\\) is the predicted probability for the \\(i\\)-th sample.\n\\(y_i\\) represents the actual class label (binary: 0 or 1).\n\\(X_i\\) is the feature vector for the \\(i\\)-th observation.\n\\(\\beta\\) is the vector of model coefficients (including the intercept).\n\\(\\lambda\\) is the regularization parameter that controls the trade-off between model fit and sparsity (higher \\(\\lambda\\)) encourages sparsity by shrinking more coefficients to zero).\n\nThe lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.\nPractical benefits of LASSO:\n\nDimensionality Reduction: LASSO is particularly useful when the number of features \\(p\\) is large, potentially even larger than the number of observations \\(n\\), as it automatically reduces the number of features.\nPreventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when \\(p\\) is large or there is multicollinearity among features.\nInterpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.\n\n\n\n8.4.2 Solution Path\nTo illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter \\(\\lambda\\). This demonstration will use a simulated dataset to show how increasing \\(\\lambda\\) leads to more coefficients being set to zero.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate a classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               random_state=42)\n\n# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max\ndef get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):\n    ybar = np.mean(y)\n    xbar = np.mean(xs, axis=0)\n    xs_centered = xs - xbar\n    xty = np.dot(xs_centered.T, (y - ybar))\n    lmax = np.max(np.abs(xty))\n    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),\n                              num=nlambda)\n    return lambdas\n\n# Step 3: Calculate lambda values\nnlambda = 100\nmin_ratio = 0.01\nlambda_values = get_lambda_l1(X, y, nlambda, min_ratio)\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 5: Initialize arrays to store the coefficients for each lambda value\ncoefficients = []\n\n# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value\nfor lam in lambda_values:\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)\n    model.fit(X_scaled, y)\n    coefficients.append(model.coef_.flatten())\n\n# Convert coefficients list to a NumPy array for plotting\ncoefficients = np.array(coefficients)\n\n# Step 7: Plot the solution path for each feature\nplt.figure(figsize=(10, 6))\nfor i in range(coefficients.shape[1]):\n    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')\n    \nplt.xscale('log')\nplt.xlabel('Lambda values (log scale)')\nplt.ylabel('Coefficient value')\nplt.title('Solution Path of Logistic Lasso Regression')\nplt.grid(True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.4.3 Selection the Tuning Parameter\nIn logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter \\(C\\) (the inverse of \\(\\lambda\\)) is crucial to balancing the model’s bias and variance. A small \\(C\\) value (large \\(\\lambda\\)) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large \\(C\\) (small \\(\\lambda\\)) allows the model to fit the data more closely.\nThe best way to select the optimal \\(C\\) is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The \\(C\\) value that results in the best performance is selected.\nThe performance metric used in cross-validation can vary based on the task. Common metrics include:\n\nLog-loss: Measures how well the predicted probabilities match the actual outcomes.\nAccuracy: Measures the proportion of correctly classified instances.\nF1-Score: Balances precision and recall, especially useful for imbalanced classes.\nAUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.\n\nIn Python, the LogisticRegressionCV class from scikit-learn automates cross-validation for logistic regression. It evaluates the model’s performance for a range of \\(C\\) values and selects the best one.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation\nlog_reg_cv = LogisticRegressionCV(\n    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)\n    cv=5,                       # 5-fold cross-validation\n    penalty='l1',               # Lasso regularization (L1 penalty)\n    solver='liblinear',         # Solver for L1 regularization\n    scoring='accuracy',         # Optimize for accuracy\n    max_iter=10000              # Ensure convergence\n)\n\n# Train the model with cross-validation\nlog_reg_cv.fit(X_train, y_train)\n\n# Best C value (inverse of lambda)\nprint(f\"Best C value: {log_reg_cv.C_[0]}\")\n\n# Evaluate the model on the test set\ny_pred = log_reg_cv.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display the coefficients of the best model\nprint(\"Model Coefficients:\\n\", log_reg_cv.coef_)\n\nBest C value: 0.08858667904100823\nTest Accuracy: 0.86\nModel Coefficients:\n [[ 0.          0.          0.05552448  0.          0.          1.90889734\n   0.          0.          0.          0.          0.0096863   0.23541942\n   0.          0.         -0.0268928   0.          0.          0.\n   0.          0.        ]]\n\n\n\n\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the LASSO.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "9  Supervised Learning",
    "section": "",
    "text": "9.1 Introduction\nSupervised learning uses labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.\nIn contrast, unsupervised learning uses unlabeled data to discover patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#classification-vs-regression",
    "href": "supervised.html#classification-vs-regression",
    "title": "9  Supervised Learning",
    "section": "9.2 Classification vs Regression",
    "text": "9.2 Classification vs Regression\n\nClassificaiton: outcome variable is categorical\nRegression: outcome variable is continuous\nBoth problems can have many covariates (predictors/features)\n\n\n9.2.1 Regression metrics\n\nMean squared error (MSE)\nMean absolute error (MAE)\n\n\n\n9.2.2 Classification metrics\n\n9.2.2.1 Confusion matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTP: number of true positives\nFN: number of false negatives\nFP: number of false positives\nTN: number of true negatives\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTPR: TP / (TP + FN). Also known as sensitivity.\nFNR: TN / (TP + FN). Also known as miss rate.\nFPR: FP / (FP + TN). Also known as false alarm, fall-out.\nTNR: TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour rates from the confusion matrix with predicted (column) margins:\n\nPPV: TP / (TP + FP). Also known as precision.\nFDR: FP / (TP + FP).\nFOR: FN / (FN + TN).\nNPV: TN / (FN + TN).\n\n\n\n9.2.2.2 Measure of classification performance\nMeasures for a given confusion matrix:\n\nAccuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.\nRecall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.\nPrecision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.\nF-beta score: Harmonic mean of precision and recall with \\(\\beta\\) chosen such that recall is considered \\(\\beta\\) times as important as precision, \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}\n\\] See stackexchange post for the motivation of \\(\\beta^2\\).\n\nWhen classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\n\n\n\n9.2.3 Cross-validation\n\nGoal: strike a bias-variance tradeoff.\nK-fold: hold out each fold as testing data.\nScores: minimized to train a model\n\nCross-validation is an important measure to prevent over-fitting. Good in-sample performance does not necessarily mean good out-sample performance. A general work flow in model selection with cross-validation is as follows.\n\nSplit the data into training and testing\nFor each candidate model \\(m\\) (with possibly multiple tuning parameters)\n\nFit the model to the training data\nObtain the performance measure \\(f(m)\\) on the testing data (e.g., CV score, MSE, loss, etc.)\n\nChoose the model \\(m^* = \\arg\\max_m f(m)\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "10  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template.\n\nInstall Quarto onto your computer following the instructions of Get Started. Document the obstacles you encountered and how you overcame them.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo. Document any obstacles you have and how you overcome them.\n\nGit basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidentally removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file as if you are explaining them to someone who want to contribute too.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the zip code database.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Noise complaints in NYC The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with requests to NYPD on noise complaints that are created between 00:00:00 06/30/2024 and 24:00:00 07/06/2024. The subset is available in CSV format as data/nypd311w063024noise_by100724.csv. Read the data dictionary online to understand the meaning of the variables.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre their redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Justify and clean them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second; action_update_date after closed_date.\nSummarize your suggestions to the data curator in several bullet points.\n\nData exploration.\n\nIf we suspect that response time may depend on the time of day when a complaint is made, we can compare the response times for complaints submitted during nighttime and daytime. To do this, we can visualize the comparison by complaint type, borough, and weekday (vs weekend/holiday).\nPerform a formal hypothesis test to confirm the observations from your visualization. Formaly state your hypotheses and summary your conclusions in plain English.\nCreate a binary variable over2h to indicate that a service request took two hours or longer to close.\nDoes over2h depend on the complain type, borough, or weekday (vs weekend/holiday)? State your hypotheses and summarize your conclusions in plain English.\n\nData analysis.\n\nThe addresses of NYC police percincts are stored in data/nypd_precincts.csv. Use geocoding tools to find their geocode (longitude and lattitude) from the addresses.\nCreate a variable dist2pp which represent the distance from each request incidence to the nearest police precinct.\nCreate zip code level variables by merging with data from package uszipcode.\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over2h for the noise complains with the training data, using all the variables you can engineer from the available data. If you have tuning parameters, justify how they were selected.\nAssess the performance of your model in terms of commonly used metrics. Summarize your results to a New Yorker who is not data science savvy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fayaz, Omer. 2022. “Principles of Visual Communication.”\n\n\nGrillo, H. M., and M. Enesi. 2022. “The Impact, Importance, Types,\nand Use of Non-Verbal Communication in Social Relations.”\nLinguistics and Culture Review 6: 291–307.\n\n\nPrabavathi, R., and P. C. Nagasubramani. 2018. “Effective Oral and\nWritten Communication.” Journal of Applied and Advanced\nResearch 10: 29–32.\n\n\nPragmatic. 2024. “Communication Skills for Data Science.”\nhttps://www.pragmaticinstitute.com\n  /resources/articles/data/communication-skills-for-data-science/.\n\n\nRadovilsky, Z., V. Hegde, A. Acharya, and U. Uma. 2018. “Skills\nRequirements of Business Data Analytics and Data Science Jobs: A\nComparative Analysis.” Journal of Supply Chain and Operations\nManagement 16: 82–101.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe LASSO.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 58 (1): 267–88.\n\n\nVandemeulebroecke, Marc, Mark Baillie, Alison Margolskee, and Baldur\nMagnusson. 2019. “Effective Visual Communication for the\nQuantitative Scientist.” CPT Pharmacometrics Syst\nPharmacol 10: 705–19.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.\n\n\nYer, Simona. 2018. “Verbal Communication as a Two-Way Process in\nConnecting People.” Social Science Research Network.",
    "crumbs": [
      "References"
    ]
  }
]