[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Fall 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-f24.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2024, Spring 2023 and Spring 2022 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nTBA",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake data science more accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nAkach, Suha\nAstle, Jaden\nBabiec, Owen\nBaptista, Stef\nBienvenue, Jack\nBlanchard, Zachary\nBorowski, Emily\nClokey, Sara\nDesroches, Melanie\nFang, Zetong\nFebles, Xavier\nFreed, Brent\nJha, Aansh\nJohnson, Dorothea\nKashalapov, Olivia\nKlinowski, Amalia\nLee, Seunghyeon\nManna, Rahul\nMazzola, Julia\nParicharak, Aditya\nParvez, Mohammad Shahriyar\nTan, Qianruo\nXu, Deyu",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\n\nimport random\n## seed jointly set by the class\nrandom.seed(4737 + 8852 + 3196 + 2344 + 47)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nMaking presentations with Quarto\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use this Google Sheet to sign up.\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation.\n\n\n\nDate\nPresenter\n\n\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using IPython and Jupyter Notebook in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pan- das, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone\ngit pull\ngit status\ngit add\ngit remove\ngit commit\ngit push",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nquarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "Markdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#compiling-the-classnotes",
    "href": "quarto.html#compiling-the-classnotes",
    "title": "3  Reproducibile Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-f24. This also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (floder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-f24.git\n\n\n3.2.2 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-f24-venv\nHere .ids-f24-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-f24-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-f24-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes:\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "4  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template.\n\nInstall Quarto onto your computer following the instructions of Get Started. Document the obstacles you encountered and how you overcame them.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo. Document any obstacles you have and how you overcome them.\n\nGit basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidentally removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file as if you are explaining them to someone who want to contribute too.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nThe NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning. (JY: Add variable name cleaning next year.)\n\nUse the filter from the website to download the crash data of January 2023; save it under a directory data with an informative name (e.g., nyc_crashes_202301.csv).\nGet basic summaries of each variable: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre the LATITUDE and LONGITIDE values all look legitimate? If not (e.g., zeroes), code them as missing values.\nIf OFF STREET NAME is not missing, are there any missing LATITUDE and LONGITUDE? If so, geocode the addresses.\n(Optional) Are the missing patterns of ON STREET NAME and LATITUDE the same? Summarize the missing patterns by a cross table. If ON STREET NAME and CROSS STREET NAME are available, use geocoding by intersection to fill the LATITUDE and LONGITUDE.\nAre ZIP CODE and BOROUGH always missing together? If LATITUDE and LONGITUDE are available, use reverse geocoding to fill the ZIP CODE and BOROUGH.\nPrint the whole frequency table of CONTRIBUTING FACTOR VEHICLE 1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions do you have to make the data better based on your data exploration experience?\n\nExcept the first problem, use the cleaned data set with missing geocode imputed (data/nyc_crashes_202301_cleaned.csv).\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across borough? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable injury which is one if the number of persons injured is 1 or more; and zero otherwise. Construct a cross table for injury versus borough. Test the null hypothesis that the two variables are not associated.\nMerge the crash data with the zip code database.\nFit a logistic model with injury as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of injury with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix result from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\nThe NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.\n\nNYC Rodents Rats in NYC are widespread, as they are in many densely populated areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. In the 311 Service Request Data, there is one complain type Rodent. Extract all the requests with complain type Rodent, created between January 1, 2022 and December 31, 2023. Save them into a csv file named rodent_2022-2023.csv.\n\nAre there any complains that are not closed yet?\nAre there any complains with a closed data before the created date?\nHow many agencies were this complain type reported to?\nSummarize the missingess for each variable.\nSummarize a frequency table for the descriptor variable, and summarize a cross table by year.\nWhich types of ‘DESCRIPTOR’ do you think should be included if our interest is rodent sighting?\nTake a subset of the data with the descriptors you chose and summarize the response time by borough.\n\nNYC rodent sightings data cleaning The data appears to need some cleaning before any further analysis. Some missing values could be filled based on other columns.\n\nChecking all 47 column names suggests that some columns might be redundant. Identify them and demonstrate the redundancy.\nAre zip code and borough always missing together? If geocodes are available, use reverse geocoding to fill the zip code.\nExport the cleaned data in both csv and feather format. Comment on the file sizes.\n\nSQL Practice on NYC rodent sightings The NYC rodent sightings data that we prepared could be stored more efficiently using a database. Let us start from the csv file you exported from the last problem.\n\nCreate a table called rodent from the csv file.\nThe agency and agency_name columns are redundant in the table. Create a table called agency, which contains only these two columns, one agency a row.\nDrop the agency_name name from the rodent table. Justify why we do not need it here.\nComment on the sizes of the table (or exported csv file) of rodent before and after dropping the agency_name column.\nCome up with a scheme for the two tables that allows even more efficient storage of the agency column in the rodent table. _Hint: use an integer to code the agencies.\n\nLogistic Modeling The response time to 311 service requests is a measure of civic service quality. Let us model the response time to 311 requests with complain type Rodent.\n\nCompute the response time in hours. Note that some response will be missing because of unavailable closed date.\nCompute a binary variable over3d, which is one if the response time is greater than 3 days, and zero otherwise. Note that this variable should have no missing values.\nUse the package uszipcode to obtain the zip code level covariates such as median house income and median home value. Merge these variables to the rodent data.\nSplit the data at random into training (80%) and testing (20%). Build a logistic model to predict over3d on the training data, and validate the performance on the testing data.\nBuild a lasso logistic model to predict over3d, and justify your choice of the tuning parameter. Validate on the testing data.\n\nMidterm Project: Rodents in NYC Rodents in NYC are widespread, as they are in many densely populated areas. As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. Rat sightings in NYC was analyzed by Dr. Michael Walsh in a 2014 PeerJ article. We investigate this problem from a different angle with the NYC Rodent Inspection data, provided by the Department of Health and Mental Hygiene (DOHMH). Download the 2022-2023 data by filtering the INSPECTION_DATE to between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024 and INSPECTION_TYPE is either Initial or Compliance (which should be about 108 MB). Read the meta data information to understand the data.\n\nData cleaning.\n\nThere are two zipcode columns: ZIP_CODE and Zipcodes. Which one represent the zipcode of the inspection site? Comment on the data dictionary.\nSummarize the missing information. Are their missing values that can be filled using other columns? Fill them if yes.\nAre their redundant information in the data? Try storing the data using arrow and comment on the efficiency gain.\nAre there invalid zipcode or borough? Justify and clean them up if yes.\n\nData exploration.\n\nCreate binary variable passing indicating passing or not for the inspection result. Does passing depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.\nAre the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.\nIf we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passting rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.\nPerform a formal hypothesis test to confirm the observations from your visualization.\n\nData analytics.\n\nAggregate the inspections by zip code to create a dataset with five columns. The first three columns are zipcode; n_initial, the count of the initial inspections in that zipcode; and n_initpass, the number of initial inspections with a passing result in that zipcode. The other two variables are n_compliance and n_comppass, the counterpart for compliance inspections.\nAdd a variable to your dataset, n_sighting, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.\nMerge your dataset with the simple zipcode table in package uszipcode by zipcode to obtain demographic and socioeconomic variables at the zipcode level.\nBuild a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.\n\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "VanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.",
    "crumbs": [
      "References"
    ]
  }
]