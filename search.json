[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Fall 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our Spring 2024 repository at https://github.com/statds/ids-f24.\nStudents contributed to the lecture notes by submitting pull requests to our dedicated GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2024, Spring 2023 and Spring 2022 are both publicly accessible. These archives offer valuable insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nTBA",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nAkach, Suha\n\nChallenge and push myself to be better at python and all its libraries.\nBe confident in my abilities of programming and making statistical inferences that are correct.\nBe able to create my own personal project in class on time.\n\nAstle, Jaden\n\nI’ve used Git before, but I’d like to become more comfortable using it and get more used to different issues that arise.\nI’d like to learn more effective ways to “tell the story” of data analysis and show empowering visualizations.\nI’d like to explore more methods that professional data scientists use in their model trainings to share with UConn’s Data Science Club.\n\nBabiec, Owen\n\nBecome more comfortable with Git and Github and their applications\nBetter understand the Data Science pipeline and workflow\nLearn how to show my skills I have learned in this class during interviews\n\nBaptista, Stef\n\nDevelop a project/presentation suitible enough for industry\nImprove on my data science skills regarding pandas and numpy\nUnderstanding the scope of packages in python as a language\n\nBienvenue, Jack\n\nLearn professional visualization techniques, particularly for geospatial data\nFoster a high level of working knowledge of Git\nCreate a small portfolio of examples and projects for later reference\n\nBlanchard, Zachary\n\nGain experience working and collaborating on projects in Git\nImprove computer programming skills and familiarity with Python\nTeach other students about creating presentations using Quarto\n\nBorowski, Emily\n\nGain a greater understanding of Quarto and GitHub\nBecome more comfortable with my coding abilities\nAcquire a deeper understanding of data science\n\nClokey, Sara\n\nBecome more familiar with GitHub and Quarto\nExecute a data science project from start to finish\n\nDesroches, Melanie\n\nExplore the field of data science as a possible future career\nDevelope data science and machine learning skills\nBecome better at programming with Python and using Git/GitHub\n\nFebles, Xavier\n\nGain a further understanding of GitHub\nDevelop data visualization skills\nLearn applications of skills learned in previous courses\n\nJha, Aansh\n\nBe a better student of the data science field\nHone git to work in colabarative workspaces\nLearn better methods in data visualization\n\nJohnson, Dorothea\n\nEnter data science contests\nFamiliarize myself with using Python for data Science\nDevelop a proficiency in Github\n\nKashalapov, Olivia\n\nBetter understand neural networks\nMachine learning utilizing Python\nCreating and analyzing predictive models for informed decision making\n\nManna, Rahul\n\nUse knowledge gained and skills developed in class to study real-world problems such as climate change.\nObtain a basic understanding of machine learning\n\nMazzola, Julia\n\nBecome proficient in Git and Github.\nHave a better understanding of data science best practices and techniques.\nDeepen my knowledge of Python programming concepts and libraries.\n\nParicharak, Aditya\n\nMaster Commandline Interface\nApply my statistical knowladge and skills to course work\nUnderstand how to work with datasets\n\nParvez, Mohammad Shahriyar\n\nFamiliarizing myself with GitHub to effectively track and manage the entire data analysis process.\nAdopting Quarto for improved documentation of my data workflows.\nExploring advanced techniques for data analysis and visualization.\nDeveloping my personal Git repository and publishing data projects as a professional website.\n\nTan, Qianruo\n\nLearn how to use GitHub, and create my own page\nGet a good grade on this class\nLearn more about how to processing data\n\nXu, Deyu",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\ntarget = \"Blanchard\"  # pre-arranged 1st presenter\npresenters = [name for name in presenters if target not in name]\n\nimport random\n## seed jointly set by the class\nrandom.seed(5347 + 2896 + 9050 + 1687 + 63)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Xu,Deyu',\n 'Clokey,Sara Karen',\n 'Johnson,Dorothea Trixie',\n 'Febles,Xavier Milan',\n 'Cai,Yizhan',\n 'Bienvenue,Jack Noel',\n 'Mazzola,Julia Cecelia',\n 'Akach,Suha',\n 'Manna,Rahul',\n 'Astle,Jaden Bryce',\n 'Kashalapov,Olivia',\n 'Borowski,Emily Helen',\n 'Tan,Qianruo',\n 'Desroches,Melanie',\n 'Paricharak,Aditya Sushant',\n 'Jha,Aansh',\n 'Babiec,Owen Thomas',\n 'Baptista,Stef Clare',\n 'Parvez,Mohammad Shahriyar']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nMaking presentations with Quarto\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/11\nZachary Blanchard\nMaking presentation with Quarto\n\n\n09/16\nDeyu Xu\nImport/Export data\n\n\n09/18\nSara Clokey\nCommunications in data science\n\n\n09/23\nDorathea Johnson\nDatabase with SQL\n\n\n09/25\nXavier Febles\n\n\n\n09/30\nJack Bienvenue\nVisualizing spatial data in a Google Map\n\n\n10/02\nJulia Mazzola\nData Visualization with Plotnine\n\n\n10/07\nSuha Akach\n\n\n\n10/09\nRahul Manna\n\n\n\n10/14\nKaitlyn Anderson\nCareer readiness in data science\n\n\n10/16\nTBA\n\n\n\n10/21\nJaden Astle\nClassification and regression trees\n\n\n10/23\nOlivia Kashalapov\n\n\n\n10/28\nEmily Borowski\nRandom Forest\n\n\n10/30\nQianruo Tan\nReinforcement Learning\n\n\n11/04\nMelanie Desroches\n\n\n\n11/06\nAditya Paricharak\n\n\n\n11/11\nAansh Jha\n\n\n\n11/11\nOwen Babiec\n\n\n\n11/13\nStef Baptista\n\n\n\n11/13\nMohammad Parvez\nDeveloping a Python package\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nSara Clokey; Dorothea Johnson; Xavier Febles; Jack Bienvenue\n\n\n11/20\nJulia Mazzola; Suha Akach; Rahul Manna; Jaden Astle\n\n\n12/02\nOlivia Kashalapov; Emily Borowski；Qianruo Tan; Melanie Desroches\n\n\n12/04\nAditya Paricharak; Aansh Jha; Owen Babiec; Stef Baptista\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on []how to avoid plagiarism](https://usingsources.fas.harvard.edu/how-avoid-plagiarism). In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\n\nUse quarto source only. See 3  Reproducibile Data Science.\nFor the convenience of grading, add your html output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducibile Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#compiling-the-classnotes",
    "href": "quarto.html#compiling-the-classnotes",
    "title": "3  Reproducibile Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-f24. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-f24-venv\nHere .ids-f24-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-f24-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-f24-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (floder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-f24.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-f24\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducibile Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([-6.49040209,  2.32335861,  4.65647428, -5.49637375, -2.16629946,\n        6.03258051,  6.55954796,  2.77251687, -4.16033252,  3.20641679])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.01048353, 0.09941021, 0.07999752, 0.01722582, 0.05797914,\n       0.05999997, 0.05208366, 0.09789279, 0.03046548, 0.09530096])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n9.18 µs ± 210 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n2.05 µs ± 71.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n72.1 µs ± 2.21 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monte Hall\nHere is a function that performs the Monte Hall experiments.\n\nimport numpy as np\n\ndef montehall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontehall(3, 1000)\nmontehall(4, 1000)\n\n{'noswitch': 249, 'switch': 379}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4969455424\n4969455424\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4443959560\n4587305744\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times largger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "5  Communication in Data Science",
    "section": "",
    "text": "5.1 Introduction\nHi! My name is Sara, and I am a junior double majoring in Applied Data Analysis and Communication. The topic of my presentation today, Communication in Data Science, combines my academic and professional interests while underscoring the importance of ‘soft skills’ like public speaking, for example, within STEM fields like data science.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#importance-application-of-communication",
    "href": "communication.html#importance-application-of-communication",
    "title": "5  Communication in Data Science",
    "section": "5.2 Importance & Application of Communication",
    "text": "5.2 Importance & Application of Communication\nData science as a career path has exploded within the last decade. Some fields that offer data science positions include:\n\nFinance\nHealthcare\nMedia production\nSports\nBanking\nInsurance\nE-Commerce\nEnergy\nManufacturing\nTransportation\nConstruction\n\nBecause data science is applicable in so many industries, it is essential that data scientists have the skills and experience to communicate their work with others who do not have the same technical education. As analyzed by Radovilsky et al. (2018), job listings within the field of data science often include qualifications like “strong interpersonal skills” and “demonstrated presentation and communication ability,” highlighting the pervasive need for this skill set.\nWithin these industries, collaboration and teamwork are often at the forefront. Inexperience with data should not prevent your colleagues from being able to contribute to shared projects, and strong communication skills can mitigate this challenge!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#general-communication-skills",
    "href": "communication.html#general-communication-skills",
    "title": "5  Communication in Data Science",
    "section": "5.3 General Communication Skills",
    "text": "5.3 General Communication Skills\n\n5.3.1 Verbal Communication Skills\nVerbal Communication: “The use of sounds and language to relay a message” (Yer (2018))\nVerbal Communication Tips:\n\nMake a good first impression\nUse appropriate language (jargon, metaphors)\nPrioritize brevity\nPractice beforehand\nAllow room for questions\n\n\n\n5.3.2 Non-Verbal Communication Skills\nNon-Verbal Communication: “Information, emotion, a movement that is expressed without words and without the help of language” (Grillo and Enesi (2022))\nNon-Verbal Communication Tips:\n\nUtilize vocal variety (pitch, rate, volume)\nAvoid distracting hand and body movements\nMake eye contact\nPay attention to proxemics\n\n\n\n5.3.3 Visual Communication Skills\nVisual Communication: “Any communication that employs one’s sense of sight to deliver a message without the usage of any verbal cues” (Fayaz (2022))\nVisual Communication Tips:\n\nPrioritize clarity\nUse proper labeling and scaling\nCreate visual contrast (colors, shapes, fonts)\nChoose the most appropriate visual representation\n\n\n\n5.3.4 Written Communication Skills\nWritten Communication: “Any form of communication which is written and documented from the sender to the receiver” (Prabavathi and Nagasubramani (2018))\nWritten Communication Tips:\n\nClearly state your goal with a thesis statement\nMaintain professionalism (contractions, slang)\nProofread and utilize peer editing\nFollow a specific structure\nBalance concision with analysis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#communication-in-data-science",
    "href": "communication.html#communication-in-data-science",
    "title": "5  Communication in Data Science",
    "section": "5.4 Communication in Data Science",
    "text": "5.4 Communication in Data Science\nOften, data scientists must communicate “technical conclusions to non-technical members” (Pragmatic (2024)); this may be colleagues in other departments, like marketing, or supervisors at the managerial level. Here are some tips for effectively communicating project results specifically in the field of data science.\n\n5.4.1 Identify your Audience\nWho are you sharing information with? Is it a room of data scientists like this one? Is it full of students who want to learn about data science? Is it a group of executives looking to make a funding decision?\n\nConsider the context and prior knowledge (technical jargon)\nConsider the motivation for listening\n\n\n\n5.4.2 Utilize Data Visualization\nOne of the most effective methods of communicating results in data science, especially to those without technical coding knowledge, is data visualization techniques (Vandemeulebroecke et al. (2019)). Python uses the package ‘matplotlib’ to produce these visualizations, including:\n\nline plots\nbar plots\nbox plots\nhistograms\nheat maps\npie charts\n\nThese visualizations allow complex statistical projects to be simplified into a single graphic, focusing on project results and implications rather than methodology. Ensure that data visualization techniques are free of technical jargon and clearly label all visual aspects.\n\n\n5.4.3 Focus on Data Communication Skills\nThe following skill sets highlight technical data communication that will be more common in projects with other data scientists to communicate about your data.\n\nCoding communication: Python, R, Julia, JavaScript, SQL, etc.\nAnalysis communication: creating a storyline, descriptive versus diagnostic versus predictive analytics, problem identification\nData management: collection, cleaning/transformation, storage\nData visualization\n\n\n\n5.4.4 Give Space for Questions and Feedback\nWithin professional spaces, data scientists should provide time for their clients, supervisors, and colleagues to ask questions about their work and subsequent findings.\n\nPause for questions throughout the presentation\nOffer contact information for continued collaboration\nProvide a structure for anonymous feedback\nSchedule follow-ups if necessary\n\nTeamwork is often at the heart of data science projects within industries, and open communication makes this teamwork run much more smoothly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#further-learning",
    "href": "communication.html#further-learning",
    "title": "5  Communication in Data Science",
    "section": "5.5 Further Learning",
    "text": "5.5 Further Learning\nWhile pursuing degrees in the data science field, consider taking Communication courses at UConn that can bolster your understanding and skill set. Some applicable communication courses include:\n\nCOMM 2100: Professional Communication\nCOMM 3110: Organizational Communication\nCOMM 3430: Science Communication\nCOMM 5655: Human-Computer Interaction\nCOMM 5900: Professional Communication\n\nEffective communication also requires practice. Here are some ways to practice these skills while earning your degree:\n\nFully participate in group projects\nSeek presentation opportunities (class, conferences, etc.)\nExplain data science coursework to peers outside of your program\nExplore internship opportunities that involve collaboration with other departments\n\n\n\n\n\nFayaz, Omer. 2022. “Principles of Visual Communication.”\n\n\nGrillo, H. M., and M. Enesi. 2022. “The Impact, Importance, Types, and Use of Non-Verbal Communication in Social Relations.” Linguistics and Culture Review 6: 291–307.\n\n\nPrabavathi, R., and P. C. Nagasubramani. 2018. “Effective Oral and Written Communication.” Journal of Applied and Advanced Research 10: 29–32.\n\n\nPragmatic. 2024. “Communication Skills for Data Science.” https://www.pragmaticinstitute.com\n  /resources/articles/data/communication-skills-for-data-science/.\n\n\nRadovilsky, Z., V. Hegde, A. Acharya, and U. Uma. 2018. “Skills Requirements of Business Data Analytics and Data Science Jobs: A Comparative Analysis.” Journal of Supply Chain and Operations Management 16: 82–101.\n\n\nVandemeulebroecke, Marc, Mark Baillie, Alison Margolskee, and Baldur Magnusson. 2019. “Effective Visual Communication for the Quantitative Scientist.” CPT Pharmacometrics Syst Pharmacol 10: 705–19.\n\n\nYer, Simona. 2018. “Verbal Communication as a Two-Way Process in Connecting People.” Social Science Research Network.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#importexport-data",
    "href": "manipulation.html#importexport-data",
    "title": "6  Data Manipulation",
    "section": "6.2 Import/Export Data",
    "text": "6.2 Import/Export Data\nThis section was written by Deyu Xu, a MS student in Statistics at the time.\n\n6.2.1 Subsection 1\n\n\n6.2.2 Subsection 2\n\n\n6.2.3 Extended Reading\nGive some references here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#sql",
    "href": "manipulation.html#sql",
    "title": "6  Data Manipulation",
    "section": "6.3 SQL",
    "text": "6.3 SQL\nThis section was written by Thea Johnson, a senior in Statistics at the time.\n\n6.3.1 Subsection 1\n\n\n6.3.2 Subsection 2\n\n\n6.3.3 Extended Reading\nGive some references here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#nyc-crash-data",
    "href": "manipulation.html#nyc-crash-data",
    "title": "6  Data Manipulation",
    "section": "6.4 NYC Crash Data",
    "text": "6.4 NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on September 16, 2024, in CSC format.\n\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20240916.csv'\ndf = pd.read_csv(file_path)\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Display the first few rows of the dataset to understand its structure\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n17:30\nNaN\nNaN\n0.00000\n0.00000\n(0.0, 0.0)\nNaN\nNaN\nGOLD STREET\n...\nUnspecified\nNaN\nNaN\nNaN\n4736746\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n0:32\nNaN\nNaN\nNaN\nNaN\nNaN\nBELT PARKWAY RAMP\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n7:05\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNaN\nNaN\n2797 OCEAN PARKWAY\n...\nNaN\nNaN\nNaN\nNaN\n4737060\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n20:47\nNaN\nNaN\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n10:14\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736759\nBus\nBox Truck\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Longitude/latitude don't need double precision\ndf['latitude'] = df['latitude'].astype('float32', errors='ignore')\ndf['longitude'] = df['longitude'].astype('float32', errors='ignore')\n\n# Drop the redundant 'location' column\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nmissing_cooccur, total_missing_zip_code, total_missing_borough\n\n(np.int64(541), np.int64(541), np.int64(541))\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nzip_code = get_zip_code(latitude, longitude)\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "6  Data Manipulation",
    "section": "6.5 Cross-platform Data Format Arrow",
    "text": "6.5 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\n\ndf.to_feather('data/nyccrashes_cleaned.feather')\n\n# Compare the file sizes of the feather format and the CSV format\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20240916.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#using-the-census-data",
    "href": "manipulation.html#using-the-census-data",
    "title": "6  Data Manipulation",
    "section": "6.6 Using the Census Data",
    "text": "6.6 Using the Census Data\nThe US Census provides a lot of useful data that could be merged with the NYC crash data for further analytics.\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the zip_code column is clean and consistent. and consistent.\n\nprint(df['zip_code'].isnull().sum())\n# Standardize to 5-digit codes, if necessary\ndf['zip_code'] = df['zip_code'].astype(str).str.zfill(5) \n\n541\n\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nDemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment by pip install uszipcode.\nNow let’s work on the rodent sightings data.\nWe will first clean the zip_code column to ensure it only contains valid ZIP codes. Then, we will use a vectorized approach to fetch the required data for each unique zip code and merge this information back into the original DataFrame.\n\n# Remove rows where 'zip_code' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['zip_code']).copy()\nvalid_zip_df['zip_code'] = valid_zip_df['zip_code'].astype(str).str.zfill(5)\nunique_zips = valid_zip_df['zip_code'].unique()\n\nSince uszipcode doesn’t inherently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple times.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'zip_code'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"zip_code\")\n\nmerged_df.columns\n\nIndex(['borough', 'zip_code', 'latitude', 'longitude', 'on_street_name',\n       'cross_street_name', 'off_street_name', 'number_of_persons_injured',\n       'number_of_persons_killed', 'number_of_pedestrians_injured',\n       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n       'number_of_cyclist_killed', 'number_of_motorist_injured',\n       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5',\n       'crash_datetime', 'median_home_value', 'median_household_income'],\n      dtype='object')",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "7.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.text() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.03398\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "7  Statistical Tests and Models",
    "section": "",
    "text": "Comparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "7  Statistical Tests and Models",
    "section": "7.2 Statistical Modeling",
    "text": "7.2 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n7.2.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n7.2.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is [ y = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + . ]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nMon, 30 Sep 2024\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n12:41:45\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nMon, 30 Sep 2024\n\n\n\n\nTime:\n12:41:45\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n7.2.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function (\\(g(\\mu) = \\mu\\)) is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity (\\(g(\\mu) = \\mu\\))\nDistribution: Normal\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit (\\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\))\nDistribution: Binomial\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log (\\(g(\\mu) = \\log(\\mu)\\))\nDistribution: Poisson\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse (\\(g(\\mu) = \\frac{1}{\\mu}\\))\nDistribution: Gamma\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution.\nA logistic regression can be fit with statsmodels.api.glm.\nLet’s generate some binary data first by dichotomizing existing variables.\n\nimport statsmodels.genmod as smg\neta = x.dot([2, 2, 2, 2, 2]) - 5\np = smg.families.links.Logit().inverse(eta)\ndf[\"yb\"] = np.random.binomial(1, p, p.size)\n\nFit a logistic regression for y1b.\n\nmylogistic = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                     family = smg.families.Binomial())\nmylfit = mylogistic.fit()\nmylfit.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nyb\nNo. Observations:\n200\n\n\nModel:\nGLM\nDf Residuals:\n194\n\n\nModel Family:\nBinomial\nDf Model:\n5\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-110.80\n\n\nDate:\nMon, 30 Sep 2024\nDeviance:\n221.61\n\n\nTime:\n12:41:47\nPearson chi2:\n196.\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.2410\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-4.6982\n0.820\n-5.728\n0.000\n-6.306\n-3.091\n\n\nx1\n2.1654\n0.618\n3.505\n0.000\n0.955\n3.376\n\n\nx2\n1.1788\n0.579\n2.035\n0.042\n0.044\n2.314\n\n\nx3\n1.5921\n0.587\n2.713\n0.007\n0.442\n2.742\n\n\nx4\n2.2310\n0.630\n3.539\n0.000\n0.995\n3.466\n\n\nx5\n2.5496\n0.598\n4.263\n0.000\n1.377\n3.722\n\n\n\n\n\nIf we treat y1b as count data, a Poisson regression can be fitted.\n\nmyPois = smf.glm(formula = 'yb ~ x1 + x2 + x3 + x4 + x5', data = df,\n                 family = smg.families.Poisson())\nmypfit = myPois.fit()\nmypfit.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nyb\nNo. Observations:\n200\n\n\nModel:\nGLM\nDf Residuals:\n194\n\n\nModel Family:\nPoisson\nDf Model:\n5\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-161.33\n\n\nDate:\nMon, 30 Sep 2024\nDeviance:\n112.66\n\n\nTime:\n12:41:47\nPearson chi2:\n89.6\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1071\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.4529\n0.435\n-5.644\n0.000\n-3.305\n-1.601\n\n\nx1\n0.7638\n0.361\n2.117\n0.034\n0.057\n1.471\n\n\nx2\n0.3698\n0.330\n1.121\n0.262\n-0.277\n1.017\n\n\nx3\n0.5396\n0.345\n1.562\n0.118\n-0.137\n1.217\n\n\nx4\n0.7895\n0.368\n2.144\n0.032\n0.068\n1.511\n\n\nx5\n0.9477\n0.352\n2.691\n0.007\n0.257\n1.638",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "8  Supervised Learning",
    "section": "",
    "text": "8.1 Introduction\nSupervised learning uses labeled datasets to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross validation process.\nIn contrast, unsupervised learning uses unlabeled data to discover patterns that help solve for clustering or association problems. This is particularly useful when subject matter experts are unsure of common properties within a data set.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#classification-vs-regression",
    "href": "supervised.html#classification-vs-regression",
    "title": "8  Supervised Learning",
    "section": "8.2 Classification vs Regression",
    "text": "8.2 Classification vs Regression\n\nClassificaiton: outcome variable is categorical\nRegression: outcome variable is continuous\nBoth problems can have many covariates (predictors/features)\n\n\n8.2.1 Regression metrics\n\nMean squared error (MSE)\nMean absolute error (MAE)\n\n\n\n8.2.2 Classification metrics\n\n8.2.2.1 Confusion matrix\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTP: number of true positives\nFN: number of false negatives\nFP: number of false positives\nTN: number of true negatives\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTPR: TP / (TP + FN). Also known as sensitivity.\nFNR: TN / (TP + FN). Also known as miss rate.\nFPR: FP / (FP + TN). Also known as false alarm, fall-out.\nTNR: TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\nFour rates from the confusion matrix with predicted (column) margins:\n\nPPV: TP / (TP + FP). Also known as precision.\nFDR: FP / (TP + FP).\nFOR: FN / (FN + TN).\nNPV: TN / (FN + TN).\n\n\n\n8.2.2.2 Measure of classification performance\nMeasures for a given confusion matrix:\n\nAccuracy: (TP + TN) / (P + N). The proportion of all corrected predictions. Not good for highly imbalanced data.\nRecall (sensitivity/TPR): TP / (TP + FN). Intuitively, the ability of the classifier to find all the positive samples.\nPrecision: TP / (TP + FP). Intuitively, the ability of the classifier not to label as positive a sample that is negative.\nF-beta score: Harmonic mean of precision and recall with \\(\\beta\\) chosen such that recall is considered \\(\\beta\\) times as important as precision, \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}\n\\] See stackexchange post for the motivation of \\(\\beta^2\\).\n\nWhen classification is obtained by dichotomizing a continuous score, the receiver operating characteristic (ROC) curve gives a graphical summary of the FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at all thresholds.\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\n\n\n\n8.2.3 Cross-validation\n\nGoal: strike a bias-variance tradeoff.\nK-fold: hold out each fold as testing data.\nScores: minimized to train a model\n\nCross-validation is an important measure to prevent over-fitting. Good in-sample performance does not necessarily mean good out-sample performance. A general work flow in model selection with cross-validation is as follows.\n\nSplit the data into training and testing\nFor each candidate model \\(m\\) (with possibly multiple tuning parameters)\n\nFit the model to the training data\nObtain the performance measure \\(f(m)\\) on the testing data (e.g., CV score, MSE, loss, etc.)\n\nChoose the model \\(m^* = \\arg\\max_m f(m)\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "9  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template.\n\nInstall Quarto onto your computer following the instructions of Get Started. Document the obstacles you encountered and how you overcame them.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo. Document any obstacles you have and how you overcome them.\n\nGit basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidentally removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file as if you are explaining them to someone who want to contribute too.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Write a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trails in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy. Apply your function with 3 doors and 5 doors, both with 1000 trials. Include sufficient text around the code to explain your them.\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possibly ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable severe which is one if the number of persons injured of deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the zip code database.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database.\n\nUsing the cleaned NYC crash data, perform classification of severe with support vector machine and compare the results with the benchmark from regularized logistic regression. Use the last week’s data as testing data.\n\nExplain the parameters you used in your fitting for each method.\nExplain the confusion matrix result from each fit.\nCompare the performance of the two approaches in terms of accuracy, precision, recall, F1-score, and AUC.\n\nThe NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables,\n\nClean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.\nRemove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Created Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends of their creation and across boroughs.\nDefine a binary variable over3h which is 1 if duration is greater than 3 hours. Note that it can be obtained even for censored duration. Build a model to predict over3h. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.\n\nNYC Rodents Rats in NYC are widespread, as they are in many densely populated areas (https://en.wikipedia.org/wiki/Rats_in_New_York_City). As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. In the 311 Service Request Data, there is one complain type Rodent. Extract all the requests with complain type Rodent, created between January 1, 2022 and December 31, 2023. Save them into a csv file named rodent_2022-2023.csv.\n\nAre there any complains that are not closed yet?\nAre there any complains with a closed data before the created date?\nHow many agencies were this complain type reported to?\nSummarize the missingess for each variable.\nSummarize a frequency table for the descriptor variable, and summarize a cross table by year.\nWhich types of ‘DESCRIPTOR’ do you think should be included if our interest is rodent sighting?\nTake a subset of the data with the descriptors you chose and summarize the response time by borough.\n\nNYC rodent sightings data cleaning The data appears to need some cleaning before any further analysis. Some missing values could be filled based on other columns.\n\nChecking all 47 column names suggests that some columns might be redundant. Identify them and demonstrate the redundancy.\nAre zip code and borough always missing together? If geocodes are available, use reverse geocoding to fill the zip code.\nExport the cleaned data in both csv and feather format. Comment on the file sizes.\n\nSQL Practice on NYC rodent sightings The NYC rodent sightings data that we prepared could be stored more efficiently using a database. Let us start from the csv file you exported from the last problem.\n\nCreate a table called rodent from the csv file.\nThe agency and agency_name columns are redundant in the table. Create a table called agency, which contains only these two columns, one agency a row.\nDrop the agency_name name from the rodent table. Justify why we do not need it here.\nComment on the sizes of the table (or exported csv file) of rodent before and after dropping the agency_name column.\nCome up with a scheme for the two tables that allows even more efficient storage of the agency column in the rodent table. _Hint: use an integer to code the agencies.\n\nLogistic Modeling The response time to 311 service requests is a measure of civic service quality. Let us model the response time to 311 requests with complain type Rodent.\n\nCompute the response time in hours. Note that some response will be missing because of unavailable closed date.\nCompute a binary variable over3d, which is one if the response time is greater than 3 days, and zero otherwise. Note that this variable should have no missing values.\nUse the package uszipcode to obtain the zip code level covariates such as median house income and median home value. Merge these variables to the rodent data.\nSplit the data at random into training (80%) and testing (20%). Build a logistic model to predict over3d on the training data, and validate the performance on the testing data.\nBuild a lasso logistic model to predict over3d, and justify your choice of the tuning parameter. Validate on the testing data.\n\nMidterm Project: Rodents in NYC Rodents in NYC are widespread, as they are in many densely populated areas. As of October 2023, NYC dropped from the 2nd to the 3rd places in the annual “rattiest city” list released by a pest control company. Rat sightings in NYC was analyzed by Dr. Michael Walsh in a 2014 PeerJ article. We investigate this problem from a different angle with the NYC Rodent Inspection data, provided by the Department of Health and Mental Hygiene (DOHMH). Download the 2022-2023 data by filtering the INSPECTION_DATE to between 11:59:59 pm of 12/31/2021 and 12:00:00 am of 01/01/2024 and INSPECTION_TYPE is either Initial or Compliance (which should be about 108 MB). Read the meta data information to understand the data.\n\nData cleaning.\n\nThere are two zipcode columns: ZIP_CODE and Zipcodes. Which one represent the zipcode of the inspection site? Comment on the data dictionary.\nSummarize the missing information. Are their missing values that can be filled using other columns? Fill them if yes.\nAre their redundant information in the data? Try storing the data using arrow and comment on the efficiency gain.\nAre there invalid zipcode or borough? Justify and clean them up if yes.\n\nData exploration.\n\nCreate binary variable passing indicating passing or not for the inspection result. Does passing depend on whether the inspection is initial or compliance? State your hypothesis and summarize your test result.\nAre the passing pattern different across different boroughs for initial inspections? How about compliance inspections? State your hypothesis and summarize your test results.\nIf we suspect that the passing rate may depends on the time of a day of the inspection, we may compare the passting rates for inspections done in the mornings and inspections one in the afternoons. Visualize the comparison by borough and inspection type.\nPerform a formal hypothesis test to confirm the observations from your visualization.\n\nData analytics.\n\nAggregate the inspections by zip code to create a dataset with five columns. The first three columns are zipcode; n_initial, the count of the initial inspections in that zipcode; and n_initpass, the number of initial inspections with a passing result in that zipcode. The other two variables are n_compliance and n_comppass, the counterpart for compliance inspections.\nAdd a variable to your dataset, n_sighting, which represent the number of rodent sightings from the 311 service request data in the same 2022-2023 period.\nMerge your dataset with the simple zipcode table in package uszipcode by zipcode to obtain demographic and socioeconomic variables at the zipcode level.\nBuild a binomial regression for the passing rate of initial inspections at the zipcode level. Assess the goodness-of-fit of your model. Summarize your results to a New Yorker who is not data science savvy.\n\nNow you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fayaz, Omer. 2022. “Principles of Visual Communication.”\n\n\nGrillo, H. M., and M. Enesi. 2022. “The Impact, Importance, Types,\nand Use of Non-Verbal Communication in Social Relations.”\nLinguistics and Culture Review 6: 291–307.\n\n\nPrabavathi, R., and P. C. Nagasubramani. 2018. “Effective Oral and\nWritten Communication.” Journal of Applied and Advanced\nResearch 10: 29–32.\n\n\nPragmatic. 2024. “Communication Skills for Data Science.”\nhttps://www.pragmaticinstitute.com\n  /resources/articles/data/communication-skills-for-data-science/.\n\n\nRadovilsky, Z., V. Hegde, A. Acharya, and U. Uma. 2018. “Skills\nRequirements of Business Data Analytics and Data Science Jobs: A\nComparative Analysis.” Journal of Supply Chain and Operations\nManagement 16: 82–101.\n\n\nVandemeulebroecke, Marc, Mark Baillie, Alison Margolskee, and Baldur\nMagnusson. 2019. “Effective Visual Communication for the\nQuantitative Scientist.” CPT Pharmacometrics Syst\nPharmacol 10: 705–19.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook:\nEssential Tools for Working with Data. O’Reilly Media,\nInc.\n\n\nYer, Simona. 2018. “Verbal Communication as a Two-Way Process in\nConnecting People.” Social Science Research Network.",
    "crumbs": [
      "References"
    ]
  }
]