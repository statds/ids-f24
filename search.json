[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT 3255/5255 in Fall 2024 represent a collaborative effort between Professor Jun Yan and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-f24.\nStudents contributed to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.\nFor those interested in exploring the lecture notes from the previous years, the Spring 2024, Spring 2023 and Spring 2022 are also publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps:\n\nClone the classnotes repository to an appropriate location on your computer.\nSet up a Python virtual environment in the root folder of the source.\nInstall all the packages specified in requirements.txt.\nFor some chapters that need to interact with Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-project",
    "href": "index.html#midterm-project",
    "title": "Introduction to Data Science",
    "section": "Midterm Project",
    "text": "Midterm Project\nNYC noise complaints made to NYPD in the week of July 4, 2024. See details in the exercises.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of data challenges that you may find useful:\n\nASA Data Challenge Expo\nKaggle\nDrivenData\nTop 10 Data Science Competitions in 2024\n\nIf you work on sports analytics, you are welcome to submit a poster to UConn Sports Analytics Symposium (UCSAS) 2024.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nYan, Jun\n\nMake practical data science tools accessible to undergraduates\nCo-develop a Quarto book in collaboration with the students\nTrain students to participate real data science competitions\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nAkach, Suha\n\nChallenge and push myself to be better at python and all its libraries.\nBe confident in my abilities of programming and making statistical inferences that are correct.\nBe able to create my own personal project in class on time.\n\nAstle, Jaden\n\nI’ve used Git before, but I’d like to become more comfortable using it and get more used to different issues that arise.\nI’d like to learn more effective ways to “tell the story” of data analysis and show empowering visualizations.\nI’d like to explore more methods that professional data scientists use in their model trainings to share with UConn’s Data Science Club.\n\nBabiec, Owen\n\nBecome more comfortable with Git and Github and their applications\nBetter understand the Data Science pipeline and workflow\nLearn how to show my skills I have learned in this class during interviews\n\nBaptista, Stef\n\nDevelop a project/presentation suitible enough for industry\nImprove on my data science skills regarding pandas and numpy\nUnderstanding the scope of packages in python as a language\n\nBienvenue, Jack\n\nLearn professional visualization techniques, particularly for geospatial data\nFoster a high level of working knowledge of Git\nCreate a small portfolio of examples and projects for later reference\n\nBlanchard, Zachary\n\nGain experience working and collaborating on projects in Git\nImprove computer programming skills and familiarity with Python\nTeach other students about creating presentations using Quarto\n\nBorowski, Emily\n\nGain a greater understanding of Quarto and GitHub\nBecome more comfortable with my coding abilities\nAcquire a deeper understanding of data science\n\nClokey, Sara\n\nBecome more familiar with GitHub and Quarto\nExecute a data science project from start to finish\n\nDesroches, Melanie\n\nExplore the field of data science as a possible future career\nDevelope data science and machine learning skills\nBecome better at programming with Python and using Git/GitHub\n\nFebles, Xavier\n\nGain a further understanding of GitHub\nDevelop data visualization skills\nLearn applications of skills learned in previous courses\n\nJha, Aansh\n\nBe a better student of the data science field\nHone git to work in colabarative workspaces\nLearn better methods in data visualization\n\nJohnson, Dorothea\n\nEnter data science contests\nFamiliarize myself with using Python for data Science\nDevelop a proficiency in Github\n\nKashalapov, Olivia\n\nBetter understand neural networks\nMachine learning utilizing Python\nCreating and analyzing predictive models for informed decision making\n\nManna, Rahul\n\nUse knowledge gained and skills developed in class to study real-world problems such as climate change.\nObtain a basic understanding of machine learning\n\nMazzola, Julia\n\nBecome proficient in Git and Github.\nHave a better understanding of data science best practices and techniques.\nDeepen my knowledge of Python programming concepts and libraries.\n\nParicharak, Aditya\n\nMaster Commandline Interface\nApply my statistical knowladge and skills to course work\nUnderstand how to work with datasets\n\nParvez, Mohammad Shahriyar\n\nFamiliarizing myself with GitHub to effectively track and manage the entire data analysis process.\nAdopting Quarto for improved documentation of my data workflows.\nExploring advanced techniques for data analysis and visualization.\nDeveloping my personal Git repository and publishing data projects as a professional website.\n\nTan, Qianruo\n\nLearn how to use GitHub, and create my own page\nGet a good grade on this class\nLearn more about how to processing data\n\nXu, Deyu\n\nBe proficient in using Python to process data.\nLearn the basics of machine learning.\nHave a basic understanding of data scienc.\nLay a solid foundation for GNN and Bayes neural network.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#presentation-orders",
    "href": "index.html#presentation-orders",
    "title": "Introduction to Data Science",
    "section": "Presentation Orders",
    "text": "Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\npresenters = ug + gr\ntarget = \"Blanchard\"  # pre-arranged 1st presenter\npresenters = [name for name in presenters if target not in name]\n\nimport random\n## seed jointly set by the class\nrandom.seed(5347 + 2896 + 9050 + 1687 + 63)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\n['Xu,Deyu',\n 'Clokey,Sara Karen',\n 'Johnson,Dorothea Trixie',\n 'Febles,Xavier Milan',\n 'Cai,Yizhan',\n 'Bienvenue,Jack Noel',\n 'Mazzola,Julia Cecelia',\n 'Akach,Suha',\n 'Manna,Rahul',\n 'Astle,Jaden Bryce',\n 'Kashalapov,Olivia',\n 'Borowski,Emily Helen',\n 'Tan,Qianruo',\n 'Desroches,Melanie',\n 'Paricharak,Aditya Sushant',\n 'Jha,Aansh',\n 'Babiec,Owen Thomas',\n 'Baptista,Stef Clare',\n 'Parvez,Mohammad Shahriyar']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nPresentation Task Board\nHere are some example tasks:\n\nMaking presentations with Quarto\nData science ethics\nData science communication skills\nImport/Export data\nArrow as a cross-platform data format\nDatabase operation with Structured query language (SQL)\nGrammer of graphics\nHandling spatial data\nVisualize spatial data in a Google map\nAnimation\nClassification and regression trees\nSupport vector machine\nRandom forest\nNaive Bayes\nBagging vs boosting\nNeural networks\nDeep learning\nTensorFlow\nAutoencoders\nReinforcement learning\nCalling C/C++ from Python\nCalling R from Python and vice versa\nDeveloping a Python package\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n09/11\nZachary Blanchard\nPresentation with Quarto\n\n\n09/16\nDeyu Xu\nImport/Export data\n\n\n09/18\nSara Clokey\nCommunications in Data Science\n\n\n09/23\nDorathea Johnson\nDatabase with SQL\n\n\n09/25\nXavier Febles\nStatistical tests\n\n\n09/30\nJack Bienvenue\nVisualizing Spatial Data in a Google Map\n\n\n10/02\nJulia Mazzola\nData VisualiZation with Plotnine\n\n\n10/07\nSuha Akach\nNaive Bayes classifier\n\n\n10/09\nRahul Manna\nAnimation\n\n\n10/23\nJaden Astle\nClassification and Regression Trees\n\n\n10/23\nOlivia Kashalapov\nSynthetic Minority Oversampling Technique (SMOTE)\n\n\n10/28\nData science alumni panel\n\n\n\n10/30\nEmily Borowski\nRandom Forest\n\n\n10/30\nAditya Paricharak\nNeural Networks\n\n\n11/04\nMelanie Desroches\nWeb Scraping\n\n\n11/06\nQianruo Tan\nReinforcement Learning\n\n\n11/11\nAansh Jha\nK-means Clustering\n\n\n11/11\nOwen Babiec\nCalling R from Python and Vice Versa\n\n\n11/13\nStef Baptista\n\n\n\n11/13\nMohammad Parvez\nExtracting and Analyzing Census Data\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is availabe under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\n\n\n\n\nDate\nPresenter\n\n\n\n\n11/18\nSara Clokey; Dorothea Johnson; Xavier Febles; Jack Bienvenue\n\n\n11/20\nJulia Mazzola; Suha Akach; Rahul Manna; Jaden Astle\n\n\n12/02\nOlivia Kashalapov; Emily Borowski；Qianruo Tan; Melanie Desroches\n\n\n12/04\nAditya Paricharak; Aansh Jha; Owen Babiec; Stef Baptista\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nEdit _quarto.yml add a line for your qmd file to include it in the notes.\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file mysection.qmd and a new line to _quarto.yml as an example.\nFor more detailed style guidance, please see my notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism. In particular, in our course, one convenient way to avoid plagiarism is to use our own data (e.g., NYC Open Data). Combined with your own explanation of the code chunks, it would be hard to plagiarize.\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nUse quarto source only. See 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\n\nIntroduction\nPut an overview here. Use Markdown syntax.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\nimport pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denotes computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#expectations-from-this-course",
    "href": "intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#computing-environment",
    "href": "intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump start, here is a tutorial: Ubunto Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.2 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding styles against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n\n\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McK- inney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#set-up-gitgithub",
    "href": "git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#most-frequently-used-git-commands",
    "href": "git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#tips-on-using-git",
    "href": "git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "git.html#pull-request",
    "href": "git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction-to-quarto",
    "href": "quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "quarto.html#compiling-the-classnotes",
    "href": "quarto.html#compiling-the-classnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-f24. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Set up your Python Virtual Environment\nI suggest that a Python virtual environment for the classnotes be set up in the current directory for reproducibility. A Python virtual environment is simply a directory with a particular file structure, which contains a specific Python interpreter and software libraries and binaries needed to support a project. It allows us to isolate our Python development projects from our system installed Python and other Python environments.\nTo create a Python virtual environment for our classnotes:\npython3 -m venv .ids-f24-venv\nHere .ids-f24-venv is the name of the virtual environment to be created. Choose an informative name. This only needs to be set up once.\nTo activate this virtual environment:\n. .ids-f24-venv/bin/activate\nAfter activating the virtual environment, you will see (.ids-f24-venv) at the beginning of your shell prompt. Then, the Python interpreter and packages needed will be the local versions in this virtual environment without interfering your system-wide installation or other virtual environments.\nTo install the Python packages that are needed to compile the classnotes, we have a requirements.txt file that specifies the packages and their versions. They can be installed easily with:\npip install -r requirements.txt\nIf you are interested in learning how to create the requirements.txt file, just put your question into a Google search.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\n\n\n3.2.2 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-f24.git\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository\ncd ids-f24\nquarto render\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 Know Your Computer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#know-your-computer",
    "href": "python.html#know-your-computer",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have before its time.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n4.1.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#the-python-world",
    "href": "python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "4.2 The Python World",
    "text": "4.2 The Python World\n\nFunction: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).\n\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#standard-library",
    "href": "python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.3 Standard Library",
    "text": "4.3 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#important-libraries",
    "href": "python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.4 Important Libraries",
    "text": "4.4 Important Libraries\n\nNumPy\npandas\nmatplotlib\nIPython/Jupyter\nSciPy\nscikit-learn\nstatsmodels\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n2.0 16.0 0.0 0.0\n\n\narray([1.00445568, 9.95254072, 3.05981058, 6.41210179, 3.48799882,\n       7.85509556, 4.40049181, 9.26293574, 3.78529417, 1.66036129])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\narray([0.09669389, 0.0138209 , 0.09629558, 0.05428184, 0.09306801,\n       0.03416512, 0.0833    , 0.01918402, 0.09028037, 0.09937669])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#writing-a-function",
    "href": "python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.5 Writing a Function",
    "text": "4.5 Writing a Function\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n7.12 µs ± 114 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n2 µs ± 283 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n64.8 µs ± 3.8 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nApparently, the three solutions have very different performance for larger n.\n\n4.5.1 Monty Hall\nHere is a function that performs the Monty Hall experiments.\n\nimport numpy as np\n\ndef montyhall(ndoors, ntrials):\n    doors = np.arange(1, ndoors + 1) / 10\n    prize = np.random.choice(doors, size=ntrials)\n    player = np.random.choice(doors, size=ntrials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(ntrials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(ntrials)])\n    return {'noswitch': np.sum(prize == player), 'switch': np.sum(prize == player2)}\n\nTest it out:\n\nmontyhall(3, 1000)\nmontyhall(4, 1000)\n\n{'noswitch': 270, 'switch': 371}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#variables-versus-objects",
    "href": "python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.6 Variables versus Objects",
    "text": "4.6 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4996899392\n4996899392\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4505841912\n4616240416\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#number-representation",
    "href": "python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.7 Number Representation",
    "text": "4.7 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.7.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows\n\n\n4.7.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "python.html#virtual-environment",
    "href": "python.html#virtual-environment",
    "title": "4  Python Refreshment",
    "section": "4.8 Virtual Environment",
    "text": "4.8 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "5  Communication in Data Science",
    "section": "",
    "text": "5.1 Introduction\nThis chapter was written by Sara Clokey.\nHi! My name is Sara, and I am a junior double majoring in Applied Data Analysis and Communication. The topic of my presentation today, Communication in Data Science, combines my academic and professional interests while underscoring the importance of ‘soft skills’ like public speaking, for example, within STEM fields like data science.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#importance-application-of-communication",
    "href": "communication.html#importance-application-of-communication",
    "title": "5  Communication in Data Science",
    "section": "5.2 Importance & Application of Communication",
    "text": "5.2 Importance & Application of Communication\nData science as a career path has exploded within the last decade. Some fields that offer data science positions include:\n\nFinance\nHealthcare\nMedia production\nSports\nBanking\nInsurance\nE-Commerce\nEnergy\nManufacturing\nTransportation\nConstruction\n\nBecause data science is applicable in so many industries, it is essential that data scientists have the skills and experience to communicate their work with others who do not have the same technical education. As analyzed by Radovilsky et al. (2018), job listings within the field of data science often include qualifications like “strong interpersonal skills” and “demonstrated presentation and communication ability,” highlighting the pervasive need for this skill set.\nWithin these industries, collaboration and teamwork are often at the forefront. Inexperience with data should not prevent your colleagues from being able to contribute to shared projects, and strong communication skills can mitigate this challenge!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#general-communication-skills",
    "href": "communication.html#general-communication-skills",
    "title": "5  Communication in Data Science",
    "section": "5.3 General Communication Skills",
    "text": "5.3 General Communication Skills\n\n5.3.1 Verbal Communication Skills\nVerbal Communication: “The use of sounds and language to relay a message” (Yer, 2018).\nVerbal Communication Tips:\n\nMake a good first impression\nUse appropriate language (jargon, metaphors)\nPrioritize brevity\nPractice beforehand\nAllow room for questions\n\n\n\n5.3.2 Non-Verbal Communication Skills\nNon-Verbal Communication: “Information, emotion, a movement that is expressed without words and without the help of language” (Grillo & Enesi, 2022).\nNon-Verbal Communication Tips:\n\nUtilize vocal variety (pitch, rate, volume)\nAvoid distracting hand and body movements\nMake eye contact\nPay attention to proxemics\n\n\n\n5.3.3 Visual Communication Skills\nVisual Communication: “Any communication that employs one’s sense of sight to deliver a message without the usage of any verbal cues” (Fayaz, 2022).\nVisual Communication Tips:\n\nPrioritize clarity\nUse proper labeling and scaling\nCreate visual contrast (colors, shapes, fonts)\nChoose the most appropriate visual representation\n\n\n\n5.3.4 Written Communication Skills\nWritten Communication: “Any form of communication which is written and documented from the sender to the receiver” (Prabavathi & Nagasubramani, 2018).\nWritten Communication Tips:\n\nClearly state your goal with a thesis statement\nMaintain professionalism (contractions, slang)\nProofread and utilize peer editing\nFollow a specific structure\nBalance concision with analysis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#communication-in-data-science",
    "href": "communication.html#communication-in-data-science",
    "title": "5  Communication in Data Science",
    "section": "5.4 Communication in Data Science",
    "text": "5.4 Communication in Data Science\nOften, data scientists must communicate “technical conclusions to non-technical members”; this may be colleagues in other departments, like marketing, or supervisors at the managerial level. Here are some tips for effectively communicating project results specifically in the field of data science.\n\n5.4.1 Identify your Audience\nWho are you sharing information with? Is it a room of data scientists like this one? Is it full of students who want to learn about data science? Is it a group of executives looking to make a funding decision?\n\nConsider the context and prior knowledge (technical jargon)\nConsider the motivation for listening\n\n\n\n5.4.2 Utilize Data Visualization\nOne of the most effective methods of communicating results in data science, especially to those without technical coding knowledge, is data visualization techniques (Vandemeulebroecke et al., 2019). Python uses the package matplotlib to produce these visualizations, including:\n\nline plots\nbar plots\nbox plots\nhistograms\nheat maps\npie charts\n\nThese visualizations allow complex statistical projects to be simplified into a single graphic, focusing on project results and implications rather than methodology. Ensure that data visualization techniques are free of technical jargon and clearly label all visual aspects.\n\n\n5.4.3 Focus on Data Communication Skills\nThe following skill sets highlight technical data communication that will be more common in projects with other data scientists to communicate about your data.\n\nCoding communication: Python, R, Julia, JavaScript, SQL, etc.\nAnalysis communication: creating a storyline, descriptive versus diagnostic versus predictive analytics, problem identification\nData management: collection, cleaning/transformation, storage\nData visualization\n\n\n\n5.4.4 Give Space for Questions and Feedback\nWithin professional spaces, data scientists should provide time for their clients, supervisors, and colleagues to ask questions about their work and subsequent findings.\n\nPause for questions throughout the presentation\nOffer contact information for continued collaboration\nProvide a structure for anonymous feedback\nSchedule follow-ups if necessary\n\nTeamwork is often at the heart of data science projects within industries, and open communication makes this teamwork run much more smoothly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "communication.html#further-learning",
    "href": "communication.html#further-learning",
    "title": "5  Communication in Data Science",
    "section": "5.5 Further Learning",
    "text": "5.5 Further Learning\nWhile pursuing degrees in the data science field, consider taking Communication courses at UConn that can bolster your understanding and skill set. Some applicable communication courses include:\n\nCOMM 2100: Professional Communication\nCOMM 3110: Organizational Communication\nCOMM 3430: Science Communication\nCOMM 5655: Human-Computer Interaction\nCOMM 5900: Professional Communication\n\nEffective communication also requires practice. Here are some ways to practice these skills while earning your degree:\n\nFully participate in group projects\nSeek presentation opportunities (class, conferences, etc.)\nExplain data science coursework to peers outside of your program\nExplore internship opportunities that involve collaboration with other departments\n\nPragmatic. (2024). Communication skills for data science. https://www.pragmaticinstitute.com/resources/articles/data/communication-skills-for-data-science/.\n\n\n\n\nFayaz, O. (2022). Principles of visual communication.\n\n\nGrillo, H. M., & Enesi, M. (2022). The impact, importance, types, and use of non-verbal communication in social relations. Linguistics and Culture Review, 6, 291–307.\n\n\nPrabavathi, R., & Nagasubramani, P. C. (2018). Effective oral and written communication. Journal of Applied and Advanced Research, 10, 29–32.\n\n\nRadovilsky, Z., Hegde, V., Acharya, A., & Uma, U. (2018). Skills requirements of business data analytics and data science jobs: A comparative analysis. Journal of Supply Chain and Operations Management, 16, 82–101.\n\n\nVandemeulebroecke, M., Baillie, M., Margolskee, A., & Magnusson, B. (2019). Effective visual communication for the quantitative scientist. CPT Pharmacometrics Syst Pharmacol, 10, 705–719.\n\n\nYer, S. (2018). Verbal communication as a two-way process in connecting people. Social Science Research Network.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Communication in Data Science</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nData manipulation is crucial for transforming raw data into a more analyzable format, essential for uncovering patterns and ensuring accurate analysis. This chapter introduces the core techniques for data manipulation in Python, utilizing the Pandas library, a cornerstone for data handling within Python’s data science toolkit.\nPython’s ecosystem is rich with libraries that facilitate not just data manipulation but comprehensive data analysis. Pandas, in particular, provides extensive functionality for data manipulation tasks including reading, cleaning, transforming, and summarizing data. Using real-world datasets, we will explore how to leverage Python for practical data manipulation tasks.\nBy the end of this chapter, you will learn to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#introduction",
    "href": "manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Import/export data from/to diverse sources.\nClean and preprocess data efficiently.\nTransform and aggregate data to derive insights.\nMerge and concatenate datasets from various origins.\nAnalyze real-world datasets using these techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#importexport-data",
    "href": "manipulation.html#importexport-data",
    "title": "6  Data Manipulation",
    "section": "6.2 Import/Export Data",
    "text": "6.2 Import/Export Data\nThis section was written by Deyu Xu, a MS student in Statistics at the time.\n\n6.2.1 Summary\nI would like to divide all of the content into five sections. The fisrt one is exporting data to a .csv file. The second one is importing common formats of data. The third one is importing data from other softwares. The forth one is viewing basic information of the data we have imported. The last one is finding null values.\n\n\n6.2.2 Package Pandas\n\n6.2.2.1 Import data based on Package Pandas\nWe need to use the Package, Pandas provided by Python to import data. The first step is to install the Package, Pandas. Python allows us to install different versions of Pandas. We are able to use the following code to install the common cersion.\n## install the common version of Pandas\npip install pandas\nThe code for installing the latest version is listed.\n## install the latest version of Pandas\npip install --upgrade pandas\nDifferent versions mean there are differnces in the code to achieve the same goal. We will see the specific example in the part of importing .xlsx files.\n\n\n\n6.2.3 Export the data to a .csv file:\n\n6.2.3.1 Import the cleaned crashes data at first\nFisrtly, we need to import the file named “nyccrashes_cleaned.feather” data source. ‌.feather file is a binary file format for storing and sharing data. It is especially suitable for large-scale data analysis and data science workflows. ‌ It uses Apache Arrow’s columnar storage format, which can store data in binary form. The advantage of using this format of file is elvaluating the standard of reading and writing. We need to choose the function read_feather from Pandas to import the crashes data.\n\n## Choose the Package Pandas\nimport pandas as pd\nimport os\n## Import the cleaned crashes data\n## Choose the fcuntion, read_feather from Pandas\n## Add the relative address of the data file to let your cooperator deal with the code smoothly\ndf_feather = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n## Show the top 5 rows of data\n## Determine whether we import the data successfully\nprint(df_feather.head(5)) # Use the fucntion \"head()\"\n\n        crash_datetime    borough  zip_code  latitude  longitude  \\\n0  2024-06-30 17:30:00       None       NaN       NaN        NaN   \n1  2024-06-30 00:32:00       None       NaN       NaN        NaN   \n2  2024-06-30 07:05:00   BROOKLYN   11235.0  40.58106  -73.96744   \n3  2024-06-30 20:47:00  MANHATTAN   10021.0  40.76363  -73.95330   \n4  2024-06-30 10:14:00   BROOKLYN   11222.0  40.73046  -73.95149   \n\n                location     on_street_name      cross_street_name  \\\n0             (0.0, 0.0)               None                   None   \n1                   None  BELT PARKWAY RAMP                   None   \n2  (40.58106, -73.96744)               None                   None   \n3   (40.76363, -73.9533)          FDR DRIVE                   None   \n4  (40.73046, -73.95149)  GREENPOINT AVENUE  MC GUINNESS BOULEVARD   \n\n           off_street_name  number_of_persons_injured  ...  \\\n0              GOLD STREET                          0  ...   \n1                     None                          0  ...   \n2  2797      OCEAN PARKWAY                          0  ...   \n3                     None                          0  ...   \n4                     None                          0  ...   \n\n   contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                    Unspecified                           None   \n1                    Unspecified                           None   \n2                           None                           None   \n3                           None                           None   \n4                    Unspecified                           None   \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                           None                           None       4736746   \n1                           None                           None       4736768   \n2                           None                           None       4737060   \n3                           None                           None       4737510   \n4                           None                           None       4736759   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n2  Station Wagon/Sport Utility Vehicle                                 None   \n3                                Sedan                                 None   \n4                                  Bus                            Box Truck   \n\n  vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                None                None                None  \n1                None                None                None  \n2                None                None                None  \n3                None                None                None  \n4                None                None                None  \n\n[5 rows x 28 columns]\n\n\n\nWe have imported the cleaned crashes data successfully.\nWe utilize the fucntion head(5) to show the top 5 rows of the data.\n\n\n\n6.2.3.2 Export the crashes data to a .csv file\nIt is easy to export the data. The fucntion that helps us to complete this goal is to_csv from Pandas.\n\n## Choose the Package Pandas\n## Choose the function \"to_csv\" from Pandas\n## Use the argument, \"df_feather\" storing the data\n## Export the data to the default working directory\ndf_feather.to_csv(\"foreign/nyccrashes_cleaned.csv\") # Add the name of the CSV file\n\n\nWe can check whether the corresponding .csv file is generated in the default working directory.\n\nWe have exported the data to a .csv file in the default working directory.\nWe will use this .csv file later.\n\n\n\n6.2.4 Import files in common formats: .csv/.xlsx/.txt\n\n6.2.4.1 .csv files\nWe are familiar with .csv files as utilizing them to print some charts by R in the past courses. Now let us import this generated .csv file. We are supposed to choose the function read_csv from Pandas. The following code shows how to import it.\n\n## Choose the Package Pandas\nimport pandas as pd\n## Choose the function \"read_csv\"\n## Add the relative address of the generated CSV file\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\n## Check the data we have imported\n## Use the above function \"head()\" I have introduced\nprint(df_csv.head(2))\n\n   Unnamed: 0       crash_datetime borough  zip_code  latitude  longitude  \\\n0           0  2024-06-30 17:30:00     NaN       NaN       NaN        NaN   \n1           1  2024-06-30 00:32:00     NaN       NaN       NaN        NaN   \n\n     location     on_street_name cross_street_name off_street_name  ...  \\\n0  (0.0, 0.0)                NaN               NaN     GOLD STREET  ...   \n1         NaN  BELT PARKWAY RAMP               NaN             NaN  ...   \n\n   contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                    Unspecified                            NaN   \n1                    Unspecified                            NaN   \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                            NaN                            NaN       4736746   \n1                            NaN                            NaN       4736768   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n   vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                  NaN                 NaN                 NaN  \n1                  NaN                 NaN                 NaN  \n\n[2 rows x 29 columns]\n\n\n\n\n6.2.4.2 .xlsx files\nWe want to import .xlsx files but there are not suitable .xlsx files. We can transfer the CSV file to a .xlsx file by the fucntion to_excel from Pandas. Let’s see how to achieve this goal according to the following code.\n\n## Choose the Package Pandas\nimport pandas as pd\n## Use the function \"to_excel\"\n## Export the data to the default working directory\ndf_csv.to_excel(\"foreign/nyccrashes_cleaned.xlsx\") # Add the name of the Excel file\n\n\nCheck whether the corresponding .xlsx file is generated in the working directory\n\nNow we have generated the .xlsx file covering the same data. And then we can learn how to import .xlsx files. The function wen use is read_excel no matter what Pandas version is.\n\nThe latest version of Pandas corresponds to the following code.\n\n\nimport pandas as pd\n## Choose the function \"read_excel\"\n## Add the command \"engine\" to read the file smoothly\ndf_excel = pd.read_excel(\"foreign/nyccrashes_cleaned.xlsx\", engine = \"openpyxl\")\n## Print top 2 rows of the data\nprint(df_excel.head(2))\n\n   Unnamed: 0.1  Unnamed: 0       crash_datetime borough  zip_code  latitude  \\\n0             0           0  2024-06-30 17:30:00     NaN       NaN       NaN   \n1             1           1  2024-06-30 00:32:00     NaN       NaN       NaN   \n\n   longitude    location     on_street_name cross_street_name  ...  \\\n0        NaN  (0.0, 0.0)                NaN               NaN  ...   \n1        NaN         NaN  BELT PARKWAY RAMP               NaN  ...   \n\n  contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                   Unspecified                            NaN   \n1                   Unspecified                            NaN   \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                            NaN                            NaN       4736746   \n1                            NaN                            NaN       4736768   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n   vehicle_type_code_3  vehicle_type_code_4 vehicle_type_code_5  \n0                  NaN                  NaN                 NaN  \n1                  NaN                  NaN                 NaN  \n\n[2 rows x 30 columns]\n\n\nThe code of the common Pandas version is below. What we need to adjust is to add correct encoding.\ndf_excel = pd.read_excel(\"foreign/nyccrashes_cleaned.xlsx\", engine = \"openpyxl\", \nencoding = \"utf-8\")\n\n\n6.2.4.3 .txt files\nThe last common kind of file is .txt files. We are able to generate the .txt file in the similar way as generatng the .xlsx file. We choose the function to_csv from Pandas. It is necessary to add the command sep=\"\\t\". At the same time, we are supposed to add index=False to avoid the index of Dataframe. The specific code is following.\n\nimport pandas as pd\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\n## Choose the function \"to_csv\"\n## Add the command \"sep='\\t'\"\n## Add the command \"index=False\"\n## Export the data to the default working directory\ndf_csv.to_csv(\"foreign/nyccrashes_cleaned.txt\", sep = \"\\t\", index = False)\n\nNow we get the corresponding .txt file successfully. The next step is to determine the correct encoding of this .txt file. This is because the computer will not read the file successfully without correct encoding. I have listed the code helping us to obtain the correct encoding. We use with statement to deal with data. And then we use the function detect from Package Chardet. The intention of detect is to detect character encoding of text files.\n\nimport chardet\n## Use \"with\" statement \nwith open(\"foreign/nyccrashes_cleaned.txt\", \"rb\") as f:\n    # Execute the command \"open\"\n    # And then assign the result to variable \"f\"\n    raw_data = f.read() # Read the content from \"f\"\n    result = chardet.detect(raw_data) \n    encoding = result[\"encoding\"]\n    print(str(encoding))\n\nascii\n\n\n\nWarning: It is possible to generate the encoding which is not “utf-8”.\n\nNow we own the .txt file and its correct encoding. The last step is to use the function read_table to import the .txt file. We need to insert the correct encoding too. The corresponding code is following.\n\nimport pandas as pd\n## Choose the function \"read_table\"\n## Add the encoding behind the relative address\ndf_txt = pd.read_table(\"foreign/nyccrashes_cleaned.txt\", encoding = \"utf-8\")\n## The defualt of function \"head()\" is top five rows \nprint(df_txt.head())\n\n   Unnamed: 0       crash_datetime    borough  zip_code  latitude  longitude  \\\n0           0  2024-06-30 17:30:00        NaN       NaN       NaN        NaN   \n1           1  2024-06-30 00:32:00        NaN       NaN       NaN        NaN   \n2           2  2024-06-30 07:05:00   BROOKLYN   11235.0  40.58106  -73.96744   \n3           3  2024-06-30 20:47:00  MANHATTAN   10021.0  40.76363  -73.95330   \n4           4  2024-06-30 10:14:00   BROOKLYN   11222.0  40.73046  -73.95149   \n\n                location     on_street_name      cross_street_name  \\\n0             (0.0, 0.0)                NaN                    NaN   \n1                    NaN  BELT PARKWAY RAMP                    NaN   \n2  (40.58106, -73.96744)                NaN                    NaN   \n3   (40.76363, -73.9533)          FDR DRIVE                    NaN   \n4  (40.73046, -73.95149)  GREENPOINT AVENUE  MC GUINNESS BOULEVARD   \n\n           off_street_name  ...  contributing_factor_vehicle_2  \\\n0              GOLD STREET  ...                    Unspecified   \n1                      NaN  ...                    Unspecified   \n2  2797      OCEAN PARKWAY  ...                            NaN   \n3                      NaN  ...                            NaN   \n4                      NaN  ...                    Unspecified   \n\n   contributing_factor_vehicle_3  contributing_factor_vehicle_4  \\\n0                            NaN                            NaN   \n1                            NaN                            NaN   \n2                            NaN                            NaN   \n3                            NaN                            NaN   \n4                            NaN                            NaN   \n\n   contributing_factor_vehicle_5  collision_id  \\\n0                            NaN       4736746   \n1                            NaN       4736768   \n2                            NaN       4737060   \n3                            NaN       4737510   \n4                            NaN       4736759   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n2  Station Wagon/Sport Utility Vehicle                                  NaN   \n3                                Sedan                                  NaN   \n4                                  Bus                            Box Truck   \n\n   vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                  NaN                 NaN                 NaN  \n1                  NaN                 NaN                 NaN  \n2                  NaN                 NaN                 NaN  \n3                  NaN                 NaN                 NaN  \n4                  NaN                 NaN                 NaN  \n\n[5 rows x 29 columns]\n\n\n\n\n\n6.2.5 Import the data from other software\nIn this section, all of the specific files generated by the code I provided are stored in the folder which name is “foreign” and can be accessed according to the relative path.\n\n6.2.5.1 SAS files\n\n6.2.5.1.1 Transfer the .csv file to a .xpt file\nThe reason why we choose .xpt file is to ensure data types remain consistent during conversion. Firstly, we need to process the data to ensure there is no space in the name of columns. If we don’t do that, we will not achieve the goal. The code of dealing with data is following.\nimport pandas as pd\ndf = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\ndf_csv_without_unnamed = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\ndf_csv_without_unnamed.columns=df_csv_without_unnamed.columns.str.replace(\" \", \"_\")\ndf_csv_without_unnamed.to_csv(\"foreign/Without_Unamed.csv\", index=False)\nWe use the Package pyreadstat and the function write_xport from pyreadstat to transfer the .csv file. The corresponding code is following.\nimport pandas as pd\nimport pyreadstat\ndf_without_unnamed = pd.read_csv(\"foreign/Without_Unamed.csv\")\nsas_file = \"foreign/SAS.xpt\"\n## Export the data to the default working directory\npyreadstat.write_xport(df_without_unnamed, \"SAS.xpt\")\n\n\n6.2.5.1.2 Import the generated .xpt file\nWe use the package pyreadstat too. We choose the function read_xport to import the data. Here is the code.\n\nimport pyreadstat\n## Define the Dataframe and metadata\ndf_1, meta = pyreadstat.read_xport(\"foreign/SAS.xpt\")\n## Show the Dataframe\nprint(df_1.head(2))\n## Show the metadata\nprint(meta)\n\n        crash_datetime borough  zip_code  latitude  longitude    location  \\\n0  2024-06-30 17:30:00               NaN       NaN        NaN  (0.0, 0.0)   \n1  2024-06-30 00:32:00               NaN       NaN        NaN               \n\n      on_street_name cross_street_name off_street_name  \\\n0                                          GOLD STREET   \n1  BELT PARKWAY RAMP                                     \n\n   number_of_persons_injured  ...  contributing_factor_vehicle_2  \\\n0                        0.0  ...                    Unspecified   \n1                        0.0  ...                    Unspecified   \n\n   contributing_factor_vehicle_3  contributing_factor_vehicle_4  \\\n0                                                                 \n1                                                                 \n\n   contributing_factor_vehicle_5  collision_id  \\\n0                                    4736746.0   \n1                                    4736768.0   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n  vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                                                              \n1                                                              \n\n[2 rows x 28 columns]\n&lt;pyreadstat._readstat_parser.metadata_container object at 0x115625400&gt;\n\n\n\n\n\n6.2.5.2 rdata files(the suffix of this file is .RData)\n\n6.2.5.2.1 Transfer the .csv file to a .Rdata file\nWe need to install the package rpy2\npip install rpy2\nAnd then we choose the function pandas2ri The following code helps us achieve the goal.\nimport pandas as pd\n## Use the Package rpy2\nimport rpy2.robjects as ro\nfrom rpy2.robjects import pandas2ri\n## Activate conversion between Pandas and R\npandas2ri.activate()\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\n## Transfer the Pandas DataFrame to R DataFrame\ndf_r = pandas2ri.py2rpy(df_csv)\n## Save as .Rdata file\n## Export the data to the default working directory\nro.globalenv[\"R\"] = df_r\nro.r(\"save(R, file = 'foreign/nyccrashes_cleaned.Rdata')\")\nprint(\"The CSV file has been transfered to a .Rdata file successfully.\")\n\nThe error means : 1.Type conversion failure of columns. Some columns were not converted to the correct data typr in r as epected, and were instead coerced to strings. 2.The data is still saved, but there are potential data type issues. 3.These errores will not influence importing the .Rdata file.\nIf you want to perform necessary type cpnversions, the following code is suitable.\n\ndf_csv[\"boroughs\"] = df[\"boroughs\"].astype(str)\n\n\n6.2.5.2.2 Import the generated .Rdata file\nWe also use the Package rpy2. We need the function pandas2ri too. The code is following\n\nimport pandas as pd\nimport rpy2.robjects as ro\n## Load the .Rdata file\nr_file_path = \"foreign/nyccrashes_cleaned.Rdata\"\nro.r[\"load\"](r_file_path)\n## View loaded variables\nloaded_objects = ro.r(\"ls()\")\n## Show the loaded vatiables\nprint(\"Loaded R objects:\", loaded_objects)\n## We have set the name of dataframe as \"R\" above\nr_dataframe = ro.r[\"R\"]\nfrom rpy2.robjects import pandas2ri\n## Transfer R Dataframe to Pandas Dataframe \n## Aim to deal with the data conveniently\npandas2ri.activate()\ndf_2 = pandas2ri.rpy2py(r_dataframe)\nprint(df_2)\n\nLoading custom .RprofileLoaded R objects: [1] \"R\"\n\n      Unnamed: 0       crash_datetime    borough  zip_code   latitude  \\\n0              0  2024-06-30 17:30:00       None       NaN        NaN   \n1              1  2024-06-30 00:32:00       None       NaN        NaN   \n2              2  2024-06-30 07:05:00   BROOKLYN   11235.0  40.581060   \n3              3  2024-06-30 20:47:00  MANHATTAN   10021.0  40.763630   \n4              4  2024-06-30 10:14:00   BROOKLYN   11222.0  40.730460   \n...          ...                  ...        ...       ...        ...   \n1870        1870  2024-07-07 21:25:00      BRONX   10457.0  40.852520   \n1871        1871  2024-07-07 10:31:00      BRONX   10460.0  40.843945   \n1872        1872  2024-07-07 20:15:00     QUEENS   11436.0  40.677982   \n1873        1873  2024-07-07 14:45:00      BRONX   10452.0  40.843822   \n1874        1874  2024-07-07 14:12:00      BRONX   10468.0  40.861084   \n\n      longitude                 location           on_street_name  \\\n0           NaN               (0.0, 0.0)                     None   \n1           NaN                     None        BELT PARKWAY RAMP   \n2    -73.967440    (40.58106, -73.96744)                     None   \n3    -73.953300     (40.76363, -73.9533)                FDR DRIVE   \n4    -73.951490    (40.73046, -73.95149)        GREENPOINT AVENUE   \n...         ...                      ...                      ...   \n1870 -73.900020    (40.85252, -73.90002)          EAST 180 STREET   \n1871 -73.885800    (40.843945, -73.8858)                     None   \n1872 -73.791214  (40.677982, -73.791214)        SUTPHIN BOULEVARD   \n1873 -73.927500    (40.843822, -73.9275)  MAJOR DEEGAN EXPRESSWAY   \n1874 -73.911490   (40.861084, -73.91149)                     None   \n\n          cross_street_name            off_street_name  ...  \\\n0                      None                GOLD STREET  ...   \n1                      None                       None  ...   \n2                      None    2797      OCEAN PARKWAY  ...   \n3                      None                       None  ...   \n4     MC GUINNESS BOULEVARD                       None  ...   \n...                     ...                        ...  ...   \n1870                   None                       None  ...   \n1871                   None  855       EAST 178 STREET  ...   \n1872             120 AVENUE                       None  ...   \n1873                   None                       None  ...   \n1874                   None    2258      HAMPDEN PLACE  ...   \n\n      contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                       Unspecified                           None   \n1                       Unspecified                           None   \n2                              None                           None   \n3                              None                           None   \n4                       Unspecified                           None   \n...                             ...                            ...   \n1870                    Unspecified                           None   \n1871                    Unspecified                           None   \n1872                    Unspecified                           None   \n1873                    Unspecified                           None   \n1874                           None                           None   \n\n      contributing_factor_vehicle_4  contributing_factor_vehicle_5  \\\n0                              None                           None   \n1                              None                           None   \n2                              None                           None   \n3                              None                           None   \n4                              None                           None   \n...                             ...                            ...   \n1870                           None                           None   \n1871                           None                           None   \n1872                           None                           None   \n1873                           None                           None   \n1874                           None                           None   \n\n      collision_id                  vehicle_type_code_1  \\\n0          4736746                                Sedan   \n1          4736768  Station Wagon/Sport Utility Vehicle   \n2          4737060  Station Wagon/Sport Utility Vehicle   \n3          4737510                                Sedan   \n4          4736759                                  Bus   \n...            ...                                  ...   \n1870       4744144                        Pick-up Truck   \n1871       4744576  Station Wagon/Sport Utility Vehicle   \n1872       4745391                                Sedan   \n1873       4746540                                Sedan   \n1874       4746320                                Sedan   \n\n                      vehicle_type_code_2  vehicle_type_code_3  \\\n0                                   Sedan                 None   \n1     Station Wagon/Sport Utility Vehicle                 None   \n2                                    None                 None   \n3                                    None                 None   \n4                               Box Truck                 None   \n...                                   ...                  ...   \n1870                                Sedan                 None   \n1871                                 None                 None   \n1872                                Sedan                 None   \n1873                                Sedan                 None   \n1874                                 None                 None   \n\n     vehicle_type_code_4 vehicle_type_code_5  \n0                   None                None  \n1                   None                None  \n2                   None                None  \n3                   None                None  \n4                   None                None  \n...                  ...                 ...  \n1870                None                None  \n1871                None                None  \n1872                None                None  \n1873                None                None  \n1874                None                None  \n\n[1875 rows x 29 columns]\n\n\n\n\n\n6.2.5.3 stata data(the suffix of this file is .dta)\n\n6.2.5.3.1 Transfer the .csv file to a .dta file\nWe can only use Pandas. We choose the fucntion to_stata to save the .dta file.\nimport pandas as pd\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\n## Export the data to the default working directory\ndf_csv.to_stata(\"foreign/stata.dta\")\n\n\n6.2.5.3.2 Import the .dta file\nWe use the function read_stata from Pandas. And here is the specific code.\n\nimport pandas as pd\ndf_3 = pd.read_stata(\"foreign/stata.dta\")\nprint(df_3.head(2))\n\n   index  Unnamed__0       crash_datetime borough  zip_code  latitude  \\\n0      0           0  2024-06-30 17:30:00               NaN       NaN   \n1      1           1  2024-06-30 00:32:00               NaN       NaN   \n\n   longitude    location     on_street_name cross_street_name  ...  \\\n0        NaN  (0.0, 0.0)                                       ...   \n1        NaN              BELT PARKWAY RAMP                    ...   \n\n  contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                   Unspecified                                  \n1                   Unspecified                                  \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                                                                     4736746   \n1                                                                     4736768   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n   vehicle_type_code_3  vehicle_type_code_4 vehicle_type_code_5  \n0                                                                \n1                                                                \n\n[2 rows x 30 columns]\n\n\n\n\n\n6.2.5.4 spss data(the suffix of this file is .sav)\n\n6.2.5.4.1 Transfer the .csv file to a .sav file\nWe need to use the Package pyreadstat We choose the function write_sav form pyreadstat We are supposed to use the CSV file which is without space. We can uese the following code.\nimport pandas as pd\nimport pyreadstat\ndf_csv = pd.read_csv(\"foreign/Without_Unamed.csv\")\noutput_file = os.path.join(\"foreign\", \"SPSS.sav\")\n## Export the data to the default working directory\npyreadstat.write_sav(df_csv, output_file)\n\n\n6.2.5.4.2 Import the generated .sav file\nWe also use the Package pyreadstat. We utilize the function read_sav from pyreadstat. The following code helps us import the .sav file.\n\nimport pandas as pd\nimport pyreadstat\ndf_4, meta = pyreadstat.read_sav(\"foreign/SPSS.sav\")\nprint(df_4.head(2))\nprint(meta)\n\n        crash_datetime borough  zip_code  latitude  longitude    location  \\\n0  2024-06-30 17:30:00               NaN       NaN        NaN  (0.0, 0.0)   \n1  2024-06-30 00:32:00               NaN       NaN        NaN               \n\n      on_street_name cross_street_name off_street_name  \\\n0                                          GOLD STREET   \n1  BELT PARKWAY RAMP                                     \n\n   number_of_persons_injured  ...  contributing_factor_vehicle_2  \\\n0                        0.0  ...                    Unspecified   \n1                        0.0  ...                    Unspecified   \n\n   contributing_factor_vehicle_3  contributing_factor_vehicle_4  \\\n0                                                                 \n1                                                                 \n\n   contributing_factor_vehicle_5  collision_id  \\\n0                                    4736746.0   \n1                                    4736768.0   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n  vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                                                              \n1                                                              \n\n[2 rows x 28 columns]\n&lt;pyreadstat._readstat_parser.metadata_container object at 0x118b65af0&gt;\n\n\n\n\n\n6.2.5.5 Matlab files(the suffix of this file is .mat)\n\n6.2.5.5.1 Transfer the .csv file to a .mat file\nWe need to install the package scipy.io\npip install scipy\nAnd then we choose the function savemat from scipy. The specific code is following.\nimport pandas as pd\nfrom scipy.io import savemat\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\n## Convert DataFrame to dicotionary form\n## MATLAB.mat require dictionary format\ndata_dict = {\"data\": df_csv.to_dict(\"list\")}\n## Save the dictionary as a .mat file\noutput_file = os.path.join(\"foreign\", \"MATLAB.mat\")\n## Export the data to the default working directory\nsavemat(output_file, data_dict)\n\n\n6.2.5.5.2 Import the generated .mat file\nWe use the Package scipy.io too. We choose the function loadmat from spicy.io And the corresponding code is following.\n\nimport pandas as pd\nfrom scipy.io import loadmat\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\ndf_5 = loadmat(\"foreign/MATLAB.mat\")\n## Show the data keys\nprint(df_5.keys())\n## Show the contents of the \"data\" keys\nprint(df_5[\"data\"])\n\ndict_keys(['__header__', '__version__', '__globals__', 'data'])\n[[(array([[   0,    1,    2, ..., 1872, 1873, 1874]]), array(['2024-06-30 17:30:00', '2024-06-30 00:32:00',\n         '2024-06-30 07:05:00', ..., '2024-07-07 20:15:00',\n         '2024-07-07 14:45:00', '2024-07-07 14:12:00'], dtype='&lt;U19'), array(['nan                             ',\n         'nan                             ',\n         'BROOKLYN                        ', ...,\n         'QUEENS                          ',\n         'BRONX                           ',\n         'BRONX                           '], dtype='&lt;U32'), array([[   nan,    nan, 11235., ..., 11436., 10452., 10468.]]), array([[      nan,       nan, 40.58106 , ..., 40.677982, 40.843822,\n          40.861084]]), array([[       nan,        nan, -73.96744 , ..., -73.791214, -73.9275  ,\n          -73.91149 ]]), array(['(0.0, 0.0)                      ',\n         'nan                             ',\n         '(40.58106, -73.96744)           ', ...,\n         '(40.677982, -73.791214)         ',\n         '(40.843822, -73.9275)           ',\n         '(40.861084, -73.91149)          '], dtype='&lt;U32'), array(['nan                             ',\n         'BELT PARKWAY RAMP               ',\n         'nan                             ', ...,\n         'SUTPHIN BOULEVARD               ',\n         'MAJOR DEEGAN EXPRESSWAY         ',\n         'nan                             '], dtype='&lt;U32'), array(['nan                             ',\n         'nan                             ',\n         'nan                             ', ...,\n         '120 AVENUE                      ',\n         'nan                             ',\n         'nan                             '], dtype='&lt;U32'), array(['GOLD STREET                        ',\n         'nan                                ',\n         '2797      OCEAN PARKWAY            ', ...,\n         'nan                                ',\n         'nan                                ',\n         '2258      HAMPDEN PLACE            '], dtype='&lt;U35'), array([[0, 0, 0, ..., 1, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array([[0, 0, 0, ..., 1, 0, 0]]), array([[0, 0, 0, ..., 0, 0, 0]]), array(['Passing Too Closely                                  ',\n         'Unspecified                                          ',\n         'Unspecified                                          ', ...,\n         'Passing or Lane Usage Improper                       ',\n         'Driver Inexperience                                  ',\n         'Unspecified                                          '],\n        dtype='&lt;U53'), array(['Unspecified                                          ',\n         'Unspecified                                          ',\n         'nan                                                  ', ...,\n         'Unspecified                                          ',\n         'Unspecified                                          ',\n         'nan                                                  '],\n        dtype='&lt;U53'), array(['nan                             ',\n         'nan                             ',\n         'nan                             ', ...,\n         'nan                             ',\n         'nan                             ',\n         'nan                             '], dtype='&lt;U32'), array(['nan                             ',\n         'nan                             ',\n         'nan                             ', ...,\n         'nan                             ',\n         'nan                             ',\n         'nan                             '], dtype='&lt;U32'), array(['nan                             ',\n         'nan                             ',\n         'nan                             ', ...,\n         'nan                             ',\n         'nan                             ',\n         'nan                             '], dtype='&lt;U32'), array([[4736746, 4736768, 4737060, ..., 4745391, 4746540, 4746320]]), array(['Sedan                              ',\n         'Station Wagon/Sport Utility Vehicle',\n         'Station Wagon/Sport Utility Vehicle', ...,\n         'Sedan                              ',\n         'Sedan                              ',\n         'Sedan                              '], dtype='&lt;U35'), array(['Sedan                              ',\n         'Station Wagon/Sport Utility Vehicle',\n         'nan                                ', ...,\n         'Sedan                              ',\n         'Sedan                              ',\n         'nan                                '], dtype='&lt;U35'), array(['nan                                ',\n         'nan                                ',\n         'nan                                ', ...,\n         'nan                                ',\n         'nan                                ',\n         'nan                                '], dtype='&lt;U35'), array(['nan                                ',\n         'nan                                ',\n         'nan                                ', ...,\n         'nan                                ',\n         'nan                                ',\n         'nan                                '], dtype='&lt;U35'), array(['nan                                ',\n         'nan                                ',\n         'nan                                ', ...,\n         'nan                                ',\n         'nan                                ',\n         'nan                                '], dtype='&lt;U35'))                                                                                                                                                                                                                                                                                                                                                ]]\n\n\n\n\n\n6.2.5.6 HDF5 files(the suffix of this file is .h5)\n\n6.2.5.6.1 Transfer the .csv file to a .h5 file\nWe can only use Pandas. At the same time, the function to_hdf helps us achive the goal. The code is following.\nimport pandas as pd\nimport tables\ndf_csv = pd.read_csv(\"foreign/nyccrashes_cleaned.csv\")\noutput_file = os.path.join( \"foreign\", \"HDF5.h5\")\n## Export the data to the default working directory\ndf_csv.to_hdf(output_file, key = \"data\", mode = \"w\")\n\n\n6.2.5.6.2 Import the generated .h5 file\nWe only use Pandas too. We need the function read_h5. The code of importing .h5 file is following.\n\nimport pandas as pd\ndf_6 = pd.read_hdf(\"foreign/HDF5.h5\", key = \"data\")\nprint(df_6.head(2))\n\n   Unnamed: 0       crash_datetime borough  zip_code  latitude  longitude  \\\n0           0  2024-06-30 17:30:00     NaN       NaN       NaN        NaN   \n1           1  2024-06-30 00:32:00     NaN       NaN       NaN        NaN   \n\n     location     on_street_name cross_street_name off_street_name  ...  \\\n0  (0.0, 0.0)                NaN               NaN     GOLD STREET  ...   \n1         NaN  BELT PARKWAY RAMP               NaN             NaN  ...   \n\n   contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                    Unspecified                            NaN   \n1                    Unspecified                            NaN   \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                            NaN                            NaN       4736746   \n1                            NaN                            NaN       4736768   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n   vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                  NaN                 NaN                 NaN  \n1                  NaN                 NaN                 NaN  \n\n[2 rows x 29 columns]\n\n\n\n\n\n6.2.5.7 Import multiple files and merge them into a new file\nI have introduced the method of importing single file of data. Python also allows us to import multiple files simultaneously. We choose the Package glob and Package Pandas\n## Install Package Glob\npip install glob\nThe effect of Package glob is to find files and directories that match the specified pattern. We use the function glob from Package glob. The intention of function glob is to find all file paths that match a specitic pattern and return a list of file paths. The following fucntion is the corresponding code.\n\n## Use the package globe and the package pandas\nimport glob \nimport pandas as pd\n## Merge multiple arrays\n## * means match any number of characters( including the null characters)\nall_files = glob.glob(\"foreign/*.csv\") \n## Create a list to store the data\nall_data = []\n## Use \"for\" statement to import all of the csv files\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    all_data.append(df)\n## Combine multiple pandas objects into one along a fixed axis using some merging methods\ndata_merge = pd.concat(all_data, axis=0, ignore_index=True)\n## Check the result\nprint(data_merge.head(2))\n\n   Unnamed: 0       crash_datetime borough  zip_code  latitude  longitude  \\\n0         0.0  2024-06-30 17:30:00     NaN       NaN       NaN        NaN   \n1         1.0  2024-06-30 00:32:00     NaN       NaN       NaN        NaN   \n\n     location     on_street_name cross_street_name off_street_name  ...  \\\n0  (0.0, 0.0)                NaN               NaN     GOLD STREET  ...   \n1         NaN  BELT PARKWAY RAMP               NaN             NaN  ...   \n\n   contributing_factor_vehicle_2  contributing_factor_vehicle_3  \\\n0                    Unspecified                            NaN   \n1                    Unspecified                            NaN   \n\n   contributing_factor_vehicle_4  contributing_factor_vehicle_5  collision_id  \\\n0                            NaN                            NaN       4736746   \n1                            NaN                            NaN       4736768   \n\n                   vehicle_type_code_1                  vehicle_type_code_2  \\\n0                                Sedan                                Sedan   \n1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n\n   vehicle_type_code_3 vehicle_type_code_4 vehicle_type_code_5  \n0                  NaN                 NaN                 NaN  \n1                  NaN                 NaN                 NaN  \n\n[2 rows x 29 columns]\n\n\n\n\n\n6.2.6 View data information\nIt is natural for us to be interested in the fundamental information of the data we have imported. As a result, I have listed some useful functions to get the basic knowledge of the data.\nThe following code helps us know how much the data is. We choose the basic function shape from pandas.\n\n## How much is the crashes data is\ndf_csv.shape\n\n(1875, 29)\n\n\n\nThere are 1875 data and 29 columns in the file.\n\nThe following code helps us check the type of each variable in data. The fucntion is dtypes from Pandas\n\n## Show all types of the crashes' variables\ndf_csv.dtypes\n\nUnnamed: 0                         int64\ncrash_datetime                    object\nborough                           object\nzip_code                         float64\nlatitude                         float64\nlongitude                        float64\nlocation                          object\non_street_name                    object\ncross_street_name                 object\noff_street_name                   object\nnumber_of_persons_injured          int64\nnumber_of_persons_killed           int64\nnumber_of_pedestrians_injured      int64\nnumber_of_pedestrians_killed       int64\nnumber_of_cyclist_injured          int64\nnumber_of_cyclist_killed           int64\nnumber_of_motorist_injured         int64\nnumber_of_motorist_killed          int64\ncontributing_factor_vehicle_1     object\ncontributing_factor_vehicle_2     object\ncontributing_factor_vehicle_3     object\ncontributing_factor_vehicle_4     object\ncontributing_factor_vehicle_5     object\ncollision_id                       int64\nvehicle_type_code_1               object\nvehicle_type_code_2               object\nvehicle_type_code_3               object\nvehicle_type_code_4               object\nvehicle_type_code_5               object\ndtype: object\n\n\n\nOur computer has listed 29 variables and their corresponding types.\n\nThe following code is suitable for viewing overall data information. We use the basic function info from Pandas\n\ndf_csv.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1875 entries, 0 to 1874\nData columns (total 29 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   Unnamed: 0                     1875 non-null   int64  \n 1   crash_datetime                 1875 non-null   object \n 2   borough                        1749 non-null   object \n 3   zip_code                       1749 non-null   float64\n 4   latitude                       1722 non-null   float64\n 5   longitude                      1722 non-null   float64\n 6   location                       1729 non-null   object \n 7   on_street_name                 1329 non-null   object \n 8   cross_street_name              943 non-null    object \n 9   off_street_name                546 non-null    object \n 10  number_of_persons_injured      1875 non-null   int64  \n 11  number_of_persons_killed       1875 non-null   int64  \n 12  number_of_pedestrians_injured  1875 non-null   int64  \n 13  number_of_pedestrians_killed   1875 non-null   int64  \n 14  number_of_cyclist_injured      1875 non-null   int64  \n 15  number_of_cyclist_killed       1875 non-null   int64  \n 16  number_of_motorist_injured     1875 non-null   int64  \n 17  number_of_motorist_killed      1875 non-null   int64  \n 18  contributing_factor_vehicle_1  1864 non-null   object \n 19  contributing_factor_vehicle_2  1425 non-null   object \n 20  contributing_factor_vehicle_3  174 non-null    object \n 21  contributing_factor_vehicle_4  52 non-null     object \n 22  contributing_factor_vehicle_5  14 non-null     object \n 23  collision_id                   1875 non-null   int64  \n 24  vehicle_type_code_1            1842 non-null   object \n 25  vehicle_type_code_2            1230 non-null   object \n 26  vehicle_type_code_3            162 non-null    object \n 27  vehicle_type_code_4            48 non-null     object \n 28  vehicle_type_code_5            14 non-null     object \ndtypes: float64(3), int64(10), object(16)\nmemory usage: 424.9+ KB\n\n\n\nThe basic information of crashes data has been listed.\n\nThe function describe from Pandas helps generate descriptive statistics.\n\n## Show the basic descriptive statistics of crashes data\ndf_csv.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nzip_code\nlatitude\nlongitude\nnumber_of_persons_injured\nnumber_of_persons_killed\nnumber_of_pedestrians_injured\nnumber_of_pedestrians_killed\nnumber_of_cyclist_injured\nnumber_of_cyclist_killed\nnumber_of_motorist_injured\nnumber_of_motorist_killed\ncollision_id\n\n\n\n\ncount\n1875.000000\n1749.000000\n1722.000000\n1722.000000\n1875.000000\n1875.000000\n1875.000000\n1875.000000\n1875.000000\n1875.0\n1875.000000\n1875.000000\n1.875000e+03\n\n\nmean\n937.000000\n10892.563179\n40.719287\n-73.919898\n0.617067\n0.004267\n0.093333\n0.002667\n0.065067\n0.0\n0.433067\n0.001600\n4.738587e+06\n\n\nstd\n541.410196\n525.579066\n0.081315\n0.085191\n0.915610\n0.103219\n0.338452\n0.095207\n0.246709\n0.0\n0.891185\n0.039979\n1.659947e+03\n\n\nmin\n0.000000\n10000.000000\n40.513510\n-74.237366\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.736561e+06\n\n\n25%\n468.500000\n10455.000000\n40.662752\n-73.968543\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.737666e+06\n\n\n50%\n937.000000\n11208.000000\n40.712778\n-73.922933\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n4.738257e+06\n\n\n75%\n1405.500000\n11239.000000\n40.767641\n-73.869405\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n1.000000\n0.000000\n4.738883e+06\n\n\nmax\n1874.000000\n11694.000000\n40.907246\n-73.702190\n11.000000\n4.000000\n7.000000\n4.000000\n1.000000\n0.0\n11.000000\n1.000000\n4.752444e+06\n\n\n\n\n\n\n\nIf we want to summarize the names of all columns, it is a good choice to use the function columns from Pandas\n\n## Get all columns' names of crashes data\ndf_csv.columns\n\nIndex(['Unnamed: 0', 'crash_datetime', 'borough', 'zip_code', 'latitude',\n       'longitude', 'location', 'on_street_name', 'cross_street_name',\n       'off_street_name', 'number_of_persons_injured',\n       'number_of_persons_killed', 'number_of_pedestrians_injured',\n       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n       'number_of_cyclist_killed', 'number_of_motorist_injured',\n       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5'],\n      dtype='object')\n\n\nThe function tail from Pandas allow us to view the last rows.\n\n## Show the last 2 rows of crashes data\ndf_csv.tail(n = 2)\n\n\n\n\n\n\n\n\nUnnamed: 0\ncrash_datetime\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n1873\n1873\n2024-07-07 14:45:00\nBRONX\n10452.0\n40.843822\n-73.92750\n(40.843822, -73.9275)\nMAJOR DEEGAN EXPRESSWAY\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4746540\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1874\n1874\n2024-07-07 14:12:00\nBRONX\n10468.0\n40.861084\n-73.91149\n(40.861084, -73.91149)\nNaN\nNaN\n2258 HAMPDEN PLACE\n...\nNaN\nNaN\nNaN\nNaN\n4746320\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2 rows × 29 columns\n\n\n\nThe function unique from Pandas dedicates in unique values ​​of one column.\n\n## Show the unique values in the column named \"crash_datetime\"\ndf_csv[\"crash_datetime\"].unique()\n\narray(['2024-06-30 17:30:00', '2024-06-30 00:32:00',\n       '2024-06-30 07:05:00', ..., '2024-07-07 09:13:00',\n       '2024-07-07 10:31:00', '2024-07-07 14:12:00'], dtype=object)\n\n\nWe can get the values of one column(without deduplication). The choose of function is values from Pandas instead of unique\n\n## Show all values of the column named \"crash_datetime\"\ndf_csv[\"crash_datetime\"].values\n\narray(['2024-06-30 17:30:00', '2024-06-30 00:32:00',\n       '2024-06-30 07:05:00', ..., '2024-07-07 20:15:00',\n       '2024-07-07 14:45:00', '2024-07-07 14:12:00'], dtype=object)\n\n\n\n\n6.2.7 Find Null Values\nIt is necessary for us to find null values before we clean and preprocess the data. The following content covers how to find null values.\n\n6.2.7.1 Determine whether there are missing values.\nWe need to use the function isnull firstly. The aim is to detect missing values in data. And then we add the command any to determine whether there are missing values.\n\nDetermine whether there are missing values in columns\n\n\n## Determine whether there are missing values in columns of crashes data\ndf_csv.isnull().any(axis = 0) # \"axis=0\" means columns\n\nUnnamed: 0                       False\ncrash_datetime                   False\nborough                           True\nzip_code                          True\nlatitude                          True\nlongitude                         True\nlocation                          True\non_street_name                    True\ncross_street_name                 True\noff_street_name                   True\nnumber_of_persons_injured        False\nnumber_of_persons_killed         False\nnumber_of_pedestrians_injured    False\nnumber_of_pedestrians_killed     False\nnumber_of_cyclist_injured        False\nnumber_of_cyclist_killed         False\nnumber_of_motorist_injured       False\nnumber_of_motorist_killed        False\ncontributing_factor_vehicle_1     True\ncontributing_factor_vehicle_2     True\ncontributing_factor_vehicle_3     True\ncontributing_factor_vehicle_4     True\ncontributing_factor_vehicle_5     True\ncollision_id                     False\nvehicle_type_code_1               True\nvehicle_type_code_2               True\nvehicle_type_code_3               True\nvehicle_type_code_4               True\nvehicle_type_code_5               True\ndtype: bool\n\n\n\nDetermine whether there are missing values ​​in rows.\n\n\n## Determine whether there are missing values in rows of crashes data\ndf_csv.isnull().any(axis = 1) # \"axis=1\" means rows\n\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n1870    True\n1871    True\n1872    True\n1873    True\n1874    True\nLength: 1875, dtype: bool\n\n\n\n\n6.2.7.2 Locate the missing values of rows/columns\nWe utilize the function loc from Pandas. The function of loc selects or modifies data in a DataFrame or Series. The slection and modification are based on labels.\n\nLocate the missing values of rows\n\n\n## Locate the missing values in crashes data rows\ndf_csv.loc[df_csv.isnull().any(axis = 1)]\n\n\n\n\n\n\n\n\nUnnamed: 0\ncrash_datetime\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n0\n2024-06-30 17:30:00\nNaN\nNaN\nNaN\nNaN\n(0.0, 0.0)\nNaN\nNaN\nGOLD STREET\n...\nUnspecified\nNaN\nNaN\nNaN\n4736746\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1\n1\n2024-06-30 00:32:00\nNaN\nNaN\nNaN\nNaN\nNaN\nBELT PARKWAY RAMP\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n2\n2\n2024-06-30 07:05:00\nBROOKLYN\n11235.0\n40.581060\n-73.967440\n(40.58106, -73.96744)\nNaN\nNaN\n2797 OCEAN PARKWAY\n...\nNaN\nNaN\nNaN\nNaN\n4737060\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n3\n3\n2024-06-30 20:47:00\nMANHATTAN\n10021.0\n40.763630\n-73.953300\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n4\n4\n2024-06-30 10:14:00\nBROOKLYN\n11222.0\n40.730460\n-73.951490\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736759\nBus\nBox Truck\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1870\n1870\n2024-07-07 21:25:00\nBRONX\n10457.0\n40.852520\n-73.900020\n(40.85252, -73.90002)\nEAST 180 STREET\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4744144\nPick-up Truck\nSedan\nNaN\nNaN\nNaN\n\n\n1871\n1871\n2024-07-07 10:31:00\nBRONX\n10460.0\n40.843945\n-73.885800\n(40.843945, -73.8858)\nNaN\nNaN\n855 EAST 178 STREET\n...\nUnspecified\nNaN\nNaN\nNaN\n4744576\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n1872\n1872\n2024-07-07 20:15:00\nQUEENS\n11436.0\n40.677982\n-73.791214\n(40.677982, -73.791214)\nSUTPHIN BOULEVARD\n120 AVENUE\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4745391\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1873\n1873\n2024-07-07 14:45:00\nBRONX\n10452.0\n40.843822\n-73.927500\n(40.843822, -73.9275)\nMAJOR DEEGAN EXPRESSWAY\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4746540\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1874\n1874\n2024-07-07 14:12:00\nBRONX\n10468.0\n40.861084\n-73.911490\n(40.861084, -73.91149)\nNaN\nNaN\n2258 HAMPDEN PLACE\n...\nNaN\nNaN\nNaN\nNaN\n4746320\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n1875 rows × 29 columns\n\n\n\n\n\n6.2.7.3 Determine the number of missing values.\nWe also use the function isnull. But this time we add the command sum rather than any.\n\n## Calculate the number of missing values in crashes data columns\ndf_csv.isnull().sum(axis = 0)\n\nUnnamed: 0                          0\ncrash_datetime                      0\nborough                           126\nzip_code                          126\nlatitude                          153\nlongitude                         153\nlocation                          146\non_street_name                    546\ncross_street_name                 932\noff_street_name                  1329\nnumber_of_persons_injured           0\nnumber_of_persons_killed            0\nnumber_of_pedestrians_injured       0\nnumber_of_pedestrians_killed        0\nnumber_of_cyclist_injured           0\nnumber_of_cyclist_killed            0\nnumber_of_motorist_injured          0\nnumber_of_motorist_killed           0\ncontributing_factor_vehicle_1      11\ncontributing_factor_vehicle_2     450\ncontributing_factor_vehicle_3    1701\ncontributing_factor_vehicle_4    1823\ncontributing_factor_vehicle_5    1861\ncollision_id                        0\nvehicle_type_code_1                33\nvehicle_type_code_2               645\nvehicle_type_code_3              1713\nvehicle_type_code_4              1827\nvehicle_type_code_5              1861\ndtype: int64",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#sql",
    "href": "manipulation.html#sql",
    "title": "6  Data Manipulation",
    "section": "6.3 SQL",
    "text": "6.3 SQL\nThis section was written by Thea Johnson, a senior in Statistics at the time.\n\n6.3.1 Table of Contents\n\nWhat is a Database?\nWhat is SQL\nCRUD Model\nCreating Tables with SQL\nInserting into SQL Tables\nUpdating/Deleting SQL Tables\nUsing SQL to work with NYC Open Data\nQueries\n\n\n\n6.3.2 What is a Database?\n\nCollection of related information\nEx: Phone books, grocery list, student records\nRelational and non-relational\nRelational databases are structured into columns and rows\nEach row represents an observation and each column represents an attribute\nA key uniquely identifies each row\nTrying to input repeat keys causes an error\n\n\n\n6.3.3 Example Relational Database\n\nimport sqlite3\nimport pandas as pd\n\n# creates a database file called phonebook.db and lets you connect\nconnection = sqlite3.connect(\"phonebook.db\")\n\n# created a cursor object lets you use the cursor function from SQlite3 module\ncursor = connection.cursor()\n\ncursor.execute(\"DROP TABLE IF EXISTS phonebook\")\n\n# Execute function allows you to send commands in the SQL language as strings\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS phonebook (\n    phonebook_id INTEGER PRIMARY KEY,\n    name TEXT,\n    phone_num TEXT UNIQUE,\n    address TEXT\n)\n\"\"\")\n\ncursor.execute(\"INSERT INTO phonebook VALUES (1, 'Greta Colic', '2035452367', '1 Hartsdale road')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(2, 'Carlos Alavarez', '9145652761', '13 Porter street')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(3, 'Marin Yanko', '5753917568', '110 Ocean avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(4, 'Mira Watson', '9146522761', '12 Hindmarsh avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(5, 'Harry Smith', '2036658279', '180 Wallace road')\")\n\nconnection.commit()\n \n\n# alternative way of displaying output\n\"\"\"cursor.execute(\"SELECT * FROM phonebook\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\"\"\"\n \noutput = pd.read_sql_query(\"SELECT * FROM phonebook\", connection)\nprint()\nprint()\nprint(output)\n\nconnection.close()\n\n\n\n   phonebook_id             name   phone_num              address\n0             1      Greta Colic  2035452367     1 Hartsdale road\n1             2  Carlos Alavarez  9145652761     13 Porter street\n2             3      Marin Yanko  5753917568     110 Ocean avenue\n3             4      Mira Watson  9146522761  12 Hindmarsh avenue\n4             5      Harry Smith  2036658279     180 Wallace road\n\n\n\n\n6.3.4 What is SQL?\n\nStructured Query Language\nAllows users to interact with databases to store and retrieve data\nThe essential operations follow the CRUD acronym\n\n\n\n6.3.5 CRUD\n\nCreate, read, update and delete\nEssential operations for SQL to manage a database\nCreate: adds a new record (row) to a database with unique attributes\nRead: Returns records based on specified search criteria\nUpdate: Allows you to change attribute(s) of the record\nDelete: Allows you to remove records from the database\n\n\n\n6.3.6 Implementing SQL Through Python to Create a Table\nimport sqlite3\n\n# creates a database file called phonebook.db and lets you connect\nconnection = sqlite3.connect(\"phonebook.db\")\n\n# creates a cursor object using the cursor function from SQlite3 module\ncursor = connection.cursor()\n\n\"\"\"\nCREATE TABLE tablename\n(\n    attribute1 datatype PRIMARY KEY,\n    attribute2 datatype,\n    attribute3 datatype\n);\n\"\"\"\n# You MUST include a primary key to uniquely identify entries\n\n# Execute function allows you to send commands in the SQL language as strings\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS phonebook (\n    phonebook_id INTEGER PRIMARY KEY,\n    name TEXT NOT NULL,\n    phone_num TEXT UNIQUE,\n    address TEXT\n)\n\"\"\")\nconnection.commit()\nconnection.close()\n\n\n6.3.7 How to Insert into the SQL table?\nimport sqlite3\nimport pandas as pd\n\n# Connects the the previously created phonebook.db\nconnection = sqlite3.connect(\"phonebook.db\")\n\n# creates a cursor object using the cursor function from SQlite3 module\ncursor = connection.cursor()\n\n\"\"\"\nINSERT INTO database VALUES(\"value1\", \"value2\", \"value3\");\n\n\"\"\"\n\ncursor.execute(\"INSERT INTO phonebook VALUES (1, 'Greta Colic', '2035452367', '1 Hartsdale road')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(2, 'Carlos Alavarez', '9145652761', '13 Porter street')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(3, 'Marin Yanko', '5753917568', '110 Ocean avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(4, 'Mira Watson', '9146522761', '12 Hindmarsh avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(5, 'Harry Smith', '2036658279', '180 Wallace road')\")\n\n# How to input data if there's a missing value?\n\"\"\"\nINSERT INTO database(attribute1, attribute2) VALUES(val1, val2);\n\"\"\"\n# only works if the missing value is not a primary key\ncursor.execute(\"INSERT INTO phonebook(phonebook_id, name, phone_num) VALUES(6, 'Stacy Yang', '9178852765')\")\n\nconnection.commit()\n\n# Allows you to see the created table\noutput = pd.read_sql_query(\"SELECT * FROM phonebook\", connection)\nprint(output)\nconnection.close()\n\n\n\n6.3.8 How to Update/Delete Using SQL?\n# Updating an attribute (WHERE statement is optional)\nconnection = sqlite3.connect(\"phonebook.db\")\n\n# created a cursor object lets you use the cursor function from SQlite3 module\ncursor = connection.cursor()\n\n# Updates Greta's number\ncursor.execute(\"UPDATE phonebook SET phone_num = '2035151234' WHERE name = 'Greta Colic';\")\n\n# Deletes Harry Smith from the phonebook\ncursor.execute(\"DELETE FROM phonebook WHERE name = 'Harry Smith';\")\n\n# Changes Carlos's last name\ncursor.execute(\"UPDATE phonebook SET name = 'Carlos Ramos' WHERE name = 'Carlos Alavarez';\")\n\n# Updating multiple columns\nupdate_multiple_query = \"\"\"\nUPDATE phonebook \nSET phone_num = '7777777777', address = '45 Main St' \nWHERE name = 'Marin Yanko';\n\"\"\"\ncursor.execute(update_multiple_query)\n\n# deleting a table \ncursor.execute(\"DROP TABLE phonebook;\")\n\nconnection.commit()\nconnection.close()\n\n\n6.3.9 \n\nimport sqlite3\nimport pandas as pd\n\n# creates a database file called phonebook.db and lets you connect\nconnection = sqlite3.connect(\"phonebook.db\")\n\n# created a cursor object lets you use the cursor function from SQlite3 module\ncursor = connection.cursor()\n\ncursor.execute(\"DROP TABLE IF EXISTS phonebook;\")\n# Execute function allows you to send commands in the SQL language as strings\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS phonebook (\n    phonebook_id INT,\n    name VARCHAR(30), \n    phone_num VARCHAR(15) PRIMARY KEY,\n    address VARCHAR(30)\n)\n\"\"\")\n\ncursor.execute(\"INSERT INTO phonebook VALUES (1, 'Greta Colic', '2035452367', '1 Hartsdale road')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(2, 'Carlos Alavarez', '9145652761', '13 Porter street')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(3, 'Marin Yanko', '5753917568', '110 Ocean avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(4, 'Mira Watson', '9146522761', '12 Hindmarsh avenue')\")\ncursor.execute(\"INSERT INTO phonebook VALUES(5, 'Harry Smith', '2036658279', '180 Wallace road')\")\ncursor.execute(\"INSERT INTO phonebook(phonebook_id, name, phone_num) VALUES(6, 'Stacy Yang', '9178852765')\")\n\nconnection.commit()\n \n\n# alternative way of displaying output\n\"\"\"cursor.execute(\"SELECT * FROM phonebook\")\nrows = cursor.fetchall()\nfor row in rows:\n    print(row)\"\"\"\n\noutput = pd.read_sql_query(\"SELECT * FROM phonebook\", connection)\nprint(output)\nprint(\" \")\nprint(\" \")\n\ncursor.execute(\"UPDATE phonebook SET phone_num = '2035151234' WHERE name = 'Greta Colic';\")\n\n# Deletes Harry Smith from the phonebook\ncursor.execute(\"DELETE FROM phonebook WHERE name = 'Harry Smith';\")\n\n# Updating multiple columns\nupdate_multiple_query = \"\"\"\nUPDATE phonebook \nSET phone_num = '7777777777', address = '45 Main St' \nWHERE name = 'Marin Yanko';\n\"\"\"\ncursor.execute(update_multiple_query)\n\ncursor.execute(\"UPDATE phonebook SET name = 'Carlos Ramos' WHERE name = 'Carlos Alavarez';\")\n\nconnection.commit()\noutput = pd.read_sql_query(\"SELECT * FROM phonebook\", connection)\nprint(output)\n\nconnection.close()\n\n   phonebook_id             name   phone_num              address\n0             1      Greta Colic  2035452367     1 Hartsdale road\n1             2  Carlos Alavarez  9145652761     13 Porter street\n2             3      Marin Yanko  5753917568     110 Ocean avenue\n3             4      Mira Watson  9146522761  12 Hindmarsh avenue\n4             5      Harry Smith  2036658279     180 Wallace road\n5             6       Stacy Yang  9178852765                 None\n \n \n   phonebook_id          name   phone_num              address\n0             1   Greta Colic  2035151234     1 Hartsdale road\n1             2  Carlos Ramos  9145652761     13 Porter street\n2             3   Marin Yanko  7777777777           45 Main St\n3             4   Mira Watson  9146522761  12 Hindmarsh avenue\n4             6    Stacy Yang  9178852765                 None\n\n\n\n\n6.3.10 Using SQL to Work on a CSV file\nimport pandas as pd\nimport sqlite3\n\n# Creates an SQL Database\nconn = sqlite3.connect(\"nyc.db\")\n\n# Reads the CSV file using the path\ndata = pd.read_csv(r\"data/nyccrashes_2024w0630_by20240916.csv\")\n\ncursor = conn.cursor()\n\n# automatically converts data to an SQL table\ndata.to_sql('nyccrashes', conn, if_exists='replace', index=False)\n\nimport pandas as pd\nimport sqlite3\n\n# Creates an SQL Database\nconn = sqlite3.connect(\"nyc.db\")\n\n# Reads the CSV file using the path\ndata = pd.read_csv(r\"data/nyccrashes_2024w0630_by20240916.csv\")\n\ncursor = conn.cursor()\n\n# automatically converts data to an SQL table\ndata.to_sql('nyccrashes', conn, if_exists='replace', index=False)\n\n1875\n\n\n\n\n6.3.11 Queries\n\nCommands used to pull needed data out of a database\n\n# Query for everythin in the table\nquery1 = pd.read_sql_query(\"SELECT * FROM nyccrashes;\", conn)\nprint(query1.head(3))\n\n# Query to count total crashes\ntotal_crashes = pd.read_sql_query(\"SELECT COUNT(*) FROM nyccrashes;\", conn)\nprint(total_crashes)\n\n# Query to only retrieve fixed attributes\nspecific_columns = pd.read_sql_query(\"SELECT \\\"ZIP CODE\\\", \\\"CRASH TIME\\\" FROM nyccrashes;\", conn)\nprint(specific_columns.head(3))\n\n# Groups Crashes by borough\ncrashes_by_borough = pd.read_sql_query(\"\"\"\n    SELECT BOROUGH, COUNT(*) AS Total_Crashes\n    FROM nyccrashes\n    GROUP BY BOROUGH;\n\"\"\", conn)\nprint(crashes_by_borough)\n\n# Query to show the fatal crashes\nfatal_crashes = pd.read_sql_query(\"SELECT * FROM nyccrashes WHERE \\\"NUMBER OF PERSONS KILLED\\\" &gt; 0;\", conn)\nprint(fatal_crashes.head())\n\n\n6.3.12 Queries Output\n\n# Query for everythin in the table\n# query1 = pd.read_sql_query(\"SELECT * FROM nyccrashes;\", conn)\n# print(query1.head(3))\n\n# Query to count total crashes\nprint(\"Here's the output for the count query.\")\ntotal_crashes = pd.read_sql_query(\"SELECT COUNT(*) FROM nyccrashes;\", conn)\nprint(total_crashes)\nprint()\n\n# Groups Crashes by borough\nprint(\"Here's the output for the query to group crashes by borough.\")\ncrashes_by_borough = pd.read_sql_query(\"\"\"\n    SELECT BOROUGH, COUNT(*) AS Total_Crashes\n    FROM nyccrashes\n    GROUP BY BOROUGH;\n\"\"\", conn)\nprint(crashes_by_borough)\nprint()\n# Query to only retrieve fixed attributes\nprint(\"Here's the output for the specific columns query.\")\nspecific_columns = pd.read_sql_query(\"SELECT \\\"ZIP CODE\\\", \\\"CRASH TIME\\\" FROM nyccrashes;\", conn)\nprint(specific_columns.head(5))\n\n# Query to show the fatal crashes\n# fatal_crashes = pd.read_sql_query(\"SELECT * FROM nyccrashes #WHERE \\\"NUMBER OF PERSONS KILLED\\\" &gt; 0;\", conn)\n# print(fatal_crashes.head())\nconn.commit()\n\nHere's the output for the count query.\n   COUNT(*)\n0      1875\n\nHere's the output for the query to group crashes by borough.\n         BOROUGH  Total_Crashes\n0           None            541\n1          BRONX            213\n2       BROOKLYN            462\n3      MANHATTAN            228\n4         QUEENS            381\n5  STATEN ISLAND             50\n\nHere's the output for the specific columns query.\n   ZIP CODE CRASH TIME\n0       NaN      17:30\n1       NaN       0:32\n2   11235.0       7:05\n3       NaN      20:47\n4   11222.0      10:14\n\n\n\n\n6.3.13 Conclusion\n\nSQL works with relational databases\nSQL performs the CRUD functions: create, read, update, and delete to work with databases\nA query is a request from a database for information\nSQL can be used to manipulate data from various formats including CSV files\n\n\n\n6.3.14 Further reading\n\nPython Software Foundation. (n.d.). sqlite3 — DB-API 2.0 interface for SQLite databases. Python Documentation. https://docs.python.org/3/library/sqlite3.html",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#nyc-crash-data",
    "href": "manipulation.html#nyc-crash-data",
    "title": "6  Data Manipulation",
    "section": "6.4 NYC Crash Data",
    "text": "6.4 NYC Crash Data\nConsider a subset of the NYC Crash Data, which contains all NYC motor vehicle collisions data with documentation from NYC Open Data. We downloaded the crash data for the week of June 30, 2024, on September 16, 2024, in CSC format.\n\nimport pandas as pd\n\n# Load the dataset\nfile_path = 'data/nyccrashes_2024w0630_by20240916.csv'\ndf = pd.read_csv(file_path)\n\n# Replace column names: convert to lowercase and replace spaces with underscores\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Display the first few rows of the dataset to understand its structure\ndf.head()\n\n\n\n\n\n\n\n\ncrash_date\ncrash_time\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\n...\ncontributing_factor_vehicle_2\ncontributing_factor_vehicle_3\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\n\n\n\n\n0\n06/30/2024\n17:30\nNaN\nNaN\n0.00000\n0.00000\n(0.0, 0.0)\nNaN\nNaN\nGOLD STREET\n...\nUnspecified\nNaN\nNaN\nNaN\n4736746\nSedan\nSedan\nNaN\nNaN\nNaN\n\n\n1\n06/30/2024\n0:32\nNaN\nNaN\nNaN\nNaN\nNaN\nBELT PARKWAY RAMP\nNaN\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736768\nStation Wagon/Sport Utility Vehicle\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\n\n\n2\n06/30/2024\n7:05\nBROOKLYN\n11235.0\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNaN\nNaN\n2797 OCEAN PARKWAY\n...\nNaN\nNaN\nNaN\nNaN\n4737060\nStation Wagon/Sport Utility Vehicle\nNaN\nNaN\nNaN\nNaN\n\n\n3\n06/30/2024\n20:47\nNaN\nNaN\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n4737510\nSedan\nNaN\nNaN\nNaN\nNaN\n\n\n4\n06/30/2024\n10:14\nBROOKLYN\n11222.0\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNaN\n...\nUnspecified\nNaN\nNaN\nNaN\n4736759\nBus\nBox Truck\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\nNow we can do some cleaning after a quick browse.\n\n# Replace invalid coordinates (latitude=0, longitude=0 or NaN) with NaN\ndf.loc[(df['latitude'] == 0) & (df['longitude'] == 0), \n       ['latitude', 'longitude']] = pd.NA\ndf['latitude'] = df['latitude'].replace(0, pd.NA)\ndf['longitude'] = df['longitude'].replace(0, pd.NA)\n\n# Longitude/latitude don't need double precision\ndf['latitude'] = df['latitude'].astype('float32', errors='ignore')\ndf['longitude'] = df['longitude'].astype('float32', errors='ignore')\n\n# Drop the redundant 'location' column\ndf = df.drop(columns=['location'])\n\n# Converting 'crash_date' and 'crash_time' columns into a single datetime column\ndf['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' \n                       + df['crash_time'], format='%m/%d/%Y %H:%M', errors='coerce')\n\n# Drop the original 'crash_date' and 'crash_time' columns\ndf = df.drop(columns=['crash_date', 'crash_time'])\n\nAre missing in zip code and borough always co-occur?\n\n# Check if missing values in 'zip_code' and 'borough' always co-occur\n# Count rows where both are missing\nmissing_cooccur = df[['zip_code', 'borough']].isnull().all(axis=1).sum()\n# Count total missing in 'zip_code' and 'borough', respectively\ntotal_missing_zip_code = df['zip_code'].isnull().sum()\ntotal_missing_borough = df['borough'].isnull().sum()\n\n# If missing in both columns always co-occur, the number of missing\n# co-occurrences should be equal to the total missing in either column\nmissing_cooccur, total_missing_zip_code, total_missing_borough\n\n(np.int64(541), np.int64(541), np.int64(541))\n\n\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes by reverse geocoding.\nFirst make sure geopy is installed.\npip install geopy\nNow we use model Nominatim in package geopy to reverse geocode.\n\nfrom geopy.geocoders import Nominatim\nimport time\n\n# Initialize the geocoder; the `user_agent` is your identifier \n# when using the service. Be mindful not to crash the server\n# by unlimited number of queries, especially invalid code.\ngeolocator = Nominatim(user_agent=\"jyGeopyTry\")\n\nWe write a function to do the reverse geocoding given lattitude and longitude.\n\n# Function to fill missing zip_code\ndef get_zip_code(latitude, longitude):\n    try:\n        location = geolocator.reverse((latitude, longitude), timeout=10)\n        if location:\n            address = location.raw['address']\n            zip_code = address.get('postcode', None)\n            return zip_code\n        else:\n            return None\n    except Exception as e:\n        print(f\"Error: {e} for coordinates {latitude}, {longitude}\")\n        return None\n    finally:\n        time.sleep(1)  # Delay to avoid overwhelming the service\n\nLet’s try it out:\n\n# Example usage\nlatitude = 40.730610\nlongitude = -73.935242\nzip_code = get_zip_code(latitude, longitude)\n\nThe function get_zip_code can then be applied to rows where zip code is missing but geocodes are not to fill the missing zip code.\nOnce zip code is known, figuring out burough is simple because valid zip codes from each borough are known.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#cross-platform-data-format-arrow",
    "href": "manipulation.html#cross-platform-data-format-arrow",
    "title": "6  Data Manipulation",
    "section": "6.5 Cross-platform Data Format Arrow",
    "text": "6.5 Cross-platform Data Format Arrow\nThe CSV format (and related formats like TSV - tab-separated values) for data tables is ubiquitous, convenient, and can be read or written by many different data analysis environments, including spreadsheets. An advantage of the textual representation of the data in a CSV file is that the entire data table, or portions of it, can be previewed in a text editor. However, the textual representation can be ambiguous and inconsistent. The format of a particular column: Boolean, integer, floating-point, text, factor, etc. must be inferred from text representation, often at the expense of reading the entire file before these inferences can be made. Experienced data scientists are aware that a substantial part of an analysis or report generation is often the “data cleaning” involved in preparing the data for analysis. This can be an open-ended task — it required numerous trial-and-error iterations to create the list of different missing data representations we use for the sample CSV file and even now we are not sure we have them all.\nTo read and export data efficiently, leveraging the Apache Arrow library can significantly improve performance and storage efficiency, especially with large datasets. The IPC (Inter-Process Communication) file format in the context of Apache Arrow is a key component for efficiently sharing data between different processes, potentially written in different programming languages. Arrow’s IPC mechanism is designed around two main file formats:\n\nStream Format: For sending an arbitrary length sequence of Arrow record batches (tables). The stream format is useful for real-time data exchange where the size of the data is not known upfront and can grow indefinitely.\nFile (or Feather) Format: Optimized for storage and memory-mapped access, allowing for fast random access to different sections of the data. This format is ideal for scenarios where the entire dataset is available upfront and can be stored in a file system for repeated reads and writes.\n\nApache Arrow provides a columnar memory format for flat and hierarchical data, optimized for efficient data analytics. It can be used in Python through the pyarrow package. Here’s how you can use Arrow to read, manipulate, and export data, including a demonstration of storage savings.\nFirst, ensure you have pyarrow installed on your computer (and preferrably, in your current virtual environment):\npip install pyarrow\nFeather is a fast, lightweight, and easy-to-use binary file format for storing data frames, optimized for speed and efficiency, particularly for IPC and data sharing between Python and R or Julia.\n\ndf.to_feather('data/nyccrashes_cleaned.feather')\n\n# Compare the file sizes of the feather format and the CSV format\nimport os\n\n# File paths\ncsv_file = 'data/nyccrashes_2024w0630_by20240916.csv'\nfeather_file = 'data/nyccrashes_cleaned.feather'\n\n# Get file sizes in bytes\ncsv_size = os.path.getsize(csv_file)\nfeather_size = os.path.getsize(feather_file)\n\n# Convert bytes to a more readable format (e.g., MB)\ncsv_size_mb = csv_size / (1024 * 1024)\nfeather_size_mb = feather_size / (1024 * 1024)\n\n# Print the file sizes\nprint(f\"CSV file size: {csv_size_mb:.2f} MB\")\nprint(f\"Feather file size: {feather_size_mb:.2f} MB\")\n\nRead the feather file back in:\n\ndff = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\ndff.shape",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#using-the-census-data",
    "href": "manipulation.html#using-the-census-data",
    "title": "6  Data Manipulation",
    "section": "6.6 Using the Census Data",
    "text": "6.6 Using the Census Data\nThe US Census provides a lot of useful data that could be merged with the NYC crash data for further analytics.\nFirst, ensure the DataFrame (df) is ready for merging with census data. Specifically, check that the zip_code column is clean and consistent.and consistent.\n\nimport pandas as pd\ndf = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\nvalid_zip_df = df.dropna(subset=['zip_code']).copy()\nvalid_zip_df['zip_code'] = valid_zip_df['zip_code'].astype(int).astype(str).str.zfill(5)\nunique_zips = valid_zip_df['zip_code'].unique()\n\nWe can use the uszipcode package to get basic demographic data for each zip code. For more detailed or specific census data, using the CensusData package or direct API calls to the Census Bureau’s API.\nThe uszipcode package provides a range of information about ZIP codes in the United States. When you query a ZIP code using uszipcode, you can access various attributes related to demographic data, housing, geographic location, and more. Here are some of the key variables available at the ZIP code level:\nDemographic Information\n\npopulation: The total population.\npopulation_density: The population per square kilometer.\nhousing_units: The total number of housing units.\noccupied_housing_units: The number of occupied housing units.\nmedian_home_value: The median value of homes.\nmedian_household_income: The median household income.\nage_distribution: A breakdown of the population by age.\n\nGeographic Information\n\nzipcode: The ZIP code.\nzipcode_type: The type of ZIP code (e.g., Standard, PO Box).\nmajor_city: The major city associated with the ZIP code.\npost_office_city: The city name recognized by the U.S. Postal Service.\ncommon_city_list: A list of common city names for the ZIP code.\ncounty: The county in which the ZIP code is located.\nstate: The state in which the ZIP code is located.\nlat: The latitude of the approximate center of the ZIP code.\nlng: The longitude of the approximate center of the ZIP code.\ntimezone: The timezone of the ZIP code.\n\nEconomic and Housing Data\n\nland_area_in_sqmi: The land area in square miles.\nwater_area_in_sqmi: The water area in square miles.\noccupancy_rate: The rate of occupancy for housing units.\nmedian_age: The median age of the population.\n\nInstall the uszipcode package into the current virtual environment, if it has not been installd yet.\npip install uszipcode\n\n\n``\nWe will first clean the `zip_code` column to ensure it only\ncontains valid ZIP codes. Then, we will use a vectorized\napproach to fetch the required data for each unique zip code\nand merge this information back into the original `DataFrame`.\n`{python}\n# Remove rows where 'zip_code' is missing or not a valid ZIP code format\nvalid_zip_df = df.dropna(subset=['zip_code']).copy()\nvalid_zip_df['zip_code'] = valid_zip_df['zip_code'].astype(str).str.zfill(5)\nSince uszipcode doesn’t valid_zip_dfrently support vectorized operations for multiple ZIP code queries, we’ll optimize the process by querying each unique ZIP code once, then merging the results with the original DataFrame. This approach minimizes redundant queries for ZIP codes that appear multiple\ntimes.\n\nfrom uszipcode import SearchEngine\n\n# Initialize the SearchEngine\nsearch = SearchEngine()\n\n# Fetch median home value and median household income for each unique ZIP code\nzip_data = []\nfor zip_code in unique_zips:\n    result = search.by_zipcode(zip_code)\n    if result:  # Check if the result is not None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": result.median_home_value,\n            \"median_household_income\": result.median_household_income\n        })\n    else:  # Handle the case where the result is None\n        zip_data.append({\n            \"zip_code\": zip_code,\n            \"median_home_value\": None,\n            \"median_household_income\": None\n        })\n\n# Convert to DataFrame\nzip_info_df = pd.DataFrame(zip_data)\n\n# Merge this info back into the original DataFrame based on 'zip_code'\nmerged_df = pd.merge(valid_zip_df, zip_info_df, how=\"left\", on=\"zip_code\")\nmerged_df.to_feather('data/nyccrashes_merged.feather')\n\nmerged_df.head()\n\n\n\n\n\n\n\n\ncrash_datetime\nborough\nzip_code\nlatitude\nlongitude\nlocation\non_street_name\ncross_street_name\noff_street_name\nnumber_of_persons_injured\n...\ncontributing_factor_vehicle_4\ncontributing_factor_vehicle_5\ncollision_id\nvehicle_type_code_1\nvehicle_type_code_2\nvehicle_type_code_3\nvehicle_type_code_4\nvehicle_type_code_5\nmedian_home_value\nmedian_household_income\n\n\n\n\n0\n2024-06-30 07:05:00\nBROOKLYN\n11235\n40.58106\n-73.96744\n(40.58106, -73.96744)\nNone\nNone\n2797 OCEAN PARKWAY\n0\n...\nNone\nNone\n4737060\nStation Wagon/Sport Utility Vehicle\nNone\nNone\nNone\nNone\n531000.0\n41639.0\n\n\n1\n2024-06-30 20:47:00\nMANHATTAN\n10021\n40.76363\n-73.95330\n(40.76363, -73.9533)\nFDR DRIVE\nNone\nNone\n0\n...\nNone\nNone\n4737510\nSedan\nNone\nNone\nNone\nNone\n1000001.0\n107907.0\n\n\n2\n2024-06-30 10:14:00\nBROOKLYN\n11222\n40.73046\n-73.95149\n(40.73046, -73.95149)\nGREENPOINT AVENUE\nMC GUINNESS BOULEVARD\nNone\n0\n...\nNone\nNone\n4736759\nBus\nBox Truck\nNone\nNone\nNone\n726500.0\n63739.0\n\n\n3\n2024-06-30 15:52:00\nBRONX\n10468\n40.86685\n-73.89597\n(40.86685, -73.89597)\nNone\nNone\n60 EAST KINGSBRIDGE ROAD\n1\n...\nNone\nNone\n4736781\nStation Wagon/Sport Utility Vehicle\nMoped\nNone\nNone\nNone\n171200.0\n33776.0\n\n\n4\n2024-06-30 16:30:00\nBROOKLYN\n11226\n40.63969\n-73.95321\n(40.63969, -73.95321)\nNEWKIRK AVENUE\nEAST 25 STREET\nNone\n1\n...\nNone\nNone\n4737299\nSedan\nMoped\nNone\nNone\nNone\n480500.0\n40734.0\n\n\n\n\n5 rows × 30 columns",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#extracting-and-analyzing-census-tract-data",
    "href": "manipulation.html#extracting-and-analyzing-census-tract-data",
    "title": "6  Data Manipulation",
    "section": "6.7 Extracting and Analyzing Census Tract Data",
    "text": "6.7 Extracting and Analyzing Census Tract Data\nThis section was authored by Mohammad Shahriyar Parvez, a Ph.D. scholar in the Department of Geography, Sustainability, and Urban Studies at the University of Connecticut. For any questions or difficulties in executing the content of this section, please feel free to reach out at shahriyar@uconn.edu.\nThis section explains how to access data from the US Decennial Census and the American Community Survey (ACS). Basic operations will be applied to calculate and map poverty rates in Storrs, CT. Data will be retrieved from the US Census Bureau’s 2022 American Community Survey (ACS) (refer to this page for the dataset).\n\n# Import modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom census import Census\nfrom us import states\nimport os\n\n\n6.7.1 Accessing the cencus Data\n\n6.7.1.1 Import census data\nImporting census data into Python requires a Census API key. A key can be obtained from Census API Key. You have to provide your Institution name and email address. A unique 40 digit text string API will be sent to you. You have to keep track of this number and store it in a safe place.\n\n# Set up the API key for accessing Census data\nwith open(\"CencusAPI.txt\") as f:\n    api_key = f.read().strip()\nc = Census(api_key)\n\nOnce the Census API key is configured, access census data at the tract level for Connecticut using the 2022 ACS. Focus on these variables: total income-to-poverty ratio (C17002_001E), income less than 0.50 of the poverty level (C17002_002E), income between 0.50 and 0.99 of the poverty level (C17002_003E), and total population (B01003_001E). For details on why these variables are relevant to poverty analysis, refer to the Census Bureau’s guidelines on poverty measurement and the ACS variable documentation.\nUse the census package’s convenience methods to pull data for different geographic levels. The FIPS code for Connecticu is 09, but if necessary, use the us library to identify FIPS codes for other states.\n\n# Obtain Census variables from the ACS at the tract level for Storrs, CT\nct_census = c.acs5.state_county_tract(\n    fields=('NAME', 'C17002_001E', 'C17002_002E', 'C17002_003E', 'B01003_001E'),\n    state_fips=states.CT.fips,\n    county_fips=\"*\",\n    tract=\"*\",\n    year=2022\n)\n\nWith the data retrieved and assigned to a variable, load it into a DataFrame using the pandas library. This will allow for easy manipulation and analysis of the data.\n\n# Create a dataframe from the census data\nct_df = pd.DataFrame(ct_census)\n\n# Show the dataframe\nprint(ct_df.head(20))\nprint('Shape: ', ct_df.shape)\n\n                                                 NAME  C17002_001E  \\\n0   Census Tract 4001.01; Capitol Planning Region;...       2916.0   \n1   Census Tract 4001.02; Capitol Planning Region;...       4266.0   \n2   Census Tract 4002; Capitol Planning Region; Co...       6213.0   \n3   Census Tract 4003; Capitol Planning Region; Co...       6677.0   \n4   Census Tract 4153; Capitol Planning Region; Co...       2615.0   \n5   Census Tract 4154; Capitol Planning Region; Co...       6046.0   \n6   Census Tract 4155; Capitol Planning Region; Co...       3245.0   \n7   Census Tract 4156; Capitol Planning Region; Co...       4201.0   \n8   Census Tract 4157; Capitol Planning Region; Co...       3284.0   \n9   Census Tract 4158; Capitol Planning Region; Co...       2606.0   \n10  Census Tract 4159; Capitol Planning Region; Co...       1427.0   \n11  Census Tract 4160; Capitol Planning Region; Co...       4738.0   \n12  Census Tract 4161; Capitol Planning Region; Co...       4995.0   \n13  Census Tract 4162; Capitol Planning Region; Co...       2839.0   \n14  Census Tract 4163; Capitol Planning Region; Co...       4533.0   \n15  Census Tract 4164; Capitol Planning Region; Co...       3180.0   \n16  Census Tract 4165; Capitol Planning Region; Co...       4364.0   \n17  Census Tract 4166; Capitol Planning Region; Co...       3110.0   \n18  Census Tract 4167; Capitol Planning Region; Co...       6364.0   \n19  Census Tract 4168; Capitol Planning Region; Co...       3394.0   \n\n    C17002_002E  C17002_003E  B01003_001E state county   tract  \n0          41.0         22.0       2945.0    09    110  400101  \n1          73.0        305.0       4266.0    09    110  400102  \n2          55.0        136.0       6250.0    09    110  400200  \n3         135.0        197.0       6688.0    09    110  400300  \n4         465.0        360.0       2621.0    09    110  415300  \n5         431.0        318.0       6046.0    09    110  415400  \n6         408.0        353.0       3245.0    09    110  415500  \n7         300.0        667.0       4303.0    09    110  415600  \n8         134.0        169.0       3290.0    09    110  415700  \n9         459.0        313.0       2775.0    09    110  415800  \n10        131.0        405.0       1437.0    09    110  415900  \n11        382.0        646.0       4738.0    09    110  416000  \n12        509.0        992.0       5017.0    09    110  416100  \n13        133.0        633.0       2911.0    09    110  416200  \n14        373.0        796.0       4533.0    09    110  416300  \n15         50.0        110.0       3185.0    09    110  416400  \n16        350.0        180.0       4404.0    09    110  416500  \n17        403.0        196.0       3171.0    09    110  416600  \n18        424.0       1109.0       6481.0    09    110  416700  \n19        401.0         13.0       3394.0    09    110  416800  \nShape:  (884, 8)\n\n\nDisplaying the DataFrame reveals 884 rows, indicating 884 census tracts, along with 8 columns. This structure provides a comprehensive view of the data across all selected tracts.\n\n\n6.7.1.2 Import Shapefile\nRead the Connecticut census tracts shapefile into Python and reproject it to the UTM Zone 18N projection. The shapefile is available for download from the Census Bureau’s website, specifically on the Cartographic Boundary Files page or the TIGER/Line Shapefiles page.\n\n# Access shapefile of Connecticut census tracts\nct_tract = gpd.read_file(\"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\"\n                         \"tl_2024_09_tract.zip\")\n\n# Reproject shapefile to UTM Zone 18N\nct_tract = ct_tract.to_crs(epsg=32618)\n\n# Print GeoDataFrame of shapefile\nprint(ct_tract.head(2))\nprint('Shape: ', ct_tract.shape)\nprint(\"\\nThe shapefile projection is: {}\".format(ct_tract.crs))\n\n  STATEFP COUNTYFP TRACTCE        GEOID               GEOIDFQ  NAME  \\\n0      09      110  535200  09110535200  1400000US09110535200  5352   \n1      09      110  881200  09110881200  1400000US09110881200  8812   \n\n            NAMELSAD  MTFCC FUNCSTAT     ALAND   AWATER     INTPTLAT  \\\n0  Census Tract 5352  G5020        S  43206583  1355239  +41.9294855   \n1  Census Tract 8812  G5020        S   2273066    28584  +41.8089384   \n\n       INTPTLON                                           geometry  \n0  -072.4062911  POLYGON ((710429.8 4638466.647, 710437.611 463...  \n1  -072.2516171  POLYGON ((727399.183 4633086.018, 727416.326 4...  \nShape:  (884, 14)\n\nThe shapefile projection is: EPSG:32618\n\n\nPrinting the shapefile confirms it also contains 884 rows, representing 884 census tracts. This matches the number of census records, which is a good start!\nHowever, there’s a potential issue: while we have the census data and the shapefile of corresponding census tracts, they are currently stored in two separate variables (ct_df and ct_tract). This separation complicates mapping, as the datasets are not yet linked.\n\n\n\n6.7.2 Performing Dataframe Operations\n\n6.7.2.1 Create new column from old columns\nTo address this issue, join the two DataFrames using a shared field, or “key,” that is common to both datasets.\nAfter reviewing the two datasets, the GEOID column in ct_tract and the combination of the state, county, and tract columns in ct_df can serve as a unique key for joining. Currently, these columns are not in a format that can be directly matched, so the state, county, and tract columns from ct_df will need to be merged into a single column to align with the GEOID column in ct_tract.\nTo create this new column—or access any existing column in a DataFrame—use indexing with [] and the column name as a string. (Alternatively, columns can be accessed by index position; see the pandas documentation on indexing and selecting data for more details.)\n\n# Combine state, county, and tract columns together\nct_df[\"GEOID\"] = ct_df[\"state\"] + ct_df[\"county\"] + ct_df[\"tract\"]\n\n# Print head of dataframe\nct_df.head(10)\n\n\n\n\n\n\n\n\nNAME\nC17002_001E\nC17002_002E\nC17002_003E\nB01003_001E\nstate\ncounty\ntract\nGEOID\n\n\n\n\n0\nCensus Tract 4001.01; Capitol Planning Region;...\n2916.0\n41.0\n22.0\n2945.0\n09\n110\n400101\n09110400101\n\n\n1\nCensus Tract 4001.02; Capitol Planning Region;...\n4266.0\n73.0\n305.0\n4266.0\n09\n110\n400102\n09110400102\n\n\n2\nCensus Tract 4002; Capitol Planning Region; Co...\n6213.0\n55.0\n136.0\n6250.0\n09\n110\n400200\n09110400200\n\n\n3\nCensus Tract 4003; Capitol Planning Region; Co...\n6677.0\n135.0\n197.0\n6688.0\n09\n110\n400300\n09110400300\n\n\n4\nCensus Tract 4153; Capitol Planning Region; Co...\n2615.0\n465.0\n360.0\n2621.0\n09\n110\n415300\n09110415300\n\n\n5\nCensus Tract 4154; Capitol Planning Region; Co...\n6046.0\n431.0\n318.0\n6046.0\n09\n110\n415400\n09110415400\n\n\n6\nCensus Tract 4155; Capitol Planning Region; Co...\n3245.0\n408.0\n353.0\n3245.0\n09\n110\n415500\n09110415500\n\n\n7\nCensus Tract 4156; Capitol Planning Region; Co...\n4201.0\n300.0\n667.0\n4303.0\n09\n110\n415600\n09110415600\n\n\n8\nCensus Tract 4157; Capitol Planning Region; Co...\n3284.0\n134.0\n169.0\n3290.0\n09\n110\n415700\n09110415700\n\n\n9\nCensus Tract 4158; Capitol Planning Region; Co...\n2606.0\n459.0\n313.0\n2775.0\n09\n110\n415800\n09110415800\n\n\n\n\n\n\n\nBy printing the first few rows of the DataFrame, the newly created GEOID column is visible, showing the combined values from the state, county, and tract columns. This new column now matches the format of the GEOID column in ct_tract, setting up the data for a successful join.\n\n\n6.7.2.2 Remove dataframe columns that are no longer needed\nTo minimize clutter, delete the state, county, and tract columns from ct_df, as they’re no longer needed after creating the GEOID column. When modifying a DataFrame, reassign the modified DataFrame back to the original variable (or a new variable if preferred) to save the changes. Alternatively, use inplace=True within the drop function to apply changes directly without reassignment. For additional details, refer to the pandas documentation on drop.\n\n# Remove columns\nct_df = ct_df.drop(columns = [\"state\", \"county\", \"tract\"])\n\n# Show updated dataframe\nct_df.head(2)\n\n\n\n\n\n\n\n\nNAME\nC17002_001E\nC17002_002E\nC17002_003E\nB01003_001E\nGEOID\n\n\n\n\n0\nCensus Tract 4001.01; Capitol Planning Region;...\n2916.0\n41.0\n22.0\n2945.0\n09110400101\n\n\n1\nCensus Tract 4001.02; Capitol Planning Region;...\n4266.0\n73.0\n305.0\n4266.0\n09110400102\n\n\n\n\n\n\n\n\n\n6.7.2.3 Check column data types\nEnsure that the key column (GEOID) in both DataFrames is of the same data type to allow for a successful join. Start by checking the data type of the GEOID column in each DataFrame. If the data types differ, adjust one or both columns as needed to make them consistent.\n\n# Check column data types for census data\nprint(ct_df.dtypes)\n\n# Check column data types for census shapefile\nprint(ct_tract.dtypes)\n\nNAME            object\nC17002_001E    float64\nC17002_002E    float64\nC17002_003E    float64\nB01003_001E    float64\nGEOID           object\ndtype: object\nSTATEFP       object\nCOUNTYFP      object\nTRACTCE       object\nGEOID         object\nGEOIDFQ       object\nNAME          object\nNAMELSAD      object\nMTFCC         object\nFUNCSTAT      object\nALAND          int64\nAWATER         int64\nINTPTLAT      object\nINTPTLON      object\ngeometry    geometry\ndtype: object\n\n\n\n\n6.7.2.4 Merge dataframes\nNow it’s time to merge the two DataFrames using the GEOID columns as the primary key. Use the merge method in GeoPandas, applying it to the va_tract shapefile dataset to combine the data based on the GEOID column.\n\n# Join the attributes of the dataframes together\nct_merge = ct_tract.merge(ct_df, on = \"GEOID\")\n\n# Show result\nprint(ct_merge.head(2))\nprint('Shape: ', ct_merge.shape)\n\n  STATEFP COUNTYFP TRACTCE        GEOID               GEOIDFQ NAME_x  \\\n0      09      110  535200  09110535200  1400000US09110535200   5352   \n1      09      110  881200  09110881200  1400000US09110881200   8812   \n\n            NAMELSAD  MTFCC FUNCSTAT     ALAND   AWATER     INTPTLAT  \\\n0  Census Tract 5352  G5020        S  43206583  1355239  +41.9294855   \n1  Census Tract 8812  G5020        S   2273066    28584  +41.8089384   \n\n       INTPTLON                                           geometry  \\\n0  -072.4062911  POLYGON ((710429.8 4638466.647, 710437.611 463...   \n1  -072.2516171  POLYGON ((727399.183 4633086.018, 727416.326 4...   \n\n                                              NAME_y  C17002_001E  \\\n0  Census Tract 5352; Capitol Planning Region; Co...       6650.0   \n1  Census Tract 8812; Capitol Planning Region; Co...        502.0   \n\n   C17002_002E  C17002_003E  B01003_001E  \n0        102.0        213.0       6664.0  \n1        318.0         15.0      10590.0  \nShape:  (884, 19)\n\n\nThe merged DataFrame still has 884 rows, indicating that all (or nearly all) rows were matched correctly. The census data has now been appended to the shapefile data within the DataFrame.\nAdditional Notes on Joining DataFrames\n\nThe columns used as keys do not need to have the same name, as long as they contain matching data.\nThis join was a one-to-one relationship, where each attribute in one DataFrame matched exactly one attribute in the other. Joins with many-to-one, one-to-many, or many-to-many relationships are also possible, though they may require additional considerations. For more details, refer to the Esri ArcGIS documentation on joins and relates.\n\n\n\n6.7.2.5 Subset dataframe\nWith the DataFrames merged, further clean up the data by removing unnecessary columns. Instead of using the drop method, select only the columns needed and create a new DataFrame containing just those selected columns. This approach streamlines the DataFrame and keeps only the relevant data for analysis.\n\n# Create new dataframe from select columns\nct_poverty_tract = ct_merge[[\"STATEFP\", \"COUNTYFP\", \"TRACTCE\", \n                            \"GEOID\", \"geometry\", \"C17002_001E\", \n                            \"C17002_002E\", \"C17002_003E\", \n                            \"B01003_001E\"]]\n\n# Show dataframe\nprint(ct_poverty_tract.head(2))\nprint('Shape: ', ct_poverty_tract.shape)\n\n  STATEFP COUNTYFP TRACTCE        GEOID  \\\n0      09      110  535200  09110535200   \n1      09      110  881200  09110881200   \n\n                                            geometry  C17002_001E  \\\n0  POLYGON ((710429.8 4638466.647, 710437.611 463...       6650.0   \n1  POLYGON ((727399.183 4633086.018, 727416.326 4...        502.0   \n\n   C17002_002E  C17002_003E  B01003_001E  \n0        102.0        213.0       6664.0  \n1        318.0         15.0      10590.0  \nShape:  (884, 9)\n\n\nThe number of columns has reduced from 19 to 9, streamlining the data for analysis.\n\n\n6.7.2.6 Calculate Poverty Rates Using Column Math\nTo estimate the poverty rate, divide the sum of C17002_002E (ratio of income to poverty in the past 12 months, &lt; 0.50) and C17002_003E (ratio of income to poverty in the past 12 months, 0.50 - 0.99) by B01003_001E (total population). This calculation provides the proportion of the population below the poverty line.\nNote: C17002_001E (ratio of income to poverty in the past 12 months, total) should theoretically represent the entire population, but it does not exactly match B01003_001E (total population). This discrepancy is minor, so it will be disregarded for the purpose of this calculation.\n\n# Get poverty rate and store ctlues in new column\nct_poverty_tract[\"Poverty_Rate\"] = (\n    (ct_poverty_tract[\"C17002_002E\"] + ct_poverty_tract[\"C17002_003E\"])\n    / ct_poverty_tract[\"B01003_001E\"] * 100\n)\n\n# Show dataframe\nct_poverty_tract.head(2)\n\n/Users/junyan/work/teaching/ids-f24/ids-f24/.ids-f24-venv/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nGEOID\ngeometry\nC17002_001E\nC17002_002E\nC17002_003E\nB01003_001E\nPoverty_Rate\n\n\n\n\n0\n09\n110\n535200\n09110535200\nPOLYGON ((710429.8 4638466.647, 710437.611 463...\n6650.0\n102.0\n213.0\n6664.0\n4.726891\n\n\n1\n09\n110\n881200\n09110881200\nPOLYGON ((727399.183 4633086.018, 727416.326 4...\n502.0\n318.0\n15.0\n10590.0\n3.144476\n\n\n\n\n\n\n\n\n\n6.7.2.7 Plotting the Results\nWith the spatial component linked to the census data, the results can now be visualized on a map. Plot the data to display poverty rates across counties, taking advantage of the merged spatial information for a clear geographic representation of poverty distribution.\n\n# Create subplots\nfig, ax = plt.subplots(1, 1, figsize = (12, 8))\n\n# Plot data\nct_poverty_tract.plot(column = \"Poverty_Rate\",\n                       ax = ax,\n                       cmap = \"plasma\",\n                       legend = True)\n\n# Stylize plots\nplt.style.use('bmh')\n\n# Set title\nax.set_title('Poverty Rates (%) in Connecticut', fontdict = {\n    'fontsize': '25', 'fontweight' : '3'})\n\nText(0.5, 1.0, 'Poverty Rates (%) in Connecticut')",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1 Data Visualization with Plotnine\nThis section was written by Julia Mazzola.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#data-visualization-with-plotnine",
    "href": "visualization.html#data-visualization-with-plotnine",
    "title": "7  Visualization",
    "section": "",
    "text": "7.1.1 Introduction\nHi! My name is Julia, and I am a Senior double majoring in Statistical Data Science and Economics. I’m excited to show you the power of data visualization with Plotnine, a Python library inspired by R’s ggplot2. Visualization is a crucial tool to effectively communicate your findings to your audience and Plotnine is a useful library to use.\n\n\n7.1.2 What is Plotnine?\nPlotnine uses grammar of graphics (Wilkinson, 2012) to create layered, customizable visualizations. Grammar of graphics is a framework that provides a systematic approach to creating visual representations of data by breaking down the plot into its fundamental components. To understand this better, think about how sentences have grammar, we can layer our graphics to create complex and detailed visulizations.\nComponents of the layered grammar of graphics:\n\nLayer: used to create the objects on a plot\nData: defines the source of the information to be visualized\nMapping: defines how the variables are represented in the plot\nStatistical transformation (stat): transforms the data, generally by summarizing the information\nGeometric object (geom): determines the type of plot type (e.g., points, lines, bars)\nPosition adjustment (position): adjusts the display of overlapping points to improve clarity\nScale: controls how values are mapped to aesthetic attributes (e.g., color, size)\nCoordinate system (coord): maps the position of objects onto the plane of the plot, and controls how the axes and grid lines are drawn\nFaceting (facet): used to split the data up into subsets of the entire dataset\n\nYou can make a wide array of different graphics with Plotnine. Some common examples are:\n\nScatterplot geom_point()\nBar Chart geom_bar()\nHistogram geom_histogram()\nLine Chart geom_line()\n\n\n\n7.1.3 Installing Plotnine\nTo use Plotnine you must install it into your venv first. The instructions are as follows:\nType this command into either conda, your terminal, gitbash, or whatever you use for package install for your venv.\nFor pip:\npip install plotnine\nFor conda:\nconda install -c conda-forge plotnine\nYou can import Plotnine without a prefix:\n\nfrom plotnine import *\n\nOr with with a prefix to access each component such as:\n\nimport plotnine as p9\n\nThis way is generally recommended for larger projects or when collaborating with others for better code maintainability. But for simplicity in this section I will use the first method.\nFor the examples we will be using NYC open data to visualize motor vehicle crashes from the week of June 30, 2024.\n\nimport pandas as pd\n\nnyc_crash = pd.read_feather('data/nyccrashes_cleaned.feather').dropna(subset=['borough'])\n\n\n\n7.1.4 Scatterplot\nFirstly, we will be creating a scatterplot. This can be done with geom_point(). Our scatterplot will be displaying Crash Locations based on the longitude and latitude of the crash sites.\nCreating a Basic Scatterplot\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n(ggplot(nyc_crash, aes(x='longitude', y='latitude')) +\n# Specifies graph type\n    geom_point() +\n# Creates labels for graphic\n    labs(title='Crash Locations',\n         x='Longitude',\n         y='Latitude') +\n# Because we are plotting maps we want 1:1 ratio\n# coord_fixed(): changes the ratio of the x and y axis\n    coord_fixed(ratio = 1))\n\n\n\n\n\n\n\n\nCustomizing a Scatterplot\nYou can customize your plot further by changing the color, edge color, transparency, size, or shape of your points. This is done in geom_point().\n\n(ggplot(nyc_crash, aes(x='longitude', y='latitude')) + \n# Changes what our points look like\n# color= changes the outline color\n# fill= changes the fill color\n# alpha= changes transparency\n# size= changes size\n# shape= chanegs shape (s = square)\n    geom_point(color = 'black', fill = 'purple', \n        alpha = 0.5, size = 2, shape = 's') + \n    labs(title='Crash Locations',\n         x='Longitude',\n         y='Latitude') +\n    coord_fixed(ratio = 1))\n\n\n\n\n\n\n\n\nThis scatterplot provides a lot of information, yet there are ways we can customize our plot to be more informative for our audience. We can create a scatterplot that differentiates by contributing factor.\nChanging Shape by Variables\nChanging shape of points by contributing_factor_vehicle_1:\n\n# List of top 5 reasons for the contributing facor\n# Abbreviating names for clairity\nfactor1 = {\"Driver Inattention/Distraction\": \"Distraction\",\n            \"Failure to Yield Right-of-Way\": \"Failure to Yield\",\n            \"Following Too Closely\": \"Tailgating\",\n            \"Unsafe Speed\": \"Unsafe Speed\",\n            \"Passing or Lane Usage Improper\": \"Improper Lane Use\"}\n\n# Filter the data to only include valid contributing factors\nconfact = nyc_crash.loc[nyc_crash['contributing_factor_vehicle_1'].isin(factor1)].copy()\n\n# Change to shortened names for better visability\nconfact.loc[:, 'contributing_factor_vehicle_1'] = confact[\n                                                        'contributing_factor_vehicle_1'\n                                                        ].replace(factor1)\n\n\n# Changes shape of point according to 'contributing_factor_vehicle_1'\n(ggplot(confact, aes(x='longitude', y='latitude',\n    shape ='contributing_factor_vehicle_1')) + \n    geom_point(alpha = 0.7) + \n    labs(title='Crash Locations by Top 5 Contributing Factors',\n        x='Longitude',\n        y='Latitude',\n        shape = 'Contributing Factor',\n        color= 'Contributing Factor') +\n    coord_fixed(ratio = 1) +\n    theme(figure_size = (7,5)))\n\n\n\n\n\n\n\n\nChanging Color by Variables\nTo add color coordination to your plot in Plotnine, specify the variable you want to use for coloring by including color='variable' within the aes() function. This enables you to visually distinguish different categories in your dataset, enhancing the clarity and interpretability of your plot.\nChanging color of point according to borough:\n\n# color= changhes color according to 'borough'\n(ggplot(nyc_crash, aes(x='longitude', y='latitude', color = 'borough')) +\n    geom_point() +\n    labs(title='Crash Locations',\n        x='Longitude',\n        y='Latitude',\n# Changes key title to 'Borough'\n        color= 'Borough') +\n    coord_fixed(ratio = 1) +\n# legend_position= changes where the legend is located\n    theme(figure_size = (7,5), legend_position='bottom'))\n\n\n\n\n\n\n\n\nAs you can see, each borough is represented by its own color, allowing the audience to easily identify which borough the crash occurred in.\nChanging color of points by contributing_factor_vehicle_1:\n\n# color= changes color according to 'contributing_factor_vehicle_1'\n(ggplot(confact, aes(x='longitude', y='latitude',\n    color ='contributing_factor_vehicle_1')) + \n    geom_point() + \n    labs(title='Crash Locations by Top 5 Contributing Factors',\n        x='Longitude',\n        y='Latitude',\n        color= 'Contributing Factor') +\n        coord_fixed(ratio = 1) +\n# Changes plot size to be larger\n    theme(figure_size = (7,5)))\n\n\n\n\n\n\n\n\nThis graph uses color to distinguish what contributing factor caused the crash.\nAdding Linear Regression Line to Plot\nIf you want to fit a linear regression line, use geom_smooth(). Adding this to your plot can be really helpful to visualize trends of your data easier. To add a linear regression line to your scatterplot, you would include the following line of code:\n\ngeom_smooth(method='lm', se=False, color='red')\n\n&lt;plotnine.geoms.geom_smooth.geom_smooth at 0x1267e4860&gt;\n\n\n\n\n7.1.5 Bar Chart\nAnother common use for displaying data is a bar chart. You can create one with geom_bar(). We will start with a simple chart of crashes by borough.\nCreating a Basic Bar Chart\n\n(ggplot(nyc_crash, aes(x='borough')) + # Use 'borough' for the x-axis\n    geom_bar(fill='purple') +\n    labs(title='Number of Crashes by Borough', \n        x='Borough',\n        y='Crash Count'))\n\n\n\n\n\n\n\n\nCustomizing your Bar Chart\nYou can change up your bar chart a couple of different ways. You can handpick colors you want, designate it to variables, flip orientation, etc:\n\n# Designate your preffered colors (pastel color codes)\ncolors = ['#B3FFBA', '#E1C6FF', '#FFB3BA', '#BAE1FF', '#FFD5BA']\n\n# Adding fill= changes the color of bar according to variable\n(ggplot(nyc_crash, aes(x='borough', fill = 'borough')) +\n# Assigns your preffered colors\n   geom_bar(fill = colors) +\n# Flips orientation of the chart\n   coord_flip() + \n   labs(title='Number of Crashes by Borough', \n        x='Borough',\n        y='Crash Count'))\n\n\n\n\n\n\n\n\nMultivariable Bar Chart\nYou can also split up a bar chart to make it visually easier to understand.\n\n# Using 'confact' dataset again for better visualization\n(ggplot(confact, aes(x='contributing_factor_vehicle_1', fill='borough')) +\n    geom_bar() +\n    labs(title='Top 5 Contributing Factors by Borough',\n        x='Top 5 Contributing Factor Vehicle 1',\n        y='Number of Crashes',\n# Changes key name to \"Borough\"\n        fill ='Borough') +\n# size= creates smaller text\n# angle= rotates x-axis text for readability\n# figure_size= creates a larger image\n    theme(axis_text_x=element_text(size=9, angle=65), \n            figure_size= (7,7), legend_position='bottom'))\n\n\n\n\n\n\n\n\n\n\n7.1.6 Histogram\nAnother useful way to display data is a histogram. You can create one with geom_hisogram(). Using a histogram is very useful when displaying continuous data.\nBasic Histogram\n\n(ggplot(nyc_crash, aes(x='number_of_persons_injured')) +\n# bins= sets the amount of bars in your histogram\n    geom_histogram(bins=10, alpha=0.8, fill='green') + \n    labs(title='Distribution of Persons Injured',\n        x='Number of Persons Injured',\n        y='Count of Crashes'))\n\n\n\n\n\n\n\n\nWith a histogram it is very easy to understand trends for a dataset and you can see that our NYC crash data is positively skewed.\nMultivariable Histogram\nSimilar to bar charts, you can make Histograms that display more than one variable.\n\n(ggplot(confact, aes(x='number_of_persons_injured', fill = 'borough')) +\n# binwidth= changes width of your bars\n# color= changes outline color for better visability\n    geom_histogram(binwidth=1, color = 'black') +\n    labs(title='Distribution of Persons Injured',\n        x='Number of Persons Injured',\n        y='Count of Crashes',\n        fill = 'Borough') +\n        theme(legend_position='bottom'))\n\n\n\n\n\n\n\n\nOverlapping Histogram\nHistograms can also be useful when comparing multiple categories. Here we are comparing Manhattan and Brooklyn’s number of persons injured with an overlapping histogram.\n\n# Creating plot if crash is in 'MANHATTAN' or 'BROOKLYN'\n(ggplot(nyc_crash[nyc_crash['borough'].isin(['MANHATTAN', 'BROOKLYN'])], \n        aes(x='number_of_persons_injured', fill='borough')) +\n    geom_histogram(bins=10) +\n    labs(title='Persons Injured: Manhattan vs Brooklyn',\n        x='Number of Persons Injured',\n        y='Count',\n        fill='Borough'))\n\n\n\n\n\n\n\n\n\n\n7.1.7 Line Chart\nLine charts are great for time-series data and can be created with geom_line(). This type of chart is particularly useful for identifying patterns, fluctuations, and trends, making it easier to understand how a variable changes over a specified period. We will create one analyzing Number of Crashes by Hour.\nBasic Line Chart\n\n# Finding crashes per hour\nnyc_crash['crash_datetime'] = pd.to_datetime(nyc_crash['crash_datetime'])\n\n# Extract hour\nnyc_crash['crash_hour'] = nyc_crash['crash_datetime'].dt.hour\n\n# Count crashes per hour\ncrash_counts = (nyc_crash.groupby(['crash_hour'])\n                .size().reset_index(name='crash_count'))\n\n\n# Plot crashes by hour\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count')) +\n# Creates the line chart\n    geom_line() +\n# Adds points for better visibility\n    geom_point() +\n    labs(title='Number of Crashes by Hour',\n        x='Hour',\n        y='Crashes') +\n# Formats the x-axis to display ticks by every 2 hours\n    scale_x_continuous(breaks=range(0, 24, 2)))\n\n\n\n\n\n\n\n\nThis example is excellent for understanding the grammar of graphics. As you can see, we use geom_line() to create the line chart, while also adding geom_point(), which is typically used for scatterplots, to make the figure clearer by layering additional details.\nMultivariable Line Chart\nSimilarly to the other figures you can create a line chart with multiple variables. Now we will create a chart with number of crashes by borough.\n\n# Setting crash counts to also include borough\ncrash_counts = nyc_crash.groupby(['crash_hour', \n    'borough']).size().reset_index(name='crash_count')\n\n# Plots crashes by hour with different lines for each borough\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count', \n    color='borough')) +\n# size= changes the thinkness of the lines\n    geom_line(size=0.5) +\n    labs(title='Number of Crashes by Hour and Borough',\n        x='Hour of the Day',\n        y='Number of Crashes',\n        color = 'Borough') +\n    scale_x_continuous(breaks=range(0, 24, 2)) +\n    theme(legend_position='bottom'))\n\n\n\n\n\n\n\n\n\n\n7.1.8 Faceting Your Plots\nTo organize your data in a way that enhances interpretability, you can utilize facet_grid() or facet_wrap(). This approach allows for the creation of separate plots based on categorical variables, making it easier to identify trends and patterns. You can facet any type of plots, scatterplots, bar charts, histograms, line charts, etc. using one or two variables.\nScatterplots per Facet\nScatterplot of Crash Locations by Contributing Factor with facet_wrap():\n\n(ggplot(confact, aes(x='longitude', y='latitude')) +  \n    geom_point(alpha=0.5) +\n# Creates separate plots for each contributing factor\n    facet_wrap('contributing_factor_vehicle_1') +  \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n        x='Longitude',\n        y='Latitude') +\n    coord_fixed(ratio = 1))\n\n\n\n\n\n\n\n\nScatterplot of Two Variables, Crash Locations Contributing Factor and Borough with facet_grid():\n\n(ggplot(confact, aes(x='longitude', y='latitude')) +  \n    geom_point(alpha = 0.5) +\n# Creates a grid of subplots based on the values of two variables\n# ~'contributing_factor_vehicle_1' by 'borough'\n    facet_grid('contributing_factor_vehicle_1 ~ borough') +  \n    labs(title='Crash Locations by Top 5 Contributing Factor',\n        x='Longitude',\n        y='Latitude') +\n# Changes angle of text and size of the graphic\n    theme(axis_text_x=element_text(angle=90),\n# sprip_text=element_text changes text size of the facet titles\n        strip_text=element_text(size=5.5)) +\n    coord_fixed(ratio = 1)) \n\n\n\n\n\n\n\n\nBar Chart per Facet\nBar chart of Contributing Factors by Borough with facet_wrap:\n\n(ggplot(confact, aes(x='contributing_factor_vehicle_1', fill='borough')) +\n    geom_bar() +\n    labs(title='Top 5 Contributing Factors by Borough',\n        x='Top 5 Contributing Factor Vehicle 1',\n        y='Number of Crashes',\n        fill = 'Borough') +\n    facet_wrap('~ borough') + \n    theme(axis_text_x=element_text(size=9, angle=65), \n            figure_size= (7,7), legend_position='bottom'))\n\n\n\n\n\n\n\n\nHistograms per Facet\nHistogram of Crashes per Hour by Borough with facet_wrap:\n\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count', fill = 'borough')) +\n    geom_bar(stat='identity') +\n    labs(x='Crash Hour', \n        y='Number of Crashes', \n        title = \"Crashes by Hour\") +\n    theme(legend_position='bottom') +\n    facet_wrap('~ borough'))\n\n\n\n\n\n\n\n\nLine Chart per Facet\nYou can use plot each variable by on separate panels with facet_wrap().\n\n(ggplot(crash_counts, aes(x='crash_hour', y='crash_count')) +\n    geom_line() +\n    # Breaks the figure up by borough\n    facet_wrap(\"borough\") + \n    labs(title='Number of Crashes by Hour and Borough',\n        x='Hour of the Day',\n        y='Number of Crashes'))\n\n\n\n\n\n\n\n\n\n\n7.1.9 Conclusion\nPlotnine is a very powerful tool to make impactful and detailed graphics. The flexibility of its grammar of graphics approach means there are endless ways to modify, enhance, and be creative with your plots. You can layer geoms, adjust aesthetics, and apply scales, facets, and themes.\nCreating Specific Plots\n\nScatterplot geom_point()\nBoxplot geom_box()\nHistogram geom_histogram()\nLine Chart geom_line()\nBar Chart geom_bar()\nDensity Plot geom_denisty()\n\nFormatting and Customizing Your Figure:\n\nfill: to change the color of the data\ncolor: to change the color of the borders\nalpha: to change the transparency\nbins: to change the number of bins\nfigure_size: to change size of graphic\ngeom_smooth: to add a smoothed line\nfacet: plot each group on a separate panel\n\nfacet_wrap(): creates a series of plots arranged in a grid, wrapping into new rows or columns as needed\nfacet_grid(): allows you to create a grid layout based on two categorical variables, organizing plots in a matrix format\n\ntheme: change overall theme\n\nThere are many other features and customizations you can do with Plotnine. For more information on how to leverage the full potential of this package for your data visualization needs check out Plotnine’s Graph Gallery.\nHappy plotting!\nFurther Readings\nPython Graph Gallery. (2024). Plotnine: ggplot in python. Python Graph Gallery. https://python-graph-gallery.com/plotnine/\nSarker, D. (2018). A comprehensive guide to the grammar of graphics for effective visualization of multi-dimensional data. Towards Data Science. https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#spatial-data-visualization-with-google-map",
    "href": "visualization.html#spatial-data-visualization-with-google-map",
    "title": "7  Visualization",
    "section": "7.2 Spatial Data Visualization with Google Map",
    "text": "7.2 Spatial Data Visualization with Google Map\nHi! My name is Jack Bienvenue, a senior in Statistical Data Science.\nIn this section, I’ll walk you through how to begin with spatial data visualizations.\nSpecifically, in this section we’ll talk about using Google Maps as a tool to visualize spatial data.\nWe will start by talking about Google and their spatial data products, but work our way into something more aligned with our needs, which is integrating code into our Python scripts to generate Google Maps versions of our outputs.\nThis section will be capped off with some example code of using Google Maps to create a heatmap of crash data in New York City.\nSkip to proceed to purely technical information by clicking here\nEnjoy and keep learning!\n\n7.2.1 Introducing Spatial Data\n\n7.2.1.1 Having fun with spatial data\nSpatial data handling and visualization introduces new challenges, softwares, and packages.\nSpatial data operations can be time-consuming and the pathway to your desired results is often unclear.\nStay patient, have fun\n\n\n\nMap of all the locations in NYC where I have eaten a bagel\n\n\nAbove is an example of me having fun with spatial data.\nThere are so many fun things to do with data visualization. Remember, you have the skillset to do things that almost nobody can! Make the most of it for yourself and for the world.\n\n\n7.2.1.2 Why Spatial Data?\nSpatial trends matter. They allow us to do things like:\n\nUnderstand geographic localizations of trends\n\nUnveil hidden trends\n\nEvaluate how social factors may influence trends\n\nCreate information to address disparities\n\nProvide recommendations to relevant local authorities\n\n\n\n\n7.2.2 Google & Spatial Data\n\n7.2.2.1 Google Spatial Data Products:\nGoogle offers several products for performing Geographic Information Science (GIS) tasks. These include:\n\nGoogle MyMaps\n\nA simple, no-code way to visualize data\n\nGoogle Earth Engine\n\nEnhanced computing capabilities for analytic tasks\n\nGoogle Maps Platform\n\nComprehensive map construction, focused on visualization\n\n\n\n\n7.2.2.2 Google MyMaps\nGoogle MyMaps allows for visualization of line and point data.\n\n\n\nImage of MyMaps UI. As we can see, there is a very small extent of functionality.\n\n\n\n\n7.2.2.3 Google Earth Engine\nGoogle Earth Engine is used for remote sensing applications.\nIt uses JavaScript.\nIt pairs well with landcover classification to aid in tasks like:\n\nLandcover Classification\n\nTracking deforestation\nHabitat Monitoring\nSolar Panel Identification\n\n\n\n\n7.2.2.4 Google Earth Engine Example\nHere’s an example of Google Earth Engine being used to identify commercial solar properties to fill in missing data:\n\n\n\nExample of Landcover classifcation model in GEE\n\n\n\n\n7.2.2.5 Google Maps Platform\nGoogle Maps Platform significantly broadens capabilities.\nIt allows us to perform tasks such as:\n\nCoalescing nearby points into clusters for zooming\nCreating heat maps from point data\nEmbedding maps into websites\nAutomatically translating a map into a user’s language\n3D geospatial visualization\nand more!\n\n\n\n\n7.2.3 Getting Started in Google Maps Platform\n\n7.2.3.1 Why Google Maps Platform?\nWe want to be able to include maps right inside of our Python code, so Google Maps will be our platform of choice.\nGoogle Maps Platform uses Google Cloud computing.\nWe must log in and do some setup there first.\n\n\n7.2.3.2 Google Cloud Platform\nGoogle Cloud Platform is accessible here.\nUConn terminated Google accounts in 2024, so to access Google Cloud Console, sign in with your personal email.\n\n\n7.2.3.3 Accessing the console\nTo access Google Maps Platform, we must enter our Google Cloud Console.\nNavigate to console.cloud.google.com to view your console. It should look like this. Press “select project” and create your new project.\n\n\n\n7.2.3.4 Building Project\n→ “Select a Project”\n→ “New Project”\n→ Name your project, set location as ‘No Organization’\n→ “Create”\n→ Select newly created project\n→ Next, we’ll set up an API key in the following slides\n\n\n7.2.3.5 API: What is an API?\nAPI stands for “Application Programming Interface.”\nGoogle Maps API is a set of tools allowing us to perform tasks like:\n\nVisualizing data and embedding it into websites\nRoute finding\n3D map viewing\n\nStreet view, etc.\n\nAnd more!\n\n\n\n7.2.3.6 API: What are API keys?\nAn API Key allows us to access our projects exclusively\nGenerate an API key for your Maps project in Google Cloud here:\n\n\n\nWhere to navigate to generate API key\n\n\n\n\n7.2.3.7 API: Enabling necessary APIs\nIn this example, we will visualize our data as a heat map. For this we will need to enable the following APIs:\n\nMaps JavaScript API\nMaps Datasets API\nPlaces API\n\n(Select ‘APIs and Services’ from the left sidebar to access)\n\n\n7.2.3.8 Python and Google Maps\nLet’s begin by downloading our packages in terminal or an equivalent command shell:\n% pip install googlemaps \n% pip install gmplot\nNow we can start working with Google Maps in Python!\n\ngooglemaps\n\nProvides access to Google Maps Platform services in a Python client\n\ngmplot\n\nPrimarily for plotting\n\n\n\n\n\n7.2.4 Using googlemaps and gmplot\n\n7.2.4.1 gmplot: Initialization\nWe’ll use the standardized nyccrashes_cleaned.feather file provided in the coursenotes for our examples.\n% pip install pyarrow\n\nimport pandas as pd\nimport gmplot\nimport os\ndf = pd.read_feather('data/nyccrashes_cleaned.feather')\n\n\n\n7.2.4.2 gmplot\nHere’s some example code for gmplot using the cleaned NYC Crashes Dataset:\nimport os\nimport pandas as pd\nimport gmplot\n\n## Load the dataset using pd.read_feather()\ndf = pd.read_feather('data/nyccrashes_cleaned.feather')\n\n#NaN values prevent plotting, let's remove them:\ndf = df.dropna(subset=['latitude', 'longitude'])\n\n## Insert API key (substitute with your own!)\napi_key_file_path = \"api_key.txt\"\nwith open(file_path, 'r') as file:\n    api_key = file.read().strip()\n\n## Center the map on NYC using the mean center\ncenter_lat = df['latitude'].mean()\ncenter_lon = df['longitude'].mean()\n\n## Create gmplot object (third argument is the zoom level)\ngmap = gmplot.GoogleMapPlotter(center_lat, center_lon, 11, apikey=api_key)\n\n## Extract latitudes and longitudes directly\nlatitudes = df['latitude'].tolist()\nlongitudes = df['longitude'].tolist()\n\n## Plot all points at once\ngmap.scatter(latitudes, longitudes, marker=True, size=20, color='red')\n\n## Define output path\noutput_dir = \"map_outputs\"\noutput_path = os.path.join(output_dir, \"nyccrashes_map.html\")\n\n## Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n## Save the map\ngmap.draw(output_path)\n\n\n7.2.4.3 gmplot example code output:\nYou would retrieve this as an html generated in the specified directory, but for this presentation I’ll show this as an image:\n\n\n\nOutput of example code\n\n\n\n\n7.2.4.4 gmplot: Heat Maps\nHere’s some example code for a heat map:\n## Now create a heatmap\n\n## Create a new gmplot object for the heatmap\nheatmap = gmplot.GoogleMapPlotter(center_lat, center_lon, 11,\n          apikey=api_key)\n\n## Plot the heatmap\nheatmap.heatmap(latitudes, longitudes)\n\n## Define output path for heatmap\nheatmap_output_path = os.path.join(output_dir, \"nyccrashes_heatmap.html\")\n\n## Save the heatmap\nheatmap.draw(heatmap_output_path)\n(Woods 2017)\n\n\n\n7.2.5 gmplot Heat Map Example\n\n\n\nHeat map output; Remark: multiple gmplot html outputs might not reliably work for a single script\n\n\n\n\n7.2.6 Googlemaps\nGooglemaps as a package is valuable in allowing us to access advanced functionality, beyond the scope of this write-up. Access more information here. (Google 2023)\n\n\n7.2.7 Conclusion\n\n7.2.7.1 Geospatial Visualization\nThere are a range of different geospatial visualization platforms.\nBut, using Python, you can do all the spatial visualization you need right in your code!\n\n\n7.2.7.2 Thank you!\nBest of luck with your spatial data visualization! Keep learning.\n\n\n\n7.2.8 References\n\nWoods, M. (2017). gmplot: Plotting data on Google Maps, the easy (stupid) way. URL: Https://Github.Com/Gmplot/Gmplot.\nGoogle. (2023). googlemaps (v4.7.3) [Python package]. PyPI. https://pypi.org/project/googlemaps/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#animating-plots-and-maps",
    "href": "visualization.html#animating-plots-and-maps",
    "title": "7  Visualization",
    "section": "7.3 Animating Plots and Maps",
    "text": "7.3 Animating Plots and Maps\n\n7.3.1 Introduction\nHello everyone! My name is Rahul Manna. I am an undergraduate junior doing a dual degree in Statistical Data Science and Mechanical Engineering. I will be showing you how you can animate plots and maps using Matplotlib’s FuncAnimation function.\nAnimated maps and plots are valuable for showing changes over time or across locations, making trends and patterns easier to see. They’re useful in fields like public health, where animated maps can show how a disease spreads, or in economics, where plots can track market trends. Environmental scientists also use animated weather maps to illustrate seasonal shifts. These visuals make complex data clearer and help in understanding and decision-making.\nOverview\n\nMatplotlib Review\nAnimating Plots\nAnimating Maps\nSaving your Animation\n\n\n\n7.3.2 Matplotlib\n\nDeveloped by John D. Hunter in 2003 as a tool for scientific computing in Python.\nInitially aimed at providing MATLAB-like plotting capabilities but has evolved into one of the most widely-used plotting libraries.\n\nSource: Hunter & Matplotlib Development Team (2023a)\n\n\n\n\n\n\nFun Fact\n\n\n\nMatplotlib was used for data visualization during the 2008 landing of the Phoenix spacecraft on Mars and in generating the first image of a black hole (Source: Collaboration (2019)).\n\n\n\n7.3.2.1 Installation:\nTo install matplotlib, you can use either of the following lines in your terminal or conda prompt respectively.\npip install matplotlib # pip users\nconda install -c conda-forge matplotlib # conda users\n\n\n7.3.2.2 Basic Matplotlib Commands\nMajority of plotting with matplotlib is done with the pyplot module which can be imported with the following code.\nimport matplotlib.pyplot as plt\nThese are some of the most common matplotlib.pyplot commands.\n\nplt.plot() : Plot y versus x as lines and/or markers.\nplt.scatter() : Create a scatter plot of points.\nplt.bar() : Create bar charts.\nplt.hist() : Create histograms.\nplt.xlabel() : Set the label for the x-axis.\nplt.ylabel() : Set the label for the y-axis.\nplt.title() : Set the title of the plot.\nplt.legend() : Display a legend for the plot.\nplt.subplots() : Create a figure and a grid of subplots.\nplt.show() : Display all open figures.\n\nSource: Hunter & Matplotlib Development Team (2023a)\n\n\n7.3.2.3 Some Examples\nHere are some examples of plots made using matplotlib.pyplot.\n\n7.3.2.3.1 Coin Toss\nLet’s create a line plot to show how the proportion of heads in coin tosses changes as the number of tosses increases.\n\nimport random\nimport matplotlib.pyplot as plt\n\nrandom.seed(3255)\n\ndef prob_heads(trials):\n    result = []\n    prop_heads = []\n    for i in range(trials):\n        toss = random.randint(0,1)\n        result.append(toss)\n        prop_heads.append(sum(result)/len(result)) \n    return prop_heads\n\nplt.figure(figsize=(6,3))\nplt.hlines(0.5,0,2500,linestyles='dashed')\nplt.plot(prob_heads(2500),color='tab:red')\nplt.ylim(0,1)\nplt.title(\"Coin Toss\")\nplt.ylabel('Proportion of Heads')\nplt.xlabel('Number of Tosses')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.3.2.3.2 Bar Chart\nLet’s create a simple bar plot of the crashes in New York City by borough in the week of July 30, 2024.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ncrash_data = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\ncrash_data = crash_data.groupby('borough')\n\nboroughs = crash_data.groups.keys()\n\nheights = [len(crash_data.get_group(x)) for x in boroughs]\n\nplt.bar(boroughs,height=heights)\nplt.title('Bar Plot of Crashes in NYC by Borough - 6/30/24 - 7/7/24')\nplt.xlabel('Borough')\nplt.ylabel('Number of Crashes')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n7.3.3 Animating Plots\n\n7.3.3.1 Matplotlib’s FuncAnimation\nfrom matplotlib.animation import FuncAnimation\nFuncAnimation is used to create animations in Matplotlib by repeatedly calling a user-defined function.\nanim = FuncAnimation(fig,func,frames,interval,repeat,repeat_delay)\nKey Inputs\n\nfig: Matplotlib figure object.\nfunc: The update function for each frame.\nframes: Sequence or number of frames.\ninterval: Time interval between frames (ms).\nrepeat: Whether to repeat animation (True/False).\nrepeat_delay: Delay before repeating (ms).\n\nSource: Hunter & Matplotlib Development Team (2023b)\n\n\n7.3.3.2 Coin Toss Animtion\nUsing FuncAnimation, we can animate the coin toss plot we previously made.\n\nprop_heads = prob_heads(2500)\n\nframes = range(len(prop_heads))\n\nfig, ax = plt.subplots(figsize=(12,6))\n\ndef update(frame):\n    # Clear previous frame\n    ax.clear()\n\n    # Add title, and labels\n    ax.set_title('Coin Toss')\n    ax.set_ylabel('Proportion of Heads')\n    ax.set_xlabel('Number of Tosses')\n    ax.set_ylim(0,1)\n\n    # Plot data\n    ax.hlines(0.5,0,frame+1,linestyles='dashed')\n    ax.plot(range(1,frame+1),prop_heads[:frame],color='tab:red')\n\n\nanim = FuncAnimation(fig,update,frames=frames,repeat=False)\n\nanim.save('coin_toss.gif',writer='Pillow',fps=50)\n\nplt.show()\n\n\n\n\n\n\n\n7.3.3.3 A Step Further - Coin Toss Animation\nWe can take this a step further by labeling the current proportion value for each frame.\nprop_heads = prob_heads(2500)\n\nframes = range(len(prop_heads))\n\nfig, ax = plt.subplots(figsize=(12,6))\n\ndef update(frame):\n    ax.clear()\n    ax.set_title('Coin Toss')\n    ax.set_ylabel('Proportion of Heads')\n    ax.set_xlabel('Number of Tosses')\n    ax.hlines(0.5,0,frame+1,linestyles='dashed')\n    ax.set_ylim(0,1)\n\n    # Add text\n    ax.text(frame+1,prop_heads[frame]*1.05,f'{prop_heads[frame]:.3f}',weight='bold')\n\n    ax.plot(range(1,frame+1),prop_heads[:frame],color='tab:red')\n\n\nanim = FuncAnimation(fig,update,frames=frames)\n\nanim.save('coin_toss_with_txt.gif',writer='Pillow',fps=50)\n\nplt.show()\n\n\n\n\n\n\n\n7.3.3.4 Bar Chart Animation\nLet’s animate the bar chart we created in section 8.1.5. Here I am using plt.barh to create a horizontal bar chart.\ncrash_data['crash_datetime'] = pd.to_datetime(crash_data['crash_datetime'], \nformat='%Y-%m-%d %H:%M:%S')\n\nfig, ax = plt.subplots(figsize=(12,6))\n\ndef update(frame):\n    ax.clear()\n    current_data = crash_data[crash_data['crash_datetime'] &lt;= frame]\n    \n    # Group data by borough and count the number of crashes\n    grouped_data = current_data.groupby('borough').size().reset_index(\n                                                        name='num_crashes')\n\n    # Sort by number of crashes for consistent bar ordering\n    grouped_data = grouped_data.sort_values(by='num_crashes', ascending=True)\n    \n    # Create horizontal bar chart\n    bars = ax.barh(grouped_data['borough'], grouped_data['num_crashes'])\n    \n    # Set titles and labels\n    ax.set_title('Bar Plot of Crashes in NYC by Borough')\n    ax.set_xlabel('Number of Crashes')\n    ax.set_ylabel('Borough')\n    ax.legend([f'Data up to {frame}'], prop={'size': 'large'})\n    \n    # Annotate bars with crash numbers\n    for i, bar in enumerate(bars):\n        ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2,\n                f'{grouped_data[\"num_crashes\"].iloc[i]}', va='center', \n                ha='left', color='black')\n        \n\nanim = FuncAnimation(fig,update,frames=pd.date_range(\n    start=crash_data['crash_datetime'].min(),\n    end=crash_data['crash_datetime'].max(),freq='h'))\n\nanim.save('bar_plot_animation.gif',writer='Pillow',fps=15)\nplt.show()\n\n\n\n7.3.3.5 Relative Bar Plot Animation\nSimilarly, we can plot and animate the relative bar plot of crashes by borough.\nfig, ax = plt.subplots(figsize=(12,6))\n\ndef update(frame):\n    ax.clear()\n    current_data = crash_data[crash_data['crash_datetime'] &lt;= frame]\n\n    total_crashes = len(current_data)\n    grouped_data = current_data.groupby('borough')\n    \n    boroughs = sorted(grouped_data.groups.keys())\n    height = [len(grouped_data.get_group(x))/total_crashes for x in boroughs]\n    \n    # Create horizontal bar chart\n    bars = ax.bar(boroughs,height)\n    \n    # Set titles and labels\n    ax.set_title('Bar Plot of Relative Percentage of Crashes in NYC by Borough')\n    ax.set_xlabel('Proportion of NYC Crashes')\n    ax.set_ylabel('Borough')\n    ax.legend([f'Data up to {frame}'])\n    ax.set_ylim(0,1)\n    \n    # Annotate bars with crash numbers\n    for i, bar in enumerate(bars):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n                f'{height[i]:.4}', va='bottom', ha='center', color='black')\n        \nanim = FuncAnimation(fig,update,frames=pd.date_range(start=\n                                crash_data['crash_datetime'].min(),\n                        end=crash_data['crash_datetime'].max(),freq='h'))\n\nplt.show()\n\n\n\n\n7.3.4 Animating Maps\nThe same FuncAnimation function can be used to animate maps.\n\n7.3.4.1 Basic Idea\n\nProcess data as a Geo-dataframe with Geopandas\nObtain map using libraries like Contextily, Cartopy, or Basemap.\nCreate a frame fuction that:\n\nClears previous plot\nPlots geo-spatial data\nAdd a basemap background\n\nAnimate the map.\n\n\n\n7.3.4.2 More Packages\nTwo more packages are needed to create an animated map.\nGeopandas (Source: Team (2024)) {.smaller} - Extends Pandas to support spatial (geometric) data operations. - Handles GeoDataFrames that store geometries like points, lines, and polygons. - Provides support for reading and writing geospatial data formats - Integrates easily with Matplotlib for plotting spatial data.\nContextily (Source: Kharlamov (2023)) {.smaller} - Adds basemaps to Matplotlib and Geopandas plots. - Fetches tile-based maps from popular online sources (like OpenStreetMap).\nInstallation\npip install geopandas contextily # pip users\nconda install -c conda-forge geopandas contextily # conda users\nimport geopandas as gpd\nimport contextily as ctx\n\n\n7.3.4.3 Step 1: Process Data as GeoDataFrame\n# Read Data\ncrash_data = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\n\n# Make Date Time Column\ncrash_data['crash_datetime'] = pd.to_datetime(crash_data['crash_datetime'],\n                                                format='%Y-%m-%d %H:%M:%S')\n\n# Create Hour and Date columns\ncrash_data['date'] = crash_data['crash_datetime'].dt.date\ncrash_data['hour'] = crash_data['crash_datetime'].dt.hour\n\n# Create GeoPandas Data frame\ncrash_gdf = gpd.GeoDataFrame(crash_data,\n    geometry=gpd.points_from_xy(crash_data['longitude'], \n                            crash_data['latitude']),crs=\"EPSG:4326\")\n\n# Transform from EPSG 4326 to EPSG 3857\ncrash_gdf = crash_gdf.to_crs(epsg=3857)\n\n# Group crash_gdf by date and then hour\ncrash_grouped = crash_gdf.groupby(['date','hour'])\n\n\n7.3.4.4 Step 2: Get Basemap\nnewyork = ctx.Place(\"New York\", source=ctx.providers.OpenStreetMap.Mapnik,zoom=12)\nny_img = newyork.im # Get Map Image\nny_bbox = newyork.bbox_map # Get Coordinates EPSG 3857\n\n\n\n7.3.4.5 Step 3: Frame Function\ncrash_grouped = crash_gdf.groupby(['date','hour'])\n\nkeys = [key for key in grouped.groups.keys()] # frames\n\nfig, ax = plt.subplots(figsize=(6,7))\n\ndef update(frame):\n    # Extrate date, hr from input\n    date, hr = frame\n\n    # Get group\n    df = crash_grouped.get_group((date,hr))\n\n    ax.clear() # Clear previous frame\n\n    # Plot using Geopandas df.plot\n    df.plot(ax=ax,color='red',edgecolor='k',label=f'Date: {date}\\nHour: {hr}')\n    \n    ax.imshow(ny_img,extent=ny_bbox) # add basemap\n    ax.legend(loc='lower right')\n    ax.set_title(\"Crashes by Hour in NYC - Week of 6/30/24 to 7/7/24\")\n    ax.axis('off')\n\n\n7.3.4.6 Step 3: Animate the Map\nfig.subplots_adjust(left=0,right=1,top=0.95,bottom=0)\n\nanim = FuncAnimation(fig,update,frames=keys)\nanim.save(\"crash_maps_nyc_24-6-30_24-7-7_300dpi.gif\",writer='Pillow',fps=2,dpi=300)\n\nplt.show()\n\nNote: a lower dpi (265) and size than shown in the chunk above was used for this figure to keep it under GitHub’s 100 Mb limit.\n\n\n7.3.4.7 Other Basemaps\nThe Contextily has several other basemaps available. Here are some of there most popular options.\n\nctx.providers.Stamen.Toner: Black-and-white map, minimal details.\nctx.providers.Stamen.Terrain: Topographic map with terrain shading.\nctx.providers.OpenStreetMap.Mapnik: Standard OpenStreetMap, detailed streets.\nctx.providers.CartoDB.Positron: Light map with subtle details.\nctx.providers.CartoDB.Voyager: Colorful and detailed street map.\nctx.providers.Esri.WorldImagery: High-res satellite imagery.\n\nSource: Kharlamov (2023)\n\n\n7.3.4.8 Satellite Example\nThis is an example of the same map animated with a satellite basemap.\n\n\n\n\n7.3.5 Advance Example - Animating Maps\nThe NYC Open Data contains traffic speed data that is updated several times a day. We will be plotting this data on the map we previously made.\nLink to data: DOT Traffic Speeds NBE\nThese are some of the variables from the dataset:\n\nDATA_AS_OF: Timestamp when the data was recorded.\nSPEED: Recorded speed in miles per hour.\nLINK_ID: Identifier for road segments.\nLINK_POINTS: List of geographic coordinates for the road segment.\nENCODED_POLY_LINE: Encoded string representing the polyline of the segment.\nBOROUGH: NYC borough where data is collected (e.g., Manhattan, Brooklyn).\n\n\n7.3.5.1 Traffic Speed Data\nLet’s process the DATA_AS_OF column as a datetime column.\nSince the data only contains the speed which is different for different roads, we can create a normalized speed column. This column contains the traffic speed at the time divided by the average speed over that week for each ‘LINK_ID’ or unique section of the road.\nspeed_data = pd.read_feather(\"data/DOT_Traffic_Speeds_NBE_20241005.feather\")\n\n# Create Date Column\nspeed_data['DATA_AS_OF'] = pd.to_datetime(speed_data['DATA_AS_OF'],\n                                        format='%m/%d/%Y %I:%M:%S %p')\nspeed_data['hour'] = speed_data['DATA_AS_OF'].dt.hour\nspeed_data['date'] = speed_data['DATA_AS_OF'].dt.date\n\ngroup_link_id = speed_data.groupby(['LINK_ID'])\n\n# Function to Normalize speed\ndef normalize_speed(row):\n    group = group_link_id.get_group(row['LINK_ID'])\n    if row['SPEED']:\n        row['normalized_speed'] = row['SPEED'] / group['SPEED'].mean()\n    else:\n        row['normalized_speed'] = None\n    return row\n\nspeed_data = speed_data.apply(normalize_speed,axis=1)\n\n\n7.3.5.2 Extract Polyline\nWe can extract the polyline to plot on the map.\nfrom shapely.geometry import LineString\nimport polyline\n\n# Example encoded-poly-line\n# {q{vFrzrcMta@kl@hDiInBiIr@}F\\mHOsR\n\ndef poly(x):\n    try:\n        # Decode Polyline\n        decoded = polyline.decode(x)\n        \n        # Create a box for NYC to filter invalid coordinates\n        valid_lat_range = (40.47, 40.91)\n        valid_lon_range = (-74.25, -73.7) \n\n        # Filter coordinates\n        cleaned_coords = [(lon, lat) for lat, lon in decoded\n            if valid_lat_range[0] &lt;= lat &lt;= valid_lat_range[1] and \n            valid_lon_range[0] &lt;= lon &lt;= valid_lon_range[1]]\n\n        # Return coordinates\n        return cleaned_coords if cleaned_coords else None\n    \n    except (IndexError, TypeError):\n        return None\n\n# Decode Polyline\nspeed_data['decoded_polyline'] = speed_data['ENCODED_POLY_LINE'].apply(poly)\n\n# Geometry for Geo data frame by processing polylines as linestring\ngeometry = speed_data['decoded_polyline'].apply(lambda x: LineString(x) \n                                                                if x else None)\n\n# Create Geo data frame and convert to ESPG 3857\nspeed_gdf = gpd.GeoDataFrame(speed_data,geometry=geometry,crs='EPSG:4326')\nspeed_gdf = speed_gdf.to_crs(epsg=3857)\n\n# Obtain data for July 2\ngrouped_speed_gdf = speed_gdf.groupby(['date'])\ngrouped_speed_gdf = grouped_speed_gdf.get_group(datetime.date(2024,7,2))\n\n# Group Dataframe by hour\ngrouped_speed_gdf = grouped_speed_gdf.groupby(['hour'])\n\n\n7.3.5.3 Updated Frame Function\nfrom matplotlib.colors import Normalize\n\n\n# Obtain Crash data for July 2 grouped by hour\ncrash_grouped = crash_gdf.groupby(['date'])\ncrash_grouped = crash_grouped.get_group(datetime.date(2024,7,2))\ncrash_grouped = crash_grouped.groupby(['hour'])\n\n# Get keys to generate frames\nkeys = [key for key in crash_grouped.groups.keys()]\n\n# Normalize range to make color bar\nnorm = Normalize(vmin=speed_gdf['normalized_speed'].min(), \n                            vmax=speed_gdf['normalized_speed'].max())\n\nfig, ax = plt.subplots(figsize=(6,7))\n\ndef update(frame):\n    hr = frame\n\n    # Get Crash and Traffic Data\n    df = crash_grouped.get_group((hr))\n    speed_df = grouped_speed_gdf.get_group((hr))\n\n    speed_df = speed_df.drop_duplicates(subset='LINK_ID', \n                                    keep='last').reset_index(drop=True)\n\n    ax.clear()\n\n    df.plot(ax=ax,color='red',edgecolor='k',\n                            label=f'Date: 2 July, 2024\\nHour: {hr}')\n\n    speed_df.plot(ax=ax,column='normalized_speed',cmap='plasma',norm=norm,\n                                    label='Normalized Traffic Speed')\n\n    ax.imshow(ny_img,extent=ny_bbox)\n    ax.legend(loc='lower right')\n    ax.set_title(\"Crash and Traffic Data by Hour - 2 July, 2024\")\n    ax.axis('off')\n    \n\n# Add Colorbar\nsm = cm.ScalarMappable(cmap='plasma', norm=norm)\nfig.colorbar(sm, ax=ax,fraction=0.02, pad=0.03, shrink=0.6,aspect=25) \n\n# Remove Whitespace\nfig.subplots_adjust(left=0.04,right=0.95,top=1,bottom=0)\n\nanim = FuncAnimation(fig,update,frames=keys)\nanim.save(\"crash_traffic_7_1_24.gif\", writer='Pillow', fps=2)\n\nplt.show()\n\n\n\n\n7.3.6 Saving your Animation\n\n7.3.6.1 GIF\nTo save your animation as a GIF:\n\nWriter: Pillow\nCommand: Use anim.save() with the writer='Pillow' option.\n\npip install pillow # pip users\nconda install -c conda-forge pillow # conda users\nExample:\nanim.save('animation.gif', writer='Pillow', fps=30, dpi=200)\n\n\n7.3.6.2 MP4\nTo save your animation as MP4:\n\nWriter: ffmpeg\nCommand: Use anim.save() with the writer='ffmpeg' option.\n\nconda install -c conda-forge ffmpeg # conda users\nPip Users:\n\nDownload from ffmeg.org\nExtract the folder\nAdd the bins folder path to your system variables.\n\nExample:\nanim.save('animation.mp4', writer='ffmpeg', fps=30, dpi=300)\n\n\n\n7.3.7 Conclusion\nHere is what was covered in this section of the book.\n\nAnimating Plots and Maps using FuncAnimation\n\nCreating a figure and axes object with plt.subplots()\nCreating an update function\nGenerating Animmation\nObtaining and adding basemap background for map animations\nProcessing geospatial data\nSaving animations\n\n\nIf you have more questions or would like to try it:\n\nMatplotlib’s FunctionAnimation Documentation\nMatplotlib Documentation\nContextily Documentation\nGeoPandas Documentation\n\n\n\n\n\nCollaboration, E. H. T. (2019). First M87 event horizon telescope results. I. The shadow of the supermassive black hole. The Astrophysical Journal Letters, 875(1), L1. https://doi.org/10.3847/2041-8213/ab0c57\n\n\nHunter, J. D., & Matplotlib Development Team, the. (2023a). Matplotlib: Visualization with python. https://matplotlib.org/stable/index.html\n\n\nHunter, J. D., & Matplotlib Development Team, the. (2023b). Matplotlib.animation.FuncAnimation. https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html\n\n\nKharlamov, M. (2023). Contextily: Adding basemaps to geospatial data. https://contextily.readthedocs.io/en/latest/intro_guide.html\n\n\nTeam, G. D. (2024). GeoPandas documentation. https://geopandas.org/en/stable/index.html\n\n\nWilkinson, L. (2012). The grammar of graphics. Springer.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "8  Statistical Tests and Models",
    "section": "",
    "text": "8.1 Tests for Exploratory Data Analysis\nA collection of functions are available from scipy.stats.\nSince R has a richer collections of statistical functions, we can call R function from Python with rpy2. See, for example, a blog on this subject.\nFor example, fisher_exact can only handle 2x2 contingency tables. For contingency tables larger than 2x2, we can call fisher.test() from R through rpy2. See this StackOverflow post. Note that the . in function names and arguments are replaced with _.\nimport pandas as pd\nimport numpy as np\nimport rpy2.robjects.numpy2ri\nfrom rpy2.robjects.packages import importr\nrpy2.robjects.numpy2ri.activate()\n\nstats = importr('stats')\n\nw0630 = pd.read_feather(\"data/nyccrashes_cleaned.feather\")\nw0630[\"injury\"] = np.where(w0630[\"number_of_persons_injured\"] &gt; 0, 1, 0)\nm = pd.crosstab(w0630[\"injury\"], w0630[\"borough\"])\nprint(m)\n\nres = stats.fisher_test(m.to_numpy(), simulate_p_value = True)\nprint(res)\n\nLoading custom .Rprofileborough  BRONX  BROOKLYN  MANHATTAN  QUEENS  STATEN ISLAND\ninjury                                                    \n0          149       345        164     249             65\n1          129       266        127     227             28\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    2000 replicates)\n\ndata:  structure(c(149L, 129L, 345L, 266L, 164L, 127L, 249L, 227L, 65L, 28L), dim = c(2L, 5L))\np-value = 0.03598\nalternative hypothesis: two.sided",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#tests-for-exploratory-data-analysis",
    "href": "stats.html#tests-for-exploratory-data-analysis",
    "title": "8  Statistical Tests and Models",
    "section": "",
    "text": "Comparing the locations of two samples\n\nttest_ind: t-test for two independent samples\nttest_rel: t-test for paired samples\nranksums: Wilcoxon rank-sum test for two independent samples\nwilcoxon: Wilcoxon signed-rank test for paired samples\n\nComparing the locations of multiple samples\n\nf_oneway: one-way ANOVA\nkruskal: Kruskal-Wallis H-test\n\nTests for associations in contigency tables\n\nchi2_contingency: Chi-square test of independence of variables\nfisher_exact: Fisher exact test on a 2x2 contingency table\n\nGoodness of fit\n\ngoodness_of_fit: distribution could contain unspecified parameters\nanderson: Anderson-Darling test\nkstest: Kolmogorov-Smirnov test\nchisquare: one-way chi-square test\nnormaltest: test for normality",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#statistical-modeling",
    "href": "stats.html#statistical-modeling",
    "title": "8  Statistical Tests and Models",
    "section": "8.2 Statistical Modeling",
    "text": "8.2 Statistical Modeling\nStatistical modeling is a cornerstone of data science, offering tools to understand complex relationships within data and to make predictions. Python, with its rich ecosystem for data analysis, features the statsmodels package— a comprehensive library designed for statistical modeling, tests, and data exploration. statsmodels stands out for its focus on classical statistical models and compatibility with the Python scientific stack (numpy, scipy, pandas).\n\n8.2.1 Installation of statsmodels\nTo start with statistical modeling, ensure statsmodels is installed:\nUsing pip:\npip install statsmodels\n\n\n8.2.2 Linear Model\nLet’s simulate some data for illustrations.\n\nimport numpy as np\n\nnobs = 200\nncov = 5\nnp.random.seed(123)\nx = np.random.random((nobs, ncov)) # Uniform over [0, 1)\nbeta = np.repeat(1, ncov)\ny = 2 + np.dot(x, beta) + np.random.normal(size = nobs)\n\nCheck the shape of y:\n\ny.shape\n\n(200,)\n\n\nCheck the shape of x:\n\nx.shape\n\n(200, 5)\n\n\nThat is, the true linear regression model is \\[\ny = 2 + x_1 + x_2 + x_3 + x_4 + x_5 + \\epsilon.\n\\]\nA regression model for the observed data can be fitted as\n\nimport statsmodels.api as sma\nxmat = sma.add_constant(x)\nmymod = sma.OLS(y, xmat)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.309\n\n\nModel:\nOLS\nAdj. R-squared:\n0.292\n\n\nMethod:\nLeast Squares\nF-statistic:\n17.38\n\n\nDate:\nWed, 23 Oct 2024\nProb (F-statistic):\n3.31e-14\n\n\nTime:\n10:00:48\nLog-Likelihood:\n-272.91\n\n\nNo. Observations:\n200\nAIC:\n557.8\n\n\nDf Residuals:\n194\nBIC:\n577.6\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n1.8754\n0.282\n6.656\n0.000\n1.320\n2.431\n\n\nx1\n1.1703\n0.248\n4.723\n0.000\n0.682\n1.659\n\n\nx2\n0.8988\n0.235\n3.825\n0.000\n0.435\n1.362\n\n\nx3\n0.9784\n0.238\n4.114\n0.000\n0.509\n1.448\n\n\nx4\n1.3418\n0.250\n5.367\n0.000\n0.849\n1.835\n\n\nx5\n0.6027\n0.239\n2.519\n0.013\n0.131\n1.075\n\n\n\n\n\n\n\n\nOmnibus:\n0.810\nDurbin-Watson:\n1.978\n\n\nProb(Omnibus):\n0.667\nJarque-Bera (JB):\n0.903\n\n\nSkew:\n-0.144\nProb(JB):\n0.637\n\n\nKurtosis:\n2.839\nCond. No.\n8.31\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQuestions to review:\n\nHow are the regression coefficients interpreted? Intercept?\nWhy does it make sense to center the covariates?\n\nNow we form a data frame with the variables\n\nimport pandas as pd\ndf = np.concatenate((y.reshape((nobs, 1)), x), axis = 1)\ndf = pd.DataFrame(data = df,\n                  columns = [\"y\"] + [\"x\" + str(i) for i in range(1,\n                  ncov + 1)])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   y       200 non-null    float64\n 1   x1      200 non-null    float64\n 2   x2      200 non-null    float64\n 3   x3      200 non-null    float64\n 4   x4      200 non-null    float64\n 5   x5      200 non-null    float64\ndtypes: float64(6)\nmemory usage: 9.5 KB\n\n\nLet’s use a formula to specify the regression model as in R, and fit a robust linear model (rlm) instead of OLS. Note that the model specification and the function interface is similar to R.\n\nimport statsmodels.formula.api as smf\nmymod = smf.rlm(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df)\nmyfit = mymod.fit()\nmyfit.summary()\n\n\nRobust linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n200\n\n\nModel:\nRLM\nDf Residuals:\n194\n\n\nMethod:\nIRLS\nDf Model:\n5\n\n\nNorm:\nHuberT\n\n\n\n\nScale Est.:\nmad\n\n\n\n\nCov Type:\nH1\n\n\n\n\nDate:\nWed, 23 Oct 2024\n\n\n\n\nTime:\n10:00:48\n\n\n\n\nNo. Iterations:\n16\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n1.8353\n0.294\n6.246\n0.000\n1.259\n2.411\n\n\nx1\n1.1254\n0.258\n4.355\n0.000\n0.619\n1.632\n\n\nx2\n0.9664\n0.245\n3.944\n0.000\n0.486\n1.447\n\n\nx3\n0.9995\n0.248\n4.029\n0.000\n0.513\n1.486\n\n\nx4\n1.3275\n0.261\n5.091\n0.000\n0.816\n1.839\n\n\nx5\n0.6768\n0.250\n2.712\n0.007\n0.188\n1.166\n\n\n\nIf the model instance has been used for another fit with different fit parameters, then the fit options might not be the correct ones anymore .\n\n\nFor model diagnostics, one can check residual plots.\n\nimport matplotlib.pyplot as plt\n\nmyOlsFit = smf.ols(formula = \"y ~ x1 + x2 + x3 + x4 + x5\", data = df).fit()\nfig = plt.figure(figsize = (6, 6))\n## residual versus x1; can do the same for other covariates\nfig = sma.graphics.plot_regress_exog(myOlsFit, 'x1', fig=fig)\n\n\n\n\n\n\n\n\nSee more on residual diagnostics and specification tests.\n\n\n8.2.3 Generalized Linear Regression\nA linear regression model cannot be applied to presence/absence or count data. Generalized Linear Models (GLM) extend the classical linear regression to accommodate such response variables, that follow distributions other than the normal distribution. GLMs consist of three main components:\n\nRandom Component: This specifies the distribution of the response variable \\(Y\\). It is assumed to be from the exponential family of distributions, such as Binomial for binary data and Poisson for count data.\nSystematic Component: This consists of the linear predictor, a linear combination of unknown parameters and explanatory variables. It is denoted as \\(\\eta = X\\beta\\), where \\(X\\) represents the explanatory variables, and \\(\\beta\\) represents the coefficients.\nLink Function: The link function, \\(g\\), provides the relationship between the linear predictor and the mean of the distribution function. For a GLM, the mean of \\(Y\\) is related to the linear predictor through the link function as \\(\\mu = g^{-1}(\\eta)\\).\n\nGLMs adapt to various data types through the selection of appropriate link functions and probability distributions. Here, we outline four special cases of GLM: normal regression, logistic regression, Poisson regression, and gamma regression.\n\nNormal Regression (Linear Regression). In normal regression, the response variable has a normal distribution. The identity link function (\\(g(\\mu) = \\mu\\)) is typically used, making this case equivalent to classical linear regression.\n\nUse Case: Modeling continuous data where residuals are normally distributed.\nLink Function: Identity (\\(g(\\mu) = \\mu\\))\nDistribution: Normal\n\nLogistic Regression. Logistic regression is used for binary response variables. It employs the logit link function to model the probability that an observation falls into one of two categories.\n\nUse Case: Binary outcomes (e.g., success/failure).\nLink Function: Logit (\\(g(\\mu) = \\log\\frac{\\mu}{1-\\mu}\\))\nDistribution: Binomial\n\nPoisson Regression. Poisson regression models count data using the Poisson distribution. It’s ideal for modeling the rate at which events occur.\n\nUse Case: Count data, such as the number of occurrences of an event.\nLink Function: Log (\\(g(\\mu) = \\log(\\mu)\\))\nDistribution: Poisson\n\nGamma Regression. Gamma regression is suited for modeling positive continuous variables, especially when data are skewed and variance increases with the mean.\n\nUse Case: Positive continuous outcomes with non-constant variance.\nLink Function: Inverse (\\(g(\\mu) = \\frac{1}{\\mu}\\))\nDistribution: Gamma\n\n\nEach GLM variant addresses specific types of data and research questions, enabling precise modeling and inference based on the underlying data distribution.\nA logistic regression can be fit with statsmodels.api.glm.\nTo demonstrate the validation of logistic regression models, we first create a simulated dataset with binary outcomes. This setup involves generating logistic probabilities and then drawing binary outcomes based on these probabilities.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a DataFrame with random features named `simdat`\nsimdat = pd.DataFrame(np.random.randn(1000, 5), columns=['x1', 'x2', 'x3', 'x4', 'x5'])\n\n# Calculating the linear combination of inputs plus an intercept\neta = simdat.dot([2, 2, 2, 2, 2]) - 5\n\n# Applying the logistic function to get probabilities using statsmodels' logit link\np = sm.families.links.Logit().inverse(eta)\n\n# Generating binary outcomes based on these probabilities and adding them to `simdat`\nsimdat['yb'] = np.random.binomial(1, p, p.size)\n\n# Display the first few rows of the dataframe\nprint(simdat.head())\n\n         x1        x2        x3        x4        x5  yb\n0  0.496714 -0.138264  0.647689  1.523030 -0.234153   0\n1 -0.234137  1.579213  0.767435 -0.469474  0.542560   0\n2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918   0\n3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304   0\n4  1.465649 -0.225776  0.067528 -1.424748 -0.544383   0\n\n\nFit a logistic regression for y1b with the formula interface.\n\nimport statsmodels.formula.api as smf\n\n# Specify the model formula\nformula = 'yb ~ x1 + x2 + x3 + x4 + x5'\n\n# Fit the logistic regression model using glm and a formula\nfit = smf.glm(formula=formula, data=simdat, family=sm.families.Binomial()).fit()\n\n# Print the summary of the model\nprint(fit.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                     yb   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      994\nModel Family:                Binomial   Df Model:                            5\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -159.17\nDate:                Wed, 23 Oct 2024   Deviance:                       318.34\nTime:                        10:00:49   Pearson chi2:                 1.47e+03\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.4197\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.0186      0.392    -12.796      0.000      -5.787      -4.250\nx1             1.9990      0.211      9.471      0.000       1.585       2.413\nx2             2.1058      0.214      9.853      0.000       1.687       2.525\nx3             1.9421      0.210      9.260      0.000       1.531       2.353\nx4             2.1504      0.232      9.260      0.000       1.695       2.606\nx5             2.0603      0.221      9.313      0.000       1.627       2.494\n==============================================================================",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#validating-the-results-of-logistic-regression",
    "href": "stats.html#validating-the-results-of-logistic-regression",
    "title": "8  Statistical Tests and Models",
    "section": "8.3 Validating the Results of Logistic Regression",
    "text": "8.3 Validating the Results of Logistic Regression\nValidating the performance of logistic regression models is crucial to assess their effectiveness and reliability. This section explores key metrics used to evaluate the performance of logistic regression models, starting with the confusion matrix, then moving on to accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC). Using simulated data, we will demonstrate how to calculate and interpret these metrics using Python.\n\n8.3.1 Confusion Matrix\nThe confusion matrix is a fundamental tool used for calculating several other classification metrics. It is a table used to describe the performance of a classification model on a set of data for which the true values are known. The matrix displays the actual values against the predicted values, providing insight into the number of correct and incorrect predictions.\n\n\n\nActual\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nhttps://en.wikipedia.org/wiki/Confusion_matrix\nFour entries in the confusion matrix:\n\nTrue Positive (TP): The cases in which the model correctly predicted the positive class.\nFalse Positive (FP): The cases in which the model incorrectly predicted the positive class (i.e., the model predicted positive, but the actual class was negative).\nTrue Negative (TN): The cases in which the model correctly predicted the negative class.\nFalse Negative (FN): The cases in which the model incorrectly predicted the negative class (i.e., the model predicted negative, but the actual class was positive).\n\nFour rates from the confusion matrix with actual (row) margins:\n\nTrue positive rate (TPR): TP / (TP + FN). Also known as sensitivity.\nFalse negative rate (FNR): FN / (TP + FN). Also known as miss rate.\nFalse positive rate (FPR): FP / (FP + TN). Also known as false alarm, fall-out.\nTrue negative rate (TNR): TN / (FP + TN). Also known as specificity.\n\nNote that TPR and FPR do not add up to one. Neither do FNR and FPR.\n\nPositive predictive value (PPV): TP / (TP + FP). Also known as precision.\nFalse discovery rate (FDR): FP / (TP + FP).\nFalse omission rate (FOR): FN / (FN + TN).\nNegative predictive value (NPV): TN / (FN + TN).\n\nNote that PPV and NP do not add up to one.\n\n\n8.3.2 Accuracy\nAccuracy measures the overall correctness of the model and is defined as the ratio of correct predictions (both positive and negative) to the total number of cases examined.\n  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n\nImbalanced Classes: Accuracy can be misleading if there is a significant imbalance between the classes. For instance, in a dataset where 95% of the samples are of one class, a model that naively predicts the majority class for all instances will still achieve 95% accuracy, which does not reflect true predictive performance.\nMisleading Interpretations: High overall accuracy might hide the fact that the model is performing poorly on a smaller, yet important, segment of the data.\n\n\n\n8.3.3 Precision\nPrecision (or PPV) measures the accuracy of positive predictions. It quantifies the number of correct positive predictions made.\n  Precision = TP / (TP + FP)\n\nNeglect of False Negatives: Precision focuses solely on the positive class predictions. It does not take into account false negatives (instances where the actual class is positive but predicted as negative). This can be problematic in cases like disease screening where missing a positive case (disease present) could be dangerous.\nNot a Standalone Metric: High precision alone does not indicate good model performance, especially if recall is low. This situation could mean the model is too conservative in predicting positives, thus missing out on a significant number of true positive instances.\n\n\n\n8.3.4 Recall\nRecall (Sensitivity or TPR) measures the ability of a model to find all relevant cases (all actual positives).\n  Recall = TP / (TP + FN)\n\nNeglect of False Positives: Recall does not consider false positives (instances where the actual class is negative but predicted as positive). High recall can be achieved at the expense of precision, leading to a large number of false positives which can be costly or undesirable in certain contexts, such as in spam detection.\nTrade-off with Precision: Often, increasing recall decreases precision. This trade-off needs to be managed carefully, especially in contexts where both false positives and false negatives carry significant costs or risks.\n\n\n\n8.3.5 F-beta Score\nThe F-beta score is a weighted harmonic mean of precision and recall, taking into account a \\(\\beta\\) parameter such that recall is considered \\(\\beta\\) times as important as precision: \\[\n(1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}\n{\\beta^2 \\text{precision} + \\text{recall}}.\n\\]\nSee stackexchange post for the motivation of \\(\\beta^2\\) instead of just \\(\\beta\\).\nThe F-beta score reaches its best value at 1 (perfect precision and recall) and worst at 0.\nIf reducing false negatives is more important (as might be the case in medical diagnostics where missing a positive diagnosis could be critical), you might choose a beta value greater than 1. If reducing false positives is more important (as in spam detection, where incorrectly classifying an email as spam could be inconvenient), a beta value less than 1 might be appropriate.\nThe F1 Score is a specific case of the F-beta score where beta is 1, giving equal weight to precision and recall. It is the harmonic mean of Precision and Recall and is a useful measure when you seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n\n\n8.3.6 Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic (ROC) curve is a plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It shows the trade-off between the TPR and FPR. The ROC plots TPR against FPR as the decision threshold is varied. It can be particularly useful in evaluating the performance of classifiers when the class distribution is imbalanced,\n\nIncreasing from \\((0, 0)\\) to \\((1, 1)\\).\nBest classification passes \\((0, 1)\\).\nClassification by random guess gives the 45-degree line.\nArea between the ROC and the 45-degree line is the Gini coefficient, a measure of inequality.\nArea under the curve (AUC) of ROC thus provides an important metric of classification results.\n\nThe Area Under the ROC Curve (AUC) is a scalar value that summarizes the performance of a classifier. It measures the total area underneath the ROC curve, providing a single metric to compare models. The value of AUC ranges from 0 to 1:\n\nAUC = 1: A perfect classifier, which perfectly separates positive and negative classes.\nAUC = 0.5: A classifier that performs no better than random chance.\nAUC &lt; 0.5: A classifier performing worse than random.\n\nThe AUC value provides insight into the model’s ability to discriminate between positive and negative classes across all possible threshold values.\n\n\n8.3.7 Demonstration\nLet’s apply these metrics to the simdat dataset to understand their practical implications. We will fit a logistic regression model, make predictions, and then compute accuracy, precision, and recall.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\ny_scores = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[104  11]\n [ 26 109]]\nAccuracy: 0.85\nPrecision: 0.91\nRecall: 0.81\n\n\nBy varying threshold, one can plot the whole ROC curve.\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(y_test, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Print AUC\nprint(f\"AUC: {roc_auc:.2f}\")\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line (random classifier)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nAUC: 0.92\n\n\n\n\n\n\n\n\n\nWe could pick the best threshold that optmizes F1-score/\n\n# Compute F1 score for each threshold\nf1_scores = []\nfor thresh in thresholds:\n    y_pred_thresh = (y_scores &gt;= thresh).astype(int)  # Apply threshold to get binary predictions\n    f1 = f1_score(y_test, y_pred_thresh)\n    f1_scores.append(f1)\n\n# Find the best threshold (the one that maximizes F1 score)\nbest_thresh = thresholds[np.argmax(f1_scores)]\nbest_f1 = max(f1_scores)\n\n# Print the best threshold and corresponding F1 score\nprint(f\"Best threshold: {best_thresh:.4f}\")\nprint(f\"Best F1 score: {best_f1:.2f}\")\n\nBest threshold: 0.3960\nBest F1 score: 0.89",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "stats.html#lasso-logistic-models",
    "href": "stats.html#lasso-logistic-models",
    "title": "8  Statistical Tests and Models",
    "section": "8.4 LASSO Logistic Models",
    "text": "8.4 LASSO Logistic Models\nThe Least Absolute Shrinkage and Selection Operator (LASSO) (Tibshirani, 1996), is a regression method that performs both variable selection and regularization. LASSO imposes an L1 penalty on the regression coefficients, which has the effect of shrinking some coefficients exactly to zero. This results in simpler, more interpretable models, especially in situations where the number of predictors exceeds the number of observations.\n\n8.4.1 Theoretical Formulation of the Problem\nThe objective function for LASSO logistic regression can be expressed as,\n\\[\n\\min_{\\beta}\n\\left\\{ -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right] + \\lambda \\sum_{j=1}^p |\\beta_j| \\right\\}\n\\]\nwhere:\n\n\\(\\hat{p}_i = \\frac{1}{1 + e^{-X_i\\beta}}\\) is the predicted probability for the \\(i\\)-th sample.\n\\(y_i\\) represents the actual class label (binary: 0 or 1).\n\\(X_i\\) is the feature vector for the \\(i\\)-th observation.\n\\(\\beta\\) is the vector of model coefficients (including the intercept).\n\\(\\lambda\\) is the regularization parameter that controls the trade-off between model fit and sparsity (higher \\(\\lambda\\)) encourages sparsity by shrinking more coefficients to zero).\n\nThe lasso penalty encourages the sum of the absolute values of the coefficients to be small, effectively shrinking some coefficients to zero. This results in sparser solutions, simplifying the model and reducing variance without substantial increase in bias.\nPractical benefits of LASSO:\n\nDimensionality Reduction: LASSO is particularly useful when the number of features \\(p\\) is large, potentially even larger than the number of observations \\(n\\), as it automatically reduces the number of features.\nPreventing Overfitting: The L1 penalty helps prevent overfitting by constraining the model, especially when \\(p\\) is large or there is multicollinearity among features.\nInterpretability: By selecting only the most important features, LASSO makes the resulting model more interpretable, which is valuable in fields like bioinformatics, economics, and social sciences.\n\n\n\n8.4.2 Solution Path\nTo illustrate the effect of the lasso penalty in logistic regression, we can plot the solution path of the coefficients as a function of the regularization parameter \\(\\lambda\\). This demonstration will use a simulated dataset to show how increasing \\(\\lambda\\) leads to more coefficients being set to zero.\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate a classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               random_state=42)\n\n# Step 2: Get a lambda grid given length of lambda and min_ratio of lambda_max\ndef get_lambda_l1(xs: np.ndarray, y: np.ndarray, nlambda: int, min_ratio: float):\n    ybar = np.mean(y)\n    xbar = np.mean(xs, axis=0)\n    xs_centered = xs - xbar\n    xty = np.dot(xs_centered.T, (y - ybar))\n    lmax = np.max(np.abs(xty))\n    lambdas = np.logspace(np.log10(lmax), np.log10(min_ratio * lmax),\n                              num=nlambda)\n    return lambdas\n\n# Step 3: Calculate lambda values\nnlambda = 100\nmin_ratio = 0.01\nlambda_values = get_lambda_l1(X, y, nlambda, min_ratio)\n\n# Step 4: Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 5: Initialize arrays to store the coefficients for each lambda value\ncoefficients = []\n\n# Step 6: Fit logistic regression with L1 regularization (Lasso) for each lambda value\nfor lam in lambda_values:\n    model = LogisticRegression(penalty='l1', solver='liblinear', C=1/lam, max_iter=1000)\n    model.fit(X_scaled, y)\n    coefficients.append(model.coef_.flatten())\n\n# Convert coefficients list to a NumPy array for plotting\ncoefficients = np.array(coefficients)\n\n# Step 7: Plot the solution path for each feature\nplt.figure(figsize=(10, 6))\nfor i in range(coefficients.shape[1]):\n    plt.plot(lambda_values, coefficients[:, i], label=f'Feature {i + 1}')\n    \nplt.xscale('log')\nplt.xlabel('Lambda values (log scale)')\nplt.ylabel('Coefficient value')\nplt.title('Solution Path of Logistic Lasso Regression')\nplt.grid(True)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.4.3 Selection the Tuning Parameter\nIn logistic regression with LASSO regularization, selecting the optimal value of the regularization parameter \\(C\\) (the inverse of \\(\\lambda\\)) is crucial to balancing the model’s bias and variance. A small \\(C\\) value (large \\(\\lambda\\)) increases the regularization effect, shrinking more coefficients to zero and simplifying the model. Conversely, a large \\(C\\) (small \\(\\lambda\\)) allows the model to fit the data more closely.\nThe best way to select the optimal \\(C\\) is through cross-validation. In cross-validation, the dataset is split into several folds, and the model is trained on some folds while evaluated on the remaining fold. This process is repeated for each fold, and the results are averaged to ensure the model generalizes well to unseen data. The \\(C\\) value that results in the best performance is selected.\nThe performance metric used in cross-validation can vary based on the task. Common metrics include:\n\nLog-loss: Measures how well the predicted probabilities match the actual outcomes.\nAccuracy: Measures the proportion of correctly classified instances.\nF1-Score: Balances precision and recall, especially useful for imbalanced classes.\nAUC-ROC: Evaluates how well the model discriminates between the positive and negative classes.\n\nIn Python, the LogisticRegressionCV class from scikit-learn automates cross-validation for logistic regression. It evaluates the model’s performance for a range of \\(C\\) values and selects the best one.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score\n\n# Generate synthetic data\nX, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Initialize LogisticRegressionCV with L1 penalty for Lasso and cross-validation\nlog_reg_cv = LogisticRegressionCV(\n    Cs=np.logspace(-4, 4, 20),  # Range of C values (inverse of lambda)\n    cv=5,                       # 5-fold cross-validation\n    penalty='l1',               # Lasso regularization (L1 penalty)\n    solver='liblinear',         # Solver for L1 regularization\n    scoring='accuracy',         # Optimize for accuracy\n    max_iter=10000              # Ensure convergence\n)\n\n# Train the model with cross-validation\nlog_reg_cv.fit(X_train, y_train)\n\n# Best C value (inverse of lambda)\nprint(f\"Best C value: {log_reg_cv.C_[0]}\")\n\n# Evaluate the model on the test set\ny_pred = log_reg_cv.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Display the coefficients of the best model\nprint(\"Model Coefficients:\\n\", log_reg_cv.coef_)\n\nBest C value: 0.08858667904100823\nTest Accuracy: 0.86\nModel Coefficients:\n [[ 0.          0.          0.05552448  0.          0.          1.90889734\n   0.          0.          0.          0.          0.0096863   0.23541942\n   0.          0.         -0.0268928   0.          0.          0.\n   0.          0.        ]]\n\n\n\n\n8.4.4 Preparing for Logistic Regression Fitting\nThe LogisticRegression() function in scikit.learn takes the design matrix of the regression as input, which needs to be prepared with care from the covariates or features that we have.\n\n8.4.4.1 Continuous Variables\nFor continuous variables, it is often desirable to standardized them so that they have mean zero and standard deviation one. There are multiple advantages of doing so. It improves numerical stability in algorithms like logistic regression that rely on gradient descent, ensuring faster convergence and preventing features with large scales from dominating the optimization process. Standardization also enhances the interpretability of model coefficients by allowing for direct comparison of the effects of different features, as coefficients then represent the change in outcome for a one standard deviation increase in each variable. Additionally, it ensures that regularization techniques like Lasso and Ridge treat all features equally, allowing the model to select the most relevant ones without being biased by feature magnitude.\nMoreover, standardization is essential for distance-based models such as k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs), where differences in feature scale can distort the calculations. It also prevents models from being sensitive to arbitrary changes in the units of measurement, improving robustness and consistency. Finally, standardization facilitates better visualizations and diagnostics by putting all variables on a comparable scale, making patterns and residuals easier to interpret. Overall, it is a simple yet powerful preprocessing step that leads to better model performance and interpretability.\nWe have already seen this with StandardScaler.\n\n\n8.4.4.2 Categorical Variables\nCategorical variables can be classified into two types: nominal and ordinal. Nominal variables represent categories with no inherent order or ranking between them. Examples include variables like “gender” (male, female) or “color” (red, blue, green), where the categories are simply labels and one category does not carry more significance than another. Ordinal variables, on the other hand, represent categories with a meaningful order or ranking. For example, education levels such as “high school,” “bachelor,” “master,” and “PhD” have a clear hierarchy, where each level is ranked higher than the previous one. However, the differences between the ranks are not necessarily uniform or quantifiable, making ordinal variables distinct from numerical variables. Understanding the distinction between nominal and ordinal variables is important when deciding how to encode and interpret them in statistical models.\nCategorical variables needs to be coded into numrical values before further processing. In Python, nominal and ordinal variables are typically encoded differently to account for their unique properties. Nominal variables, which have no inherent order, are often encoded using One-Hot Encoding, where each category is transformed into a binary column (0 or 1). For example, the OneHotEncoder from scikit-learn can be used to convert a “color” variable with categories like “red,” “blue,” and “green” into separate columns color_red, color_blue, and color_green, with only one column being 1 for each observation. On the other hand, ordinal variables, which have a meaningful order, are best encoded using Ordinal Encoding. This method assigns an integer to each category based on their rank. For example, an “education” variable with categories “high school,” “bachelor,” “master,” and “PhD” can be encoded as 0, 1, 2, and 3, respectively. The OrdinalEncoder from scikit-learn can be used to implement this encoding, which ensures that the model respects the order of the categories during analysis.\n\n\n8.4.4.3 An Example\nHere is a demo with pipeline using a simulated dataset.\nFirst we generate data with sample size 1000 from a logistic model with both categorical and numerical covariates.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport numpy as np\nfrom scipy.special import expit  # Sigmoid function\n\n# Generate a dataset with the specified size\ndataset_size = 1000\nnp.random.seed(20241014)\n\n# Simulate categorical and numerical features\ngender = np.random.choice(\n    ['male', 'female'], size=dataset_size)  # Nominal variable\neducation = np.random.choice(\n    ['high_school', 'bachelor', 'master', 'phd'], size=dataset_size)  # Ordinal variable\nage = np.random.randint(18, 65, size=dataset_size)\nincome = np.random.randint(30000, 120000, size=dataset_size)\n\n# Create a logistic relationship between the features and the outcome\ngender_num = np.where(gender == 'male', 0, 1)\n\n# Define the linear predictor with regression coefficients\nlinear_combination = (\n    0.3 * gender_num - 0.02 * age + 0.00002 * income\n)\n\n# Apply sigmoid function to get probabilities\nprobabilities = expit(linear_combination)\n\n# Generate binary outcome based on the probabilities\noutcome = np.random.binomial(1, probabilities)\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'gender': gender,\n    'education': education,\n    'age': age,\n    'income': income,\n    'outcome': outcome\n})\n\nNext we split the data into features and target and define transformers for each types of feature columns.\n\n# Split the dataset into features (X) and target (y)\nX = data[['gender', 'education', 'age', 'income']]\ny = data['outcome']\n\n# Define categorical and numerical columns\ncategorical_cols = ['gender', 'education']  \nnumerical_cols = ['age', 'income']\n\n# Define transformations for categorical variable\ncategorical_transformer = OneHotEncoder(\n    categories=[['male', 'female'], ['high_school', 'bachelor', 'master', 'phd']],\n    drop='first')\n\n# Define transformations for continuous variables\nnumerical_transformer = StandardScaler()\n\n# Use ColumnTransformer to transform the columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_cols),\n        ('num', numerical_transformer, numerical_cols)\n    ]\n)\n\nDefine a pipeline, which preprocess the data and then fits a logistic model.\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(penalty='l1', solver='liblinear',\n    max_iter=1000))\n])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=2024)\n\n# Fit the pipeline to the training data\npipeline.fit(X_train, y_train)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(categories=[['male',\n                                                                             'female'],\n                                                                            ['high_school',\n                                                                             'bachelor',\n                                                                             'master',\n                                                                             'phd']],\n                                                                drop='first'),\n                                                  ['gender', 'education']),\n                                                 ('num', StandardScaler(),\n                                                  ['age', 'income'])])),\n                ('classifier',\n                 LogisticRegression(max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(categories=[['male', 'female'],\n                                                           ['high_school',\n                                                            'bachelor',\n                                                            'master', 'phd']],\n                                               drop='first'),\n                                 ['gender', 'education']),\n                                ('num', StandardScaler(), ['age', 'income'])]) cat['gender', 'education']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(categories=[['male', 'female'],\n                          ['high_school', 'bachelor', 'master', 'phd']],\n              drop='first') num['age', 'income']  StandardScaler?Documentation for StandardScalerStandardScaler()  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000, penalty='l1', solver='liblinear') \n\n\nCheck the coefficients of the fitted logistic regression model.\n\nmodel = pipeline.named_steps['classifier']\nintercept = model.intercept_\ncoefficients = model.coef_\n\n# Check the preprocessor's encoding\nencoded_columns = pipeline.named_steps['preprocessor']\\\n.transformers_[0][1].get_feature_names_out(categorical_cols)\n\n# Show intercept, coefficients, and encoded feature names\nintercept, coefficients, list(encoded_columns)\n\n(array([0.66748582]),\n array([[ 0.30568894,  0.10069842,  0.12087311,  0.22576774, -0.24749201,\n          0.55828424]]),\n ['gender_female', 'education_bachelor', 'education_master', 'education_phd'])\n\n\nNote that the encoded columns has one for gender and three for education, with male and high_school as reference levels, respectively. The reference level was determined when calling oneHotEncoder() with drop = 'first'. If categories were not specified, the first level in alphabetical order would be dropped. With the default drop = 'none', the estimated coefficients will have two columns that are not estimable and were set to zero. Obviously, if no level were dropped in forming the model matrix, the columns of the one hot encoding for each categorical variable would be perfectly linearly dependent because they would sum to one.\nThe regression coefficients returned by the logistic regression model in this case should be interpreted on the standardized scale of the numerical covariates (e.g., age and income). This is because we applied standardization to the numerical features using StandardScaler in the pipeline before fitting the model. For example, the coefficient for age would reflect the change in the log-odds of the outcome for a 1 standard deviation increase in age, rather than a 1-unit increase in years. The coefficients for the one-hot encoded categorical variables (gender and education) are on the original scale because one-hot encoding does not change the scale of the variables. For instance, the coefficient for gender_female tells us how much the log-odds of the outcome changes when the observation is male versus the reference category (male).\n\n\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Tests and Models</span>"
    ]
  },
  {
    "objectID": "machinelearning.html",
    "href": "machinelearning.html",
    "title": "9  Machine Learning: Overview",
    "section": "",
    "text": "9.1 Introduction\nMachine Learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance over time without being explicitly programmed. At its core, machine learning algorithms aim to identify patterns in data and use those patterns to make decisions or predictions.\nMachine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type differs in the data it uses and the learning tasks it performs, addressing addresses different tasks and problems. Supervised learning aims to predict outcomes based on labeled data, unsupervised learning focuses on discovering hidden patterns within the data, and reinforcement learning centers around learning optimal actions through interaction with an environment.\nLet’s define some notations to introduce them:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#introduction",
    "href": "machinelearning.html#introduction",
    "title": "9  Machine Learning: Overview",
    "section": "",
    "text": "\\(X\\): A set of feature vectors representing the input data. Each element \\(X_i\\) corresponds to a set of features or attributes that describe an instance of data.\n\\(Y\\): A set of labels or rewards associated with outcomes. In supervised learning, \\(Y\\) is used to evaluate the correctness of the model’s predictions. In reinforcement learning, \\(Y\\) represents the rewards that guide the learning process.\n\\(A\\): A set of possible actions in a given context. In reinforcement learning, actions \\(A\\) represent choices that can be made in response to a given situation, with the goal of maximizing a reward.\n\n\n9.1.1 Supervised Learning\nSupervised learning is the most widely used type of machine learning. In supervised learning, we have both feature vectors \\(X\\) and their corresponding labels \\(Y\\). The objective is to train a model that can predict \\(Y\\) based on \\(X\\). This model is trained on labeled examples, where the correct outcome is known, and it adjusts its internal parameters to minimize the error in its predictions, which occurs as part of the cross-validation process.\nKey tasks in supervised learning include:\n\nClassification: Assigning data points to predefined categories or classes.\nRegression: Predicting a continuous value based on input data.\n\nIn supervised learning, the data consists of both feature vectors \\(X\\) and labels \\(Y\\), namely, \\((X, Y)\\).\n\n\n9.1.2 Unsupervised Learning\nUnsupervised learning involves learning patterns from data without any associated labels or outcomes. The objective is to explore and identify hidden structures in the feature vectors \\(X\\). Since there are no ground-truth labels \\(Y\\) to guide the learning process, the algorithm must discover patterns on its own. This is particularly useful when subject matter experts are unsure of common properties within a data set.\nCommon tasks in unsupervised learning include:\n\nClustering: Grouping similar data points together based on certain features.\nDimension Reduction: Simplifying the input data by reducing the number of features while preserving essential patterns.\n\nIn unsupervised learning, the data consists solely of feature vectors \\(X\\).\n\n\n9.1.3 Reinforcement Learning\nReinforcement learning involves learning how to make a sequence of decisions to maximize a cumulative reward. Unlike supervised learning, where the model learns from a static dataset of labeled examples, reinforcement learning involves an agent that interacts with an environment by taking actions \\(A\\), receiving feedback in the form of rewards \\(Y\\), and learning over time which actions lead to the highest cumulative reward.\nThe process in reinforcement learning involves:\n\nStates: The context or environment the agent is in, represented by feature vectors \\(X\\).\nActions: The set of possible choices the agent can make in response to the current state, denoted as \\(A\\).\nRewards: Feedback the agent receives after taking an action, which guides the learning process.\n\nIn reinforcement learning, the data consists of feature vectors \\(X\\), actions \\(A\\), and rewards \\(Y\\), namely, \\((X, A, Y)\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#bias-variance-tradeoff",
    "href": "machinelearning.html#bias-variance-tradeoff",
    "title": "9  Machine Learning: Overview",
    "section": "9.2 Bias-Variance Tradeoff",
    "text": "9.2 Bias-Variance Tradeoff\nThe variance-bias trade-off is a core concept in machine learning that explains the relationship between the complexity of a model, its performance on training data, and its ability to generalize to unseen data. It applies to both supervised and unsupervised learning, though it manifests differently in each.\n\n9.2.1 Bias\nBias refers to the error introduced by approximating a complex real-world problem with a simplified model. A model with high bias makes strong assumptions about the data, leading to oversimplified patterns and poor performance on both the training data and new data. High bias results in underfitting, where the model fails to capture important trends in the data.\n\nExample (Supervised): In supervised learning, using a linear regression model to fit data that has a nonlinear relationship results in high bias because the model cannot capture the complexity of the data.\nExample (Unsupervised): In clustering (an unsupervised task), setting the number of clusters too low (e.g., forcing data into two clusters when more exist) leads to high bias, as the model oversimplifies the underlying structure.\n\n\n\n9.2.2 Variance\nVariance refers to the model’s sensitivity to small changes in the training data. A model with high variance will adapt closely to the training data, potentially capturing noise or fluctuations that are not representative of the general data distribution. High variance leads to overfitting, where the model performs well on training data but poorly on new, unseen data.\n\nExample (Supervised): A decision tree with many branches can exhibit high variance. The model perfectly fits the training data but may perform poorly on test data because it overfits to specific idiosyncrasies in the training set.\nExample (Unsupervised): In clustering, setting the number of clusters too high or fitting overly flexible cluster shapes (e.g., in Gaussian Mixture Models) can lead to overfitting, where the model captures noise and splits data unnecessarily.\n\n\n\n9.2.3 The Trade-Off\nThe bias-variance trade-off reflects the tension between bias and variance. As model complexity increases:\n\nBias decreases: The model becomes more flexible and can capture more details of the data.\nVariance increases: The model becomes more sensitive to the particular training data, potentially capturing noise.\n\nConversely, a simpler model will:\n\nHave high bias: It may not capture all relevant patterns in the data.\nHave low variance: It will be less sensitive to fluctuations in the data and is more likely to generalize well to unseen data.\n\n\n\n9.2.4 Bias-Variance in Supervised Learning\nIn supervised learning, the goal is to strike the right balance between bias and variance to minimize prediction error. This balance is critical for developing models that generalize well to new data. The total error of a model can be decomposed into:\n\\[\n\\text{Total Error} = \\text{Bias}^2 + \\text{Variance}\n+\\text{Irreducible Error}.\n\\]\n\nBias: The error from using a model that is too simple.\nVariance: The error from using a model that is too complex and overfits the training data.\nIrreducible Error: This is the noise inherent in the data itself, which cannot be eliminated no matter how well the model is tuned.\n\n\n\n9.2.5 Bias-Variance in Unsupervised Learning\nIn unsupervised learning, the bias-variance trade-off is less formally defined but still relevant. For example:\n\nIn clustering, choosing the wrong number of clusters can lead to either high bias (too few clusters, oversimplifying the data) or high variance (too many clusters, overfitting the data).\nIn dimensionality reduction, keeping too few components in Principal Component Analysis (PCA) increases bias by losing important information, while keeping too many components retains noise, increasing variance.\n\nIn unsupervised learning, balancing bias and variance typically involves tuning hyperparameters (e.g., number of clusters, number of components) to find the right complexity level.\n\n\n9.2.6 Striking the Right Balance\nTo strike the balance between bias and variance in both supervised and unsupervised learning, techniques such as regularization, early stopping, cross-validation, and hyperparameter tuning are essential. These techniques help ensure the model is complex enough to capture patterns in the data but not so complex that it overfits to noise or irrelevant details.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "machinelearning.html#crossvalidation",
    "href": "machinelearning.html#crossvalidation",
    "title": "9  Machine Learning: Overview",
    "section": "9.3 Crossvalidation",
    "text": "9.3 Crossvalidation\nCross-validation is a technique used to evaluate machine learning models and tune hyperparameters by splitting the dataset into multiple subsets. This approach helps to avoid overfitting and provides a better estimate of the model’s performance on unseen data. Cross-validation is especially useful for managing the bias-variance trade-off by allowing you to test how well the model generalizes without relying on a single train-test split.\n\n9.3.1 K-Fold Cross-Validation\nThe most commonly used method is \\(k\\)-fold cross-validation:\n\nSplit the data: The dataset is divided into \\(k\\) equally-sized folds (subsets).\nTrain on \\(k - 1\\) folds: The model is trained on \\(k - 1\\) folds, leaving one fold as a test set.\nTest on the remaining fold: The model’s performance is evaluated on the fold that was left out.\nRepeat: This process is repeated \\(k\\) times, with each fold used once as the test set.\nAverage performance: The final cross-validation score is the average performance across all \\(k\\) iterations.\n\nBy averaging the results across multiple test sets, cross-validation provides a more robust estimate of model performance and helps avoid overfitting or underfitting to any particular training-test split.\nLeave-One-Out Cross-Validation (LOOCV) takes each observation as one fold. The dataset is split into \\(n\\) subsets (where \\(n\\) is the sample size), with each sample acting as a test set once. While this method provides the most exhaustive evaluation, it can be computationally expensive for large datasets.\n\n\n9.3.2 Benefits of Cross-Validation\n\nPrevents overfitting: By testing the model on multiple subsets of data, cross-validation helps to identify if the model is too complex and overfits to the training data.\nPrevents underfitting: If the model performs poorly across all folds, it may indicate that the model is too simple (high bias).\nBetter estimation: Cross-validation gives a better estimate of how the model will perform on unseen data compared to a single train-test split.\n\n\n\n9.3.3 Cross-Validation in Unsupervised Learning\nWhile cross-validation is most commonly used in supervised learning, it can also be applied to unsupervised learning through:\n\nStability testing: Running unsupervised algorithms (e.g., clustering) on different data splits and measuring the stability of the results (e.g., using the silhouette score).\nInternal validation metrics: In clustering, internal metrics like the silhouette score or Davies-Bouldin index can be used to evaluate the quality of clustering across different data splits.\n\nThe bias-variance trade-off is a universal problem in machine learning, affecting both supervised and unsupervised models. Cross-validation is a powerful tool for controlling this trade-off by providing a reliable estimate of model performance and helping to fine-tune model complexity. By balancing bias and variance through careful model selection, regularization, and cross-validation, you can develop models that generalize well to unseen data without overfitting or underfitting.\n\n\n9.3.4 A Curve-Fitting with Splines: An Example\nOverfitting occurs when a model becomes overly complex and starts to capture not just the underlying patterns in the data but also the noise or random fluctuations. This can lead to poor generalization to new, unseen data. A clear sign of overfitting is when a model performs very well on the training data but performs poorly on test data, as it fails to generalize beyond the data it was trained on.\nIn this example, we illustrate overfitting using cubic spline regression with different numbers of knots. Splines are a flexible tool that allow for piecewise polynomial regression, with knots defining where the pieces of the polynomial meet. The more knots we use, the more flexible the model becomes, which can potentially lead to overfitting.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\n\ndef true_function(X):\n    return np.sin(1.5 * X) + 0.5 * np.cos(0.5 * X) + np.sin(2 * X)\n\n# Generate synthetic data using the more complex true function\nX = np.sort(np.random.rand(200) * 10).reshape(-1, 1)\ny = true_function(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Function to explore overfitting by plotting both errors and fitted curves\ndef overfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list):\n    train_errors = []\n    test_errors = []\n    \n    # Generate fine grid for plotting the true curve and fitted models\n    X_line = np.linspace(0, 10, 1000).reshape(-1, 1)\n    y_true = true_function(X_line)\n    \n    # Plot the true function and observed data\n    plt.figure(figsize=(10, 6))\n    plt.scatter(X_train, y_train, color='blue', label='Training data', alpha=0.6)\n    plt.plot(X_line, y_true, label='True function', color='black', linestyle='--')\n    \n    for n_knots in knots_list:\n        # Create a spline model with fixed degree = 3 (cubic) and varying knots\n        spline = SplineTransformer(degree=3, n_knots=n_knots, include_bias=False)\n        model = make_pipeline(spline, LinearRegression())\n        \n        # Fit the model to training data\n        model.fit(X_train, y_train)\n        \n        # Predict on training and test data\n        y_pred_train = model.predict(X_train)\n        y_pred_test = model.predict(X_test)\n        y_pred = model.predict(X_line)\n        \n        # Calculate training and test errors (mean squared error)\n        train_errors.append(mean_squared_error(y_train, y_pred_train))\n        test_errors.append(mean_squared_error(y_test, y_pred_test))\n        \n        # Plot the fitted curve\n        plt.plot(X_line, y_pred, label=f'{n_knots} Knots (Fit)', alpha=0.7)\n\n    print(train_errors, test_errors)\n    \n    plt.title('Overfitting Example: Fitted Curves and Observed Data')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n    \n    # Plot training and test error separately\n    plt.figure(figsize=(10, 6))\n    plt.plot(knots_list, train_errors, label='Training Error', marker='o', color='blue')\n    plt.plot(knots_list, test_errors, label='Test Error', marker='o', color='red')\n    plt.title('Training vs. Test Error with Varying Knots')\n    plt.xlabel('Number of Knots')\n    plt.ylabel('Mean Squared Error')\n    plt.legend()\n    plt.show()\n\n# Explore overfitting by varying the number of knots and overlaying the fitted curves\nknots_list = [6, 8, 10, 12, 14, 16]\noverfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list)\n\n[np.float64(0.11925926334166218), np.float64(0.06706771870332887), np.float64(0.05149697267167946), np.float64(0.04800466398334981), np.float64(0.04807802162240843), np.float64(0.04753756150990699)] [np.float64(0.14245034022524056), np.float64(0.059019274114925316), np.float64(0.05102020213479817), np.float64(0.04772816344225628), np.float64(0.04671027698957559), np.float64(0.04594169668227475)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the first plot, we fit cubic splines with 2, 4, 6, 8, 10, and 12 knots to the data. As the number of knots increases, the model becomes more flexible and better able to fit the training data. With only 2 knots, the model is quite smooth and underfits the data, capturing only broad trends but missing the detailed structure of the true underlying function. With 4 or 6 knots, the model begins to capture more of the data’s structure, balancing the bias-variance trade-off effectively. However, as we increase the number of knots to 10 and 12, the model becomes too flexible. It starts to fit the noise in the training data, producing a curve that adheres too closely to the data points. This is a classic case of overfitting: the model fits the training data very well, but it no longer generalizes to new data.\nIn the second plot, we observe the training error and test error as the number of knots increases. As expected, the training error consistently decreases as the number of knots increases, since a more complex model can fit the training data better. However, the test error tells a different story. Initially, the test error decreases as the model becomes more flexible, indicating that the model is learning meaningful patterns from the data. But after a certain point, the test error begins to increase, signaling overfitting.\nThis is a key insight into the bias-variance trade-off. While adding more complexity (in this case, more knots) reduces bias and improves fit on the training data, it also increases variance, making the model more sensitive to fluctuations and noise in the data. This results in worse performance on test data, as the model becomes too specialized to the training set.\nThe example clearly demonstrates how overfitting can occur when the model becomes too complex. In practice, it’s important to find a balance between underfitting (high bias) and overfitting (high variance). Techniques such as cross-validation, regularization, or limiting model complexity (e.g., setting a reasonable number of knots in spline regression) can help manage this trade-off and produce models that generalize well to unseen data.\nBy tuning the number of knots or other model parameters, we can achieve a model that strikes the right balance, capturing the true patterns in the data without fitting the noise.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine Learning: Overview</span>"
    ]
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1 Decision Trees: Foundation\nDecision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed (Breiman et al., 1984). Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees-foundation",
    "href": "supervised.html#decision-trees-foundation",
    "title": "10  Supervised Learning",
    "section": "",
    "text": "10.1.1 Algorithm Formulation\nThe core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node \\(m\\), the data subset is denoted as \\(Q_m\\) with a sample size of \\(n_m\\). The objective is to find a candidate split \\(\\theta\\), defined as a threshold for a given feature, that minimizes an impurity or loss measure \\(H\\).\nWhen a split is made at node \\(m\\), the data is divided into two subsets: \\(Q_{m,l}\\) (left node) with sample size \\(n_{m,l}\\), and \\(Q_{m,r}\\) (right node) with sample size \\(n_{m,r}\\). The split quality, measured by \\(G(Q_m, \\theta)\\), is given by:\n\\[\nG(Q_m, \\theta) = \\frac{n_{m,l}}{n_m} H(Q_{m,l}(\\theta)) +\n\\frac{n_{m,r}}{n_m} H(Q_{m,r}(\\theta)).\n\\]\nThe algorithm aims to identify the split that minimizes the impurity:\n\\[\n\\theta^* = \\arg\\min_{\\theta} G(Q_m, \\theta).\n\\]\nThis process is applied recursively at each child node until a stopping condition is met.\n\nStopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.\nPruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves the generalization accuracy of the model.\n\n\n\n10.1.2 Search Space for Possible Splits\nAt each node in the decision tree, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the values of each feature. For a given feature, the algorithm considers each unique value in the current node’s subset as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring the data is partitioned effectively.\nFormally, let the feature set be \\(\\{X_1, X_2, \\ldots, X_p\\}\\), where \\(p\\) is the total number of features, and let the unique values of feature $ X_j $ at node $ m $ be denoted by ${v_{j,1}, v_{j,2}, , v_{j,k_j}} \\(. The search space at node\\)m$ includes:\n\nFeature candidates: \\(\\{X_1, X_2, \\ldots, X_p\\}\\).\nThreshold candidates for \\(X_j\\): \\[\n\\left\\{ \\frac{v_{j,i} + v_{j,i+1}}{2} \\mid 1 \\leq i &lt; k_j \\right\\}.\n\\]\n\nThe search space therefore encompasses all combinations of features and their respective thresholds. While the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.\n\n\n10.1.3 Metrics\n\n10.1.3.1 Classification\nIn decision tree classification, several criteria can be used to measure the quality of a split at each node. These criteria are based on how “pure” the resulting nodes are after the split. A pure node contains samples that predominantly belong to a single class. The goal is to minimize impurity, leading to nodes that are as homogeneous as possible.\n\nGini Index: The Gini index measures the impurity of a node by calculating the probability of randomly choosing two different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node \\(m\\), the Gini index is \\[\nH(Q_m) = \\sum_{k=1}^{K} p_{mk} (1 - p_{mk}),\n\\] where \\(p_{mk}\\) is the proportion of samples of class \\(k\\) at node \\(m\\); and\\(K\\) is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including sklearn.\nEntropy (Information Gain): Entropy is another measure of impurity, derived from information theory. It quantifies the “disorder” of the data at a node. Lower entropy means higher purity. At node \\(m\\), it is defined as \\[\nH(Q_m) = - \\sum_{k=1}^{K} p_{mk} \\log p_{mk}\n\\] Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.\nMisclassification Error: Misclassification error focuses solely on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node \\(m\\), it is defined as \\[\nH(Q_m) = 1 - \\max_k p_{mk},\n\\] where \\(\\max_k p_{mk}\\) is the largest proportion of samples belonging to any class \\(k\\).\n\n\n\n10.1.3.2 Regression Criteria\nIn decision tree regression, different criteria are used to assess the quality of a split. The goal is to minimize the spread or variance of the target variable within each node.\n\nMean Squared Error (MSE): Mean squared error is the most common criterion used in regression trees. It measures the average squared difference between the actual values and the predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} (y_i - \\bar{y}_m)^2,\n\\] where\n\n\\(y_i\\) is the actual value for sample \\(i\\);\n\\(\\bar{y}_m\\) is the mean value of the target at node \\(m\\);\n\\(n_m\\) is the number of samples at node \\(m\\).\n\nMSE works well when the target is continuous and normally distributed.\nHalf Poisson Deviance (for count targets): When dealing with count data, the Poisson deviance is used to model the variance in the number of occurrences of an event. It is well-suited for target variables representing counts (e.g., number of occurrences of an event). At node \\(m\\), it is \\[\nH(Q_m) = \\sum_{i=1}^{n_m} \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right),\n\\] where \\(\\hat{y}_i\\) is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.\nMean Absolute Error (MAE): Mean absolute error is another criterion that minimizes the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node \\(m\\), it is \\[\nH(Q_m) = \\frac{1}{n_m} \\sum_{i=1}^{n_m} |y_i - \\bar{y}_m|\n\\] MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.\n\n\n\n10.1.3.3 Summary\nIn decision trees, the choice of splitting criterion depends on the type of task (classification or regression) and the nature of the data. For classification tasks, the Gini index and entropy are the most commonly used, with Gini offering simplicity and speed, and entropy providing a more theoretically grounded approach. Misclassification error can be used for simpler cases. For regression tasks, MSE is the most popular choice, but Poisson deviance and MAE are useful for specific use cases such as count data and robust models, respectively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees-demonstration",
    "href": "supervised.html#decision-trees-demonstration",
    "title": "10  Supervised Learning",
    "section": "10.2 Decision Trees: Demonstration",
    "text": "10.2 Decision Trees: Demonstration",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#decision-trees-demonstration-1",
    "href": "supervised.html#decision-trees-demonstration-1",
    "title": "10  Supervised Learning",
    "section": "10.3 Decision Trees: Demonstration",
    "text": "10.3 Decision Trees: Demonstration\nThis section was presented by Jaden Astle.\n\n10.3.1 About the Trees\n\nSupervised learning algorithm\nBoth classification & regression methods\nGoal is to sort data into specific groups, one characteristic at a time\nSplitting follows a tree-like structure with binary splits\n\n\n\n10.3.2 Classification Trees Overview\n\nSplits observations based on binary classifications (categorical variables)\nex. Contains “Congratulations!”, has fur, has a pool, etc.\nNote: categorical variables that are not binary need to be processed with One-Hot Encoding before training\n\nHow exactly are these splits chosen?\n\n\n10.3.3 Splitting the Variables at Each Node\n\n10.3.3.1 Entropy\n\nMeasure of randomness or disorder in the given environment.\nObserves uncertainty of data based on distribution of classes at any given step.\nA lower entropy value means less disorder, therefore a better split.\n\n\n\n10.3.3.2 Gini Index\n\nUtilizes the probability that a random element is incorrectly labeled based on labeling from the original dataset distribution.\nA lower Gini index means a better split.\n\n\n\n\n10.3.4 Classification Tree Implementation\n\n\n10.3.5 Data Preparation\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_feather('data/nyccrashes_cleaned.feather')\n\nvehicle_columns = ['vehicle_type_code_1', 'vehicle_type_code_2',\n 'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5']\ndf['number_of_vehicles_involved'] = df[vehicle_columns].notna().sum(axis=1)\n\nscaler = StandardScaler()\ndf[['latitude', 'longitude']] = scaler.fit_transform(\n    df[['latitude', 'longitude']])\n\ndf['contributing_factor_vehicle_1'] = df['contributing_factor_vehicle_1'].str.lower().str.replace(' ', '_')\ndf = pd.get_dummies(df, \ncolumns=['borough', 'contributing_factor_vehicle_1'],\ndrop_first=True)\n\n\n\n10.3.6 Data Preparation\n\nfeatures = ['latitude', 'longitude', 'number_of_vehicles_involved'] \nfeatures += [col for col in df.columns if 'borough_' in col] \nfeatures += [col for col in df.columns if 'contributing_factor_vehicle_1_' in col]\n\ndf['severe'] = (\n    (df['number_of_persons_killed'] &gt;= 1) | (df['number_of_persons_injured'] &gt;= 1)).astype(int)\n\ndf = df.dropna(subset=features)\n\nX = df[features]\ny = df['severe']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n\n\n\n10.3.7 Entropy-based Model\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ntree = DTC(criterion='entropy', max_depth=15)\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[143 147]\n [114 113]]\n              precision    recall  f1-score   support\n\n           0       0.56      0.49      0.52       290\n           1       0.43      0.50      0.46       227\n\n    accuracy                           0.50       517\n   macro avg       0.50      0.50      0.49       517\nweighted avg       0.50      0.50      0.50       517\n\n\n\n\n\n10.3.8 Gini-based Model\n\nfrom sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ntree = DTC(criterion='gini', max_depth=15)\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n[[170 120]\n [136  91]]\n              precision    recall  f1-score   support\n\n           0       0.56      0.59      0.57       290\n           1       0.43      0.40      0.42       227\n\n    accuracy                           0.50       517\n   macro avg       0.49      0.49      0.49       517\nweighted avg       0.50      0.50      0.50       517\n\n\n\n\n\n10.3.9 Regression Trees Overview\n\nBuilt for continuous variables; predicts continuous values\nRather than have the split based on Gini or Entropy, the split is based on MSE\nCalculate all possible splits of all features; minimize total MSE\n\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum (y_i - \\bar{y})^2\n\\]\n\\[\n\\text{Total MSE} = \\frac{n_{\\text{total}}}{n_{\\text{left}}} \\cdot \\text{MSE}_{\\text{left}} + \\frac{n_{\\text{total}}}{n_{\\text{right}}} \\cdot \\text{MSE}_{\\text{right}}\n\\]\n\n\n10.3.10 Regression Trees Overview\n\nWhat about leaf nodes? What are the final classifications?\nAverage y of all remaining observations in that node becomes prediction for future observations\n\n\n\n10.3.11 Regression Tree Implementation\n\n\n10.3.12 Data Preparation\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv('data/nypd311w063024noise_by100724.csv')\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\ndf['created_date'] = pd.to_datetime(df['created_date'])\ndf['closed_date'] = pd.to_datetime(df['closed_date'])\n\ndf['hour'] = df['created_date'].dt.hour\n\ndf['tod'] = np.where((df['hour'] &gt;= 20) | (df['hour'] &lt; 6), 'Nighttime', 'Daytime')\n\ndf['dow'] = df['created_date'].dt.weekday\ndf['day_type'] = np.where(df['dow'] &lt; 5, 'Weekday', 'Weekend')\n\ndf['response_time'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 3600 #this will be the y-variable\n\ndf.columns\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_46954/2261056809.py:9: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_46954/2261056809.py:10: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\nIndex(['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',\n       'complaint_type', 'descriptor', 'location_type', 'incident_zip',\n       'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',\n       'intersection_street_1', 'intersection_street_2', 'address_type',\n       'city', 'landmark', 'facility_type', 'status', 'due_date',\n       'resolution_description', 'resolution_action_updated_date',\n       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',\n       'y_coordinate_(state_plane)', 'open_data_channel_type',\n       'park_facility_name', 'park_borough', 'vehicle_type',\n       'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name',\n       'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment',\n       'latitude', 'longitude', 'location', 'hour', 'tod', 'dow', 'day_type',\n       'response_time'],\n      dtype='object')\n\n\n\n\n10.3.13 Data Preparation\n\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.get_dummies(df, columns=['complaint_type', 'borough', 'location_type', 'address_type', 'tod', 'day_type'], drop_first=True)\n\ndf['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)\ndf['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)\n\nscaler = StandardScaler()\ndf[['latitude', 'longitude']] = scaler.fit_transform(df[['latitude', 'longitude']])\n\nfeatures = ['sin_hour', 'cos_hour', 'latitude', 'longitude', 'dow', 'tod_Nighttime', 'day_type_Weekend']\n\nfeatures += [col for col in df.columns if 'complaint_type_' in col]\nfeatures += [col for col in df.columns if 'borough_' in col]\nfeatures += [col for col in df.columns if 'location_type_' in col]\nfeatures += [col for col in df.columns if 'address_type_' in col]\n\ndf = df.dropna(subset=['latitude', 'longitude'])\n\ntraining_df = df[features]\n\ntraining_df.head()\n\n\n\n\n\n\n\n\nsin_hour\ncos_hour\nlatitude\nlongitude\ndow\ntod_Nighttime\nday_type_Weekend\ncomplaint_type_Noise - House of Worship\ncomplaint_type_Noise - Park\ncomplaint_type_Noise - Residential\n...\nborough_STATEN ISLAND\nborough_Unspecified\nlocation_type_House of Worship\nlocation_type_Park/Playground\nlocation_type_Residential Building/House\nlocation_type_Store/Commercial\nlocation_type_Street/Sidewalk\naddress_type_BLOCKFACE\naddress_type_INTERSECTION\naddress_type_UNRECOGNIZED\n\n\n\n\n0\n-0.258819\n0.965926\n-0.225836\n3.014662\n5\nTrue\nTrue\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n-0.258819\n0.965926\n-1.468384\n-0.396220\n5\nTrue\nTrue\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n-0.258819\n0.965926\n0.073297\n-0.119805\n5\nTrue\nTrue\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n-0.258819\n0.965926\n1.259327\n0.116306\n5\nTrue\nTrue\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\n-0.258819\n0.965926\n-0.873509\n0.723039\n5\nTrue\nTrue\nFalse\nFalse\nTrue\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n10.3.14 Data Preparation\n\nX = df[features]\ny = df['response_time']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n\n\n10.3.15 Model Training\n\nfrom sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ntree = DTR(max_depth=20, min_samples_split=15)\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_test)\n\nprint(mean_squared_error(y_test, y_pred))\nprint(r2_score(y_test, y_pred))\n\n3.84329959566305\n0.6251134474207172\n\n\n\n\n10.3.16 Residual plot\n\nimport matplotlib.pyplot as plt\n\nresiduals = y_test - y_pred\nplt.scatter(y_pred, residuals)\nplt.hlines(0, min(y_pred), max(y_pred), colors='r')\nplt.xlabel(\"Predicted Values (response time)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.3.17 Tree Visualization\n\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(9, 6)) \nplot_tree(tree, feature_names=X_train.columns, filled=True, rounded=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.3.18 Export Tree to Text\nfrom sklearn.tree import export_text\n\ntree_structure = export_text(tree, feature_names=list(X_train.columns))\nprint(tree_structure)\nThis will print a text version of the decision tree in a tree-like format, with each indent representing a new split.\n\n\n10.3.19 Conclusion\n\nDecision trees are effective for both classification & regression tasks\n\nclassification vs. regression trees\n\nVery flexible & easy to interpret\nEasily adjustable parameters to help prevent overfitting (max tree depth, min sample split)\n\n\n\n10.3.20 Further Readings\n\nScikit-Learn Decision Tree Documentation\n“Decision Tree Regressor, Explained: A Visual Guide with Code Examples” by Samy Baladram\nGeeksforGeeks Decision Tree Overview",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#boosted-trees",
    "href": "supervised.html#boosted-trees",
    "title": "10  Supervised Learning",
    "section": "10.4 Boosted Trees",
    "text": "10.4 Boosted Trees\nBoosted trees are a powerful ensemble technique in machine learning that combine multiple weak learners, typically decision trees, into a strong learner. Unlike bagging methods, which train trees independently, boosting fits models sequentially, with each new model correcting the errors of the previous ensemble. Gradient boosting, one of the most popular variants, optimizes a loss function by iteratively adding trees that reduce the residual errors of the current ensemble.\n\n10.4.1 Introduction\nBoosted trees build on the general concept of boosting, which aims to create a strong predictor from a series of weak predictors. In boosted trees, the weak learners are shallow decision trees, often referred to as “stumps,” and they are added sequentially to the model. At each step, a new tree focuses on the training instances that are hardest to predict, improving overall accuracy. This iterative focus on “hard-to- predict” instances is the defining characteristic of boosting.\nThe effectiveness of boosted trees has made them popular for various tasks, including classification, regression, and ranking. They also form the foundation for algorithms like XGBoost, LightGBM, and CatBoost, known for their speed and scalability.\n\n\n10.4.2 Boosting Process\nThe boosting process in gradient boosted trees builds an ensemble by adding trees iteratively, each designed to minimize the residual errors from the combined predictions of the previous trees. This iterative approach allows the model to refine its predictions by optimizing a loss function, denoted as \\(L(y, F(x))\\), where \\(y\\) is the true value and \\(F(x)\\) is the model’s prediction.\n\n10.4.2.1 Model Iteration\nThe boosting process can be delineated as follows:\n\nInitialization: Start with a base model \\(F_0(x)\\), which is usually the mean of the target variable in regression or the log odds in classification:\n\nFor regression: \\(F_0(x) = \\text{mean}(y_i)\\).\nFor classification: \\(F_0(x) = \\log \\left( \\frac{P(y=1)}{1-P(y=1)} \\right)\\).\n\nIterative Boosting:\nAt each iteration \\(m\\):\n\nCompute the pseudo-residuals, representing the negative gradient of the loss function with respect to the current model predictions. The residuals at iteration \\(m\\) are defined as:\n\\[\nr_i^{(m)} = -\\left. \\frac{\\partial L(y_i, F(x_i))}\n{\\partial F(x_i)} \\right|_{F(x) = F_{m-1}(x)}.\n\\]\nThe residuals guide the next tree to focus on reducing the largest errors from the previous iteration.\nFit a new tree \\(h_m(x)\\) to the pseudo-residuals. The new tree is trained to predict the residuals of the current ensemble model, identifying where the model needs the most improvement.\nUpdate the model as the sum of the previous model and the newly added tree, scaled by a learning rate \\(\\eta\\):\n\\[\nF_m(x) = F_{m-1}(x) + \\eta \\, h_m(x).\n\\]\nThe learning rate, a small positive number (e.g., 0.01 to 0.1), controls the contribution of each tree, ensuring incremental improvements and reducing the risk of overfitting.\n\nFinal Model:\nAfter \\(M\\) iterations, the ensemble model is given by:\n\\[\nF_M(x) = F_0(x) + \\sum_{m=1}^M \\eta \\, h_m(x).\n\\]\nThe final model \\(F_M(x)\\) represents the sum of the initial model and the incremental improvements made by each of the \\(M\\) trees, with each tree trained to correct the residuals of the ensemble up to that point.\n\n\n\n\n10.4.3 Key Concepts\n\nLoss Function: The loss function measures the discrepancy between the actual and predicted values. It guides the model updates. Common choices include:\n\nSquared error for regression: \\(L(y, F(x)) = \\frac{1}{2} (y - F(x))^2\\).\nLogistic loss for binary classification: \\(L(y, F(x)) = \\log(1 + \\exp(-y \\, F(x)))\\).\n\nLearning Rate: The learning rate scales the contribution of each tree and helps control the speed of learning. A smaller learning rate typically requires more trees but results in a more robust model with better generalization.\nRegularization: Boosted trees incorporate regularization to avoid overfitting, including:\n\nTree depth: Limits the maximum depth of each tree, reducing model complexity.\nL1/L2 penalties: Regularize the weights of the trees, similar to Lasso and Ridge regression.\nSubsampling: Uses a fraction of the training data at each iteration, making the model more robust to overfitting and improving generalization.\n\n\n\n\n10.4.4 Why Boosting Works\nThe iterative approach of boosting, focusing on correcting the errors of the ensemble at each step, distinguishes gradient boosting from other ensemble methods like bagging or random forests. Key reasons for its effectiveness include:\n\nError Correction: By focusing on the hardest-to-predict instances, boosting gradually improves model accuracy, leading to better performance than models trained independently.\nWeighted Learning: Boosting adjusts the weights of training samples based on errors, ensuring that the model learns disproportionately from difficult cases, reducing bias.\nFlexibility: Boosted trees can handle various loss functions, making them suitable for different types of tasks, including regression, classification, and ranking.\n\n\n\n10.4.5 Applications and Popular Implementations\nBoosted trees are widely used in real-world applications, ranging from financial risk modeling to predictive maintenance. They are also favored in machine learning competitions due to their interpretability and robustness. Popular implementations include:\n\nXGBoost: Known for its speed and performance, with features like regularization, column sampling, and advanced tree pruning.\nLightGBM: Optimized for speed and scalability, using histogram- based algorithms to handle large datasets efficiently.\nCatBoost: Effective with categorical features, using advanced encoding techniques and built-in support for categorical variables.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#naive-bayes",
    "href": "supervised.html#naive-bayes",
    "title": "10  Supervised Learning",
    "section": "10.5 Naive Bayes",
    "text": "10.5 Naive Bayes\nThis section was contributed by Suha Akach.\nNaive Bayes is a probabilistic classification algorithm based on Bayes’ Theorem, which is used for both binary and multiclass classification problems. It is particularly effective for high-dimensional datasets and is commonly applied in tasks like text classification, spam detection, and sentiment analysis. The algorithm is called “naive” because it assumes that all features are conditionally independent given the class label, an assumption that rarely holds in real-world data but still performs well in many cases.\n\n10.5.1 Theoretical Foundations\nThe foundation of the Naive Bayes classifier is Bayes’ Theorem, which is used to update the probability estimate of a hypothesis given new evidence. Mathematically, Bayes’ Theorem is expressed as:\n\\[\nP(y \\mid X) = \\frac{P(X \\mid y) \\, P(y)}{P(X)},\n\\]\nwhere:\n\n\\(P(y \\mid X)\\): Posterior probability of class \\(y\\) given the input features \\(X\\).\n\\(P(X \\mid y)\\): Likelihood of observing \\(X\\) given that the class is \\(y\\).\n\\(P(y)\\): Prior probability of the class \\(y\\).\n\\(P(X)\\): Marginal probability of the feature vector \\(X\\).\n\n\n10.5.1.1 Naive Assumption and Likelihood Decomposition\nThe algorithm makes the simplifying assumption that features in \\(X\\) are conditionally independent given the class \\(y\\). This assumption enables the likelihood \\(P(X \\mid y)\\) to be decomposed as:\n\\[\nP(X \\mid y) = \\prod_{i=1}^n P(x_i \\mid y),\n\\]\nwhere \\(X = \\{x_1, x_2, \\ldots, x_n\\}\\) represents the feature vector with \\(n\\) features, and \\(P(x_i \\mid y)\\) is the conditional probability of feature \\(x_i\\) given the class \\(y\\).\nThe model parameters are the prior probabilities \\(P(y)\\) and the conditional probabilities \\(P(x_i \\mid y)\\). These are estimated from the training data using the maximum likelihood estimation (MLE):\n\nPrior Estimation: The prior probability \\(P(y)\\) is estimated as the proportion of training samples in class \\(y\\):\n\\[\n\\hat{P}(y) = \\frac{\\text{count}(y)}{N},\n\\]\nwhere \\(\\text{count}(y)\\) is the number of instances belonging to class \\(y\\), and \\(N\\) is the total number of training samples.\nConditional Probability Estimation:\n\nCategorical Features: For discrete or categorical features, the conditional probability \\(P(x_i \\mid y)\\) is estimated as:\n\\[\n\\hat{P}(x_i \\mid y) = \\frac{\\text{count}(x_i, y)}{\\text{count}(y)},\n\\]\nwhere \\(\\text{count}(x_i, y)\\) is the number of samples in class \\(y\\) that have feature \\(x_i\\).\nContinuous Features: For continuous features, Naive Bayes commonly assumes a Gaussian distribution. In this case, \\(P(x_i \\mid y)\\) is modeled using the Gaussian distribution with mean \\(\\mu_{y,i}\\) and variance \\(\\sigma_{y,i}^2\\):\n\\[\nP(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y,i}^2}} \\exp \\left(\n-\\frac{(x_i - \\mu_{y,i})^2}{2\\sigma_{y,i}^2} \\right).\n\\]\nThe parameters \\(\\mu_{y,i}\\) and \\(\\sigma_{y,i}^2\\) are estimated from the training data using the sample mean and variance for each feature in each class.\n\n\n\n\n10.5.1.2 Class Prediction\nThe goal of the Naive Bayes classifier is to predict the class \\(y\\) that maximizes the posterior probability \\(P(y \\mid X)\\). After applying Bayes’ Theorem and dropping the constant denominator \\(P(X)\\), the decision rule becomes:\n\\[\ny^* = \\arg\\max_y \\, P(y) \\prod_{i=1}^n P(x_i \\mid y).\n\\]\nIn practice, the log of the posterior is used to prevent numerical underflow:\n\\[\n\\log P(y \\mid X) = \\log P(y) + \\sum_{i=1}^n \\log P(x_i \\mid y).\n\\]\nThe predicted class is the one that maximizes this expression.\n\n\n10.5.1.3 Surprisingly Good Performance\nAlthough the assumption of conditional independence among features is often unrealistic, Naive Bayes still performs well for several reasons:\n\nRobustness to Violations of Independence: Literature suggests that Naive Bayes can achieve good classification performance even when features are correlated, as long as the dependencies are consistent across classes (Domingos & Pazzani, 1997). This is because the decision boundaries produced by Naive Bayes are often well-aligned with the true boundaries, despite the imprecise probability estimates.\nDecision Rule Effectiveness: Since Naive Bayes focuses on finding the class that maximizes the posterior probability, it is less sensitive to errors in individual probability estimates, as long as the relative ordering of probabilities remains correct (Rish, 2001).\nZero-One Loss Minimization: Naive Bayes aims to minimize the zero-one loss, i.e., the number of misclassifications. The method benefits from the fact that exact probability estimation is not essential for accurate classification, as the correct class can still be chosen even with approximate probabilities (Ng & Jordan, 2001).\nHigh-Dimensional Settings: In high-dimensional settings, the conditional independence assumption can act as a form of implicit regularization, preventing overfitting by simplifying the probability model (Rish, 2001). This makes Naive Bayes particularly well-suited for text classification and other sparse feature spaces.\n\n\n\n10.5.1.4 Advantages and Limitations\nAdvantages:\n\nComputationally efficient, with linear time complexity in terms of the number of features and data samples.\nPerforms well on large datasets, especially when features are conditionally independent.\nSuitable for high-dimensional data, making it popular in text classification.\n\nLimitations:\n\nRelies on the assumption of conditional independence, which may not hold in real-world datasets, potentially affecting performance.\nIt is sensitive to zero probabilities; if a feature value never appears in the training set for a given class, its likelihood becomes zero. To address this, Laplace smoothing (or add-one smoothing) is often applied.\n\n\n\n10.5.1.5 Laplace Smoothing\nLaplace smoothing is used to handle zero probabilities in the likelihood estimation. It adds a small constant $ $ (usually 1) to the count of each feature value, preventing the probability from becoming zero:\n\\[\nP(x_i \\mid y) = \\frac{\\text{count}(x_i, y) + \\alpha}\n{\\sum_{x_i'} (\\text{count}(x_i', y) + \\alpha)}.\n\\]\nThis adjustment ensures that even unseen features in the training data do not lead to zero probabilities, thus improving the model’s robustness.\n\n\n\n10.5.2 Types of Naive Bayes:\nThere are 5 types of Naive Bayes classifiers:\n\nGaussian Naive Bayes: This type of Naive Bayes is used when the dataset consists of numerical features. It assumes that the features follow a Gaussian (normal) distribution. This model is fitted by finding the mean and standard deviation of each class (IBM, 2024).\nCategorical Naive Bayes: When the dataset contains categorical features, we use Categorical Naive Bayes. It assumes that each feature follows a categorical distribution.\nBernoulli Naive Bayes: Bernoulli Naive Bayes is applied when the features are binary or follow a Bernoulli distribution. That is, variables with two values, such as True and False or 1 and 0. (IBM, 2024).\nMultinomial Naive Bayes: Multinomial Naive Bayes is commonly used for text classification tasks. It assumes that features represent the frequencies or occurrences of different words in the text.\nComplement Naive Bayes: Complement Naive Bayes is a variation of Naive Bayes that is designed to address imbalanced datasets. It is particularly useful when the majority class overwhelms the minority class in the dataset. It aims to correct the imbalance by considering the complement of each class when making predictions (GeeksforGeeks, 2023).\n\nEach type of Naive Bayes classifier is suitable for different types of datasets based on the nature of the features and their distribution. By selecting the appropriate Naive Bayes algorithm, we can effectively model and classify data based on the given features.\n\n\n10.5.3 Naive Bayes w/ NYC Crash Data\nSince we have an imbalanced dataset where there are more non severe crashes than severe, we will use Complement Naive Bayes classifier to predict severe crashes based on our predictors.\nOur assumed independent predictors after feature engineering are: borough, location, household_median_income, crash_date, crash_time, time_category, contributing_factor_vehicle_1, vehicle_type_code_1 .\nWe assume a crash is severe if there are more than 0 persons killed and/or injured.\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport uszipcode as us\n\n# Disable warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the dataset\ndf = pd.read_feather('data/nyccrashes_cleaned.feather')\n\n# 1. Separate crash_datetime into date and time (convert datetime into numeric features)\ndf['crash_datetime'] = pd.to_datetime(df['crash_datetime'])\ndf['crash_date'] = df['crash_datetime'].dt.date\ndf['crash_time'] = df['crash_datetime'].dt.time\n\n# Extract relevant features from datetime (for example: hour)\ndf['hour'] = df['crash_datetime'].dt.hour\n\n# 2. Create time_category column with updated time intervals\ndef categorize_time(hour):\n    if 0 &lt;= hour &lt; 6:\n        return 'midnight'  # 12:00 AM to 5:59 AM\n    elif 6 &lt;= hour &lt; 12:\n        return 'morning'  # 6:00 AM to 11:59 AM\n    elif 12 &lt;= hour &lt; 18:\n        return 'afternoon'  # 12:00 PM to 5:59 PM\n    elif 18 &lt;= hour &lt; 21:\n        return 'evening'  # 6:00 PM to 8:59 PM\n    else:\n        return 'night'  # 9:00 PM to 11:59 PM\n\ndf['time_category'] = df['hour'].apply(categorize_time)\n\n# 3. Add median household income for each zip code using the uszip package\ndef get_median_income(zipcode):\n    try:\n        z = us.search.by_zipcode(str(zipcode))\n        if z:\n            return z.median_income\n        else:\n            return np.nan\n    except:\n        return np.nan\n\ndf['household_median_income'] = df['zip_code'].apply(get_median_income)\n\n\n10.5.3.1 Defining predictors and target variable.\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.naive_bayes import ComplementNB  # Complement Naive Bayes\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Target variable for severe crashes based on number of persons killed and injured\ndf['severe_crash'] = (df['number_of_persons_killed'] &gt; 0) | (df['number_of_persons_injured'] &gt; 0).astype(int)\n\n# Selected predictors\npredictors = ['borough', 'location', 'household_median_income', \n              'crash_date', 'crash_time', 'time_category',\n              'contributing_factor_vehicle_1', 'vehicle_type_code_1']\n\n# Convert categorical columns into dummy variables\nX = pd.get_dummies(df[predictors], drop_first=True)\n\n# Handle NaN or missing values in numeric columns if necessary\nX.fillna(0, inplace=True)\n\n# Target variable\ny = df['severe_crash']\n\n\n\n10.5.3.2 Fitting Our Model\nAfter accounting for imbalanced data using SMOTE, we find the k nearest neighbors in the minority class to generate synthetic points between the chosen point and its neighbors. In this case, 7 nearest neighbors are considered to generate synthetic samples. We also use alpha 0.5 for our laplace smoothing to apply an equal level of smoothing across all feature probabilities. Finally, we apply a threshold to check if the probability for class 1 (positive class) is greater than 0.4, and if so, assigns the sample to class 1, otherwise to class 0.\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3255)\n\n# Resample to balance the classes (SMOTE)\nsmote = SMOTE(random_state=3255, k_neighbors=7)\nX_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n\n# Initialize the Complement Naive Bayes model with Laplace smoothing\nmodel = ComplementNB(alpha=0.5)\n\n# Fit the model\nmodel.fit(X_train_res, y_train_res)\n\n# Get the probabilities for each class\ny_prob = model.predict_proba(X_test)\n\n# Apply threshold to the predicted probabilities\ny_pred = (y_prob[:, 1] &gt; 0.4).astype(int)\n\n# Evaluate the model with the new threshold\nconf_matrix = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nClassification Report:\")\nprint(report)\n\nConfusion Matrix:\n[[111  96]\n [ 64 104]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       False       0.63      0.54      0.58       207\n        True       0.52      0.62      0.57       168\n\n    accuracy                           0.57       375\n   macro avg       0.58      0.58      0.57       375\nweighted avg       0.58      0.57      0.57       375\n\n\n\n\n\n10.5.3.3 Interpreting our Results:\nRecall vs. Precision: Our model is better at identifying severe crashes (higher recall of 0.62) but is less accurate when it does so (lower precision of 0.52). This means that while the model catches a good portion of the severe crashes, it also misidentifies a fair number of non-severe crashes as severe.\nF1-Score: The F1-score of 0.57 for severe crashes is a balanced measure, showing that our model is moderately effective at distinguishing severe crashes from non-severe ones.\nImprovement Opportunities: The overall accuracy of 58% is moderate, so there’s potential for further optimization.\nIn conclusion, our CNB model is fairly good at predicting severe crashes with an accuracy of 58% but may benefit from more tuning to improve precision and reduce false positives, especially in the context of an imbalanced dataset where we don’t have many instances of people injured or killed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "supervised.html#handling-imbalanced-data-with-smote",
    "href": "supervised.html#handling-imbalanced-data-with-smote",
    "title": "10  Supervised Learning",
    "section": "10.6 Handling Imbalanced Data with SMOTE",
    "text": "10.6 Handling Imbalanced Data with SMOTE\nThis section is presented by Olivia Kashalapov.\n\n10.6.1 Introduction\nThis presentation on SMOTE will cover the following topics:\n\nWhat is class imbalance and why is it important?\nWhat is SMOTE?\nWhy do we use it and how does it work?\nWhy is SMOTE better than other traditional methods for handling class imbalance?\nUsing SMOTE in data visualization, analysis, model training, and evaluation.\nThe downsides and limitations of SMOTE.\n\n\n\n10.6.2 Class Imbalance\nBefore we can learn about SMOTE, we have to understand class imbalance and why it is important. - Class imbalance is a common issue in many datasets in which the distribution of examples within the dataset are either biased or skewed. - Let’s say there is a dataset for a rare medical diagnosis and there are two classes, with disease and without disease. The data can be taken to explore if there is a certain variable that makes it more likely for an individual to be diagnosed with this rare disease. Since the disease is rare, the class of people with the disease is going to be significantly smaller than the class of those without. In this case, the data will be skewed towards the class of people without the disease and this may skew the findings of the predictive model. - Addressing class imbalance improves the performance of models and increases model accuracy. - Unfortunately, “most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.” – this is where SMOTE comes in!\n\n\n10.6.3 Synthetic Model Oversampling Technique\n\nSynthetic Model Oversampling Technique, better known as SMOTE, is an algorithm that focuses on the feature space (already existing data points) in the minority class to generate new data points to create balance between the majority and minority classes.\nHere is how it works:\n\nIt identifies imbalance in the dataset and recognizes the minority class.\nUsing an existing data point in the minority class, it takes the difference between the point and a nearest neighbor.\nIt then multiplies the difference by random number between 1 and 0.\nThis difference is then added to the sample to generate new synthetic example in the featured space (minority class).\nThis continues with next nearest neighbor up to user-defined number (k), In other words, there are k synthetic points created between the selected existing point and its k nearest neighbors.\nThis is repeated for all data points within the minority class\n\nIn simpler terms, it creates synthetic data points in the minority class by creating points that lie in between pre-existing ones\nSMOTE works well because it attempts to remove bias on skewed distributions and balances the data using pre-existing data points within the dataset. It uses the data already being used to create realistic randomized data points.\n\n\n\n10.6.4 SMOTE versus Traditional Methods\n\nThere are other ways to handle class imbalance within data sets, other than SMOTE\nOne method of this is random under sampling the majority class where random points are chosen in the majority class to be discarded. This often leaves out too much data which could be important for training the predictive model and there are chances that the remaining sample ends up being biased.\nAnother option is random oversampling the minority class which is done by randomly duplicating points within the minority class. Although this fixes the class imbalance, it could also lead to overfitting of the model, making it less accurate to the true population.\nSMOTE mitigates the problems of random oversampling and under sampling since the generated data points are not replications of already occurring instances and the majority class keeps all of its existing instances. It is much more unlikely for there to be a case of overfitting the model and no useful information will be left out of the model either.\n\n\n\n10.6.5 Installation and Setup\nTo install SMOTE, you can type one of two commands into your terminal:\npip install imbalanced-learn\n# OR\nconda install imbalanced-learn\nTo import SMOTE on Python, you use this command:\n\nfrom imblearn.over_sampling import SMOTE\n\nJust like that, you are ready to use SMOTE!\n\n\n10.6.6 Data Preparation\nHere I am creating a simple data set in which there is an extremely apparent class imbalance. I am doing this rather than using past data sets so that you can truly see the work of SMOTE without other factors that can make the process confusing.\n\nimport pandas as pd\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic dataset\ndata_size = 1000\nclass_0_size = int(data_size * 0.9)  # 90% for class 0\nclass_1_size = data_size - class_0_size  # 10% for class 1\n\n# Class 0: Majority class\nfeature1_class_0 = np.random.normal(0, 1, class_0_size)\nfeature2_class_0 = np.random.normal(1, 1, class_0_size)\ntarget_class_0 = np.zeros(class_0_size)\n\n# Class 1: Minority class\nfeature1_class_1 = np.random.normal(2, 1, class_1_size)\nfeature2_class_1 = np.random.normal(2, 1, class_1_size)\ntarget_class_1 = np.ones(class_1_size)\n\n# Combine the majority and minority class\nfeature1 = np.concatenate([feature1_class_0, feature1_class_1])\nfeature2 = np.concatenate([feature2_class_0, feature2_class_1])\ntarget = np.concatenate([target_class_0, target_class_1])\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'feature1': feature1,\n    'feature2': feature2,\n    'target': target\n})\n\n# Display the first few rows\nprint(data.head())\n\n# Save the dataset as CSV for further use\ndata.to_csv('synthetic_class_imbalance_dataset.csv', index=False)\n\n   feature1  feature2  target\n0  0.496714  1.368673     0.0\n1 -0.138264  0.606661     0.0\n2  0.647689  1.028745     0.0\n3  1.523030  2.278452     0.0\n4 -0.234153  1.191099     0.0\n\n\n\n\n10.6.7 Data Visualization\nBefore using SMOTE to balance the classes, we can view the distribution of the minority and majority classes. It is quite evident that class 0 has many more instances when compared to class 1. This means any predictive models made with this exact data are likely to be skewed towards class 0.\n\nimport matplotlib.pyplot as plt\n\n# Visualize the class distribution\ndata['target'].value_counts().plot(kind='bar', color=['blue', 'orange'])\nplt.title('Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.6.8 Implementing SMOTE\nNow we can use SMOTE on the dataset I created to handle the imbalance between the classes.\n\n# Split the dataset into features (X) and target (y)\nX = data[['feature1', 'feature2']]\ny = data['target']\n\n# Check the initial class distribution\nprint(y.value_counts())\n\nfrom imblearn.over_sampling import SMOTE\n\n# Initialize SMOTE with custom parameters\nsmote = SMOTE(\n    sampling_strategy=1,  # Resample minority to 100% of majority class\n    k_neighbors=5,          # Use 5 nearest neighbors to generate synthetic samples\n    random_state=42,        # Set a random state for reproducibility\n)\n\n# Apply SMOTE to the dataset\nX_resampled, y_resampled = smote.fit_resample(X, y)\nprint(pd.Series(y_resampled).value_counts())\n\ntarget\n0.0    900\n1.0    100\nName: count, dtype: int64\ntarget\n0.0    900\n1.0    900\nName: count, dtype: int64\n\n\nIn this example of SMOTE application, I am utilizing multiple customized parameters. Without these specifications, the SMOTE() command will resample the minority to have the same number of instances as the majority class and utilize the 5 nearest neighbors to generate these samples. Without a specified random state, SMOTE will choose one so it is recommended to include that paramter for reproducibility.\n\n\n10.6.9 Visualization after SMOTE\nKeep in mind, the minority class will remain smaller than the majority due to the sampling_strategy parameter included in the previous slide.\n\nimport matplotlib.pyplot as plt\n\n# Visualize the class distribution after SMOTE\npd.Series(y_resampled).value_counts().plot(kind='bar', color=['blue', 'orange'])\nplt.title('Class Distribution After SMOTE')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\nMuch better!\n\n\n10.6.10 Model Training using SMOTE\nNow that the dataset is balanced, we can train the machine learning model. In this case, I am using logistic regression, which works well in many binary cases.\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Split the resampled data into training and testing sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# Initialize and train a Logistic Regression model\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = log_reg.predict(X_test)\n\n# Print the confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Print the classification report (includes precision, recall, F1-score)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\nConfusion Matrix:\n[[227  40]\n [ 29 244]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n         0.0       0.89      0.85      0.87       267\n         1.0       0.86      0.89      0.88       273\n\n    accuracy                           0.87       540\n   macro avg       0.87      0.87      0.87       540\nweighted avg       0.87      0.87      0.87       540\n\n\n\nThere were 227 cases in which the model correctly predicted class 1 (true positive) and 244 cases in which the model correctly predicted class 0 (true negative). There were 40 cases in which class 1 was predicted, but it was class 0. Lastly, there were 29 cases in which class 0 was predicted, but it was class 1.\nThe accuracy of this model is 87%, which means it correctly predicts the class 87% of the time.\n\n\n10.6.11 Model Evaluation\nSo how good is this model actually? Here I am going to use the ROC curve and AUC, since the last slide already touched on accuracy and confusion matrix results.\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Compute predicted probabilities for ROC curve\ny_prob = log_reg.predict_proba(X_test)[:, 1]\n\n# Generate ROC curve values\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (AUC = {:.2f})'.format(roc_auc_score(y_test, y_prob)))\nplt.plot([0, 1], [0, 1], linestyle='--', label='Random Guessing')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n\n# Compute AUC\nauc = roc_auc_score(y_test, y_prob)\nprint(\"AUC:\", auc)\n\n\n\n\n\n\n\n\nAUC: 0.9407197047646485\n\n\nThe area under the curve (AUC) determines how good a model is at distinguishing between the positive and negative classes, with a score of 1 being perfect. In this case, with an AUC of 0.941, the model is extremely good at making these distinguishments.\n\n\n10.6.12 Another Example Using NYC Crash Data\nFor the NYC Crash Severity Prediction homework we did, SMOTE came in helpful when it came to creating synthetic data in our model predictors. Classes like ‘contributing_factor_vehicle_4’ and ‘vehicle_type_code_5’ were missing a lot of data, making our prediction models very skewed.\nHere is how I used SMOTE to fix this issue.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\n\n# Identify merged data\nmerged_df = pd.read_feather(\"data/nyccrashes_merged.feather\")\n\nmerged_df['Severe Crash'] = ((merged_df['number_of_persons_killed'] &gt; 0) | \n                        (merged_df['number_of_persons_injured'] &gt; 1)).astype(int)\n\n# Select predictors\npredictors = ['borough', 'on_street_name', 'cross_street_name', \n              'off_street_name', 'contributing_factor_vehicle_1',\n              'contributing_factor_vehicle_2', \n              'contributing_factor_vehicle_3',\n              'contributing_factor_vehicle_4', \n              'contributing_factor_vehicle_5',\n              'vehicle_type_code_1', 'vehicle_type_code_2', \n              'vehicle_type_code_3', 'vehicle_type_code_4', \n              'vehicle_type_code_5', \n              'median_home_value', 'median_household_income']\n\n# Initialize data\nX = pd.get_dummies(merged_df[predictors], drop_first=True)\ny = merged_df['Severe Crash']\n\n# Impute any missing values\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, \n                                                    y, test_size=0.2, \n                                                    random_state=1234)\n\n# Apply SMOTE to the training data\nsmote = SMOTE(random_state=1234)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n\n\n10.6.13 NYC Crash Data Part 2\nNow we can continue on to logistic regression modeling and evaluating the accuracy of our predictive model with a balanced dataset!\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, confusion_matrix,\n    f1_score, roc_curve, auc\n)\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Fit the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train_resampled, y_train_resampled)\n\n# Predict labels on the test set\ny_pred = model.predict(X_test)\n\n# Get predicted probabilities for ROC curve and AUC\n# Probability for the positive class\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Print confusion matrix and metrics\nprint(\"Confusion Matrix:\\n\", cm)\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\n\nConfusion Matrix:\n [[228  86]\n [ 20  16]]\nAccuracy: 0.70\nPrecision: 0.16\nRecall: 0.44\n\n\nThis predictive model correctly determined whether a NYC car crash was severe or not 83% of the time, which is decently accurate. Without the use of SMOTE, this number would have been much lower.\n\n\n10.6.14 Limitations and Challenges of SMOTE\n\nIt does not take into consideration other neighboring classes, which can cause overlap between classes\nIt is also not the most effective for high-dimensional data (another reason I made a sample dataset rather than using one with many predicitors)\nThere is no consideration of the quality of the synthetic samples\nIt is only suitable for continuous variables\nYour choice of k can severely impact the quality of the synthetic data\n\n\n\n10.6.15 Conclusion\n\nSMOTE is a tool used to handle class imbalance in datasets\nIt creates synthetic data points utilizing instances already in the minority class\nThis creates a balanced data set which can improve model prediction and be used for a variety of machine learning applications\n\n\n\n10.6.16 Further Readings\nBrownlee, J. (2020). A gentle introduction to imbalanced classification\nBrownlee, J. (2021). Smote for imbalanced classification with python\nGalli, S. (2023). Overcoming class imbalance with SMOTE: How to tackle imbalanced datasets in Machine Learning\nImbalanced data : How to handle imbalanced classification problems. (2023)\nMaklin, C. (2022). Synthetic minority over-sampling technique (smote)\nOr, D. B. (2024). Solving the class imbalance problem\nSmote, Package Imbalanced Learning Manual\n\n\n\n\nBreiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984). Classification and regression trees. Wadsworth.\n\n\nDomingos, P., & Pazzani, M. (1997). On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning, 29(2-3), 103–130.\n\n\nGeeksforGeeks. (2023). Complement naive bayes (CNB) algorithm. https://www.geeksforgeeks.org/complement-naive-bayes-cnb-algorithm/\n\n\nIBM. (2024). What are naïve bayes classifiers? https://www.ibm.com/topics/naive-bayes\n\n\nNg, A., & Jordan, M. (2001). On discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, 841–848.\n\n\nRish, I. (2001). An empirical study of the naive Bayes classifier. IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence, 3, 41–46.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "11  Unsupervised Learning",
    "section": "",
    "text": "11.1 Principle Component Analysis\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset with potentially correlated features into a set of uncorrelated components. These components are ordered by the amount of variance each one captures, allowing PCA to simplify the data structure while retaining the most informative features. This approach is widely used in unsupervised learning, particularly for data compression and noise reduction.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#principle-component-analysis",
    "href": "unsupervised.html#principle-component-analysis",
    "title": "11  Unsupervised Learning",
    "section": "",
    "text": "11.1.1 Theory\nPCA works by identifying directions, or “principal components,” along which the variance of the data is maximized. Let \\(X\\) be a dataset with \\(n\\) observations and \\(p\\) features, represented as an \\(n \\times p\\) matrix. The principal components are derived from the eigenvectors of the data’s covariance matrix, indicating the directions in which the data exhibits the most variation.\n\nStandardization: To ensure that each feature contributes equally to the analysis, the features in \\(X\\) are often standardized to have zero mean and unit variance. This prevents features with larger scales from dominating the principal components.\nCovariance Matrix: The covariance matrix \\(S\\) of the dataset is computed as:\n\\[\nS = \\frac{1}{n-1} X^T X.\n\\]\nThis matrix captures the pairwise covariances between features, showing how they vary together.\nEigenvalue Decomposition: PCA proceeds by calculating the eigenvalues and eigenvectors of the covariance matrix \\(S\\). The eigenvectors represent the principal components, and the eigenvalues measure the amount of variance each component captures.\nDimensionality Reduction: By selecting the top \\(k\\) eigenvectors (those with the largest eigenvalues), the data can be projected into a lower-dimensional space that retains the \\(k\\) most significant components:\n\\[\nX_{\\text{reduced}} = X W_k,\n\\]\nwhere \\(W_k\\) is the matrix containing the top \\(k\\) eigenvectors.\n\n\n\n11.1.2 Properties of PCA\nPCA has several important properties that make it valuable for unsupervised learning:\n\nVariance Maximization: The first principal component is the direction that maximizes variance in the data. Each subsequent component maximizes variance under the constraint of being orthogonal to previous components.\nOrthogonality: Principal components are orthogonal to each other, ensuring that each captures unique information. This property transforms the data into an uncorrelated space, simplifying further analysis.\nDimensionality Reduction: By selecting only components with the largest eigenvalues, PCA enables dimensionality reduction while preserving most of the data’s variability. This is especially useful for large datasets.\nReconstruction: If all components are retained, the original data can be perfectly reconstructed. When fewer components are used, the reconstruction is approximate but retains the essential structure of the data.\nSensitivity to Scaling: PCA is sensitive to the scale of input data, so standardization is often necessary to ensure that each feature contributes equally to the analysis.\n\n\n\n11.1.3 Interpreting PCA Results\nThe output of PCA provides several insights into the data:\n\nPrincipal Components: Each principal component represents a linear combination of the original features. The loadings (or weights) for each feature indicate the contribution of that feature to the component. Large weights (positive or negative) suggest that the corresponding feature strongly influences the principal component.\nExplained Variance: Each principal component captures a specific amount of variance in the data. The proportion of variance explained by each component helps determine how many components are needed to retain the key information in the data. For example, if the first two components explain 90% of the variance, then these two components are likely sufficient to represent the majority of the data’s structure.\nSelecting the Number of Components: The cumulative explained variance plot indicates the total variance captured as more components are included. A common approach is to choose the number of components such that the cumulative variance reaches an acceptable threshold (e.g., 95%). This helps in balancing dimensionality reduction with information retention.\nInterpretation of Component Scores: The transformed data points, or “scores,” in the principal component space represent each original observation as a combination of the selected principal components. Observations close together in this space have similar values on the selected components and may indicate similar patterns.\nIdentifying Patterns and Clusters: By visualizing the data in the reduced space, patterns and clusters may become more apparent, especially in cases where there are inherent groupings in the data. These patterns can provide insights into underlying relationships between observations.\n\nPCA thus offers a powerful tool for both reducing data complexity and enhancing interpretability by transforming data into a simplified structure, with minimal loss of information.\n\n\n11.1.4 Example: PCA on 8x8 Digit Data\nThe 8x8 digit dataset contains grayscale images of handwritten digits (0 through 9), with each image represented by an 8x8 grid of pixel intensities. Each pixel intensity is a feature, so each image has 64 features in total.\n\n\n11.1.5 Loading and Visualizing the Data\nLet’s start by loading the data and plotting some sample images to get a sense of the dataset.\n\n# Import required libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\n\n# Load the 8x8 digit dataset\ndigits = load_digits()\nX = digits.data  # feature matrix with 64 features (8x8 pixel intensities)\ny = digits.target  # target labels (0-9 digit classes)\n\n# Display the shape of the data\nprint(\"Feature matrix shape:\", X.shape)\nprint(\"Target vector shape:\", y.shape)\n\n# Plot some sample images from the dataset\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X[i].reshape(8, 8), cmap='gray')\n    ax.set_title(f\"Digit: {y[i]}\")\n    ax.axis('off')\nplt.suptitle(\"Sample Images from 8x8 Digit Dataset\", fontsize=16)\nplt.show()\n\nFeature matrix shape: (1797, 64)\nTarget vector shape: (1797,)\n\n\n\n\n\n\n\n\n\nAfter loading and visualizing the data, we observe the following:\n\nEach digit is represented by a grid of 8x8 pixels, resulting in a 64-dimensional feature space.\nSince each pixel represents a separate feature, the dataset has high dimensionality relative to its visual simplicity.\n\nGiven the high dimensionality of the data, we may want to address the following research questions using PCA:\n\nCan we reduce the dimensionality of the dataset while preserving the essential structure of each digit? By reducing dimensions, we aim to simplify the data representation, which can aid in visualization and computational efficiency.\nHow many principal components are necessary to capture most of the variance in the data? Identifying this will help us understand how many features are truly informative in distinguishing the digits.\nAre there distinct clusters in the reduced space? Visualizing the data in two or three dimensions could reveal any inherent groupings or patterns related to the different digit classes.\n\n\n11.1.5.1 Performing PCA and Plotting Variance Contribution\nLet’s proceed by applying PCA to the digits data and plotting the explained variance to understand how much variance each principal component captures. This will help determine the optimal number of components to retain for a good balance between dimensionality reduction and information preservation.\nThe primary goal here is to identify the number of components that capture most of the variance. We’ll use a cumulative explained variance plot to visualize how much total variance is captured as we include more principal components.\n\n# Import the PCA module\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Initialize PCA without specifying the number of components\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Calculate the explained variance ratio for each component\nexplained_variance = pca.explained_variance_ratio_\ncumulative_variance = np.cumsum(explained_variance)\n\n# Plot the explained variance and cumulative variance\nplt.figure(figsize=(10, 5))\n\n# Plot individual explained variance\nplt.subplot(1, 2, 1)\nplt.plot(np.arange(1, len(explained_variance) + 1), explained_variance, marker='o')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Variance Contribution of Each Component')\n\n# Plot cumulative explained variance\nplt.subplot(1, 2, 2)\nplt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVariance Contribution of Each Component: The left plot shows the amount of variance explained by each individual component. This can help identify which components contribute significantly to capturing the variance in the data.\nCumulative Explained Variance: The right plot displays the cumulative explained variance as the number of components increases. This plot is useful for determining the number of components to retain. Generally, we look for a “knee” or “elbow” point where the cumulative variance starts to level off.\nTo select the number of components:\n\nSet a Variance Threshold: A typical approach is to select enough components to capture a certain percentage of the total variance (e.g., 90% or 95%).\nElbow Method: Identify a point on the cumulative variance plot where additional components contribute minimally to the variance. This “elbow” point represents an efficient number of components.\n\nIn this example, we see that the first 10 components contain approximately 75% of the variance; around 50 components are needed to describe close to 100% of the variance.\n\n\n11.1.5.2 PCA in Dimension Reduction\nLet’s continue by projecting the digit data onto the first two and first three principal components, allowing us to visualize the data in a lower-dimensional space. This will help us see how well PCA captures the structure of the data and whether distinct clusters form in the reduced space.\n\n# Apply PCA to reduce data to the first two and three components\npca_2d = PCA(n_components=2)\nX_pca_2d = pca_2d.fit_transform(X)\n\npca_3d = PCA(n_components=3)\nX_pca_3d = pca_3d.fit_transform(X)\n\n# Plotting the 2D projection\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', s=15, alpha=0.7)\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"2D PCA Projection of Digit Data\")\nplt.colorbar(scatter, label='Digit Label')\nplt.show()\n\n# Plotting the 3D projection\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], \n                     c=y, cmap='tab10', s=15, alpha=0.7)\nax.set_xlabel(\"Principal Component 1\")\nax.set_ylabel(\"Principal Component 2\")\nax.set_zlabel(\"Principal Component 3\")\nax.set_title(\"3D PCA Projection of Digit Data\")\nfig.colorbar(scatter, ax=ax, label='Digit Label')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis 3D projection of the MNIST digit data shows each image’s position in the space defined by the first three principal components. Here are some key observations:\n\nCluster Formation: Distinct clusters of points represent different digits. Digits with similar shapes, such as “1” and “7” (both often vertical), may appear closer to each other in this reduced space. This clustering suggests that PCA effectively captures structural features, even when reducing dimensions.\nEffectiveness of Dimensionality Reduction: Despite reducing from 64 dimensions to only three, PCA retains essential variance, allowing for distinction between different digits. This demonstrates PCA’s utility in data compression, providing a simplified representation without losing significant information.\nExploring Further Dimensions: Additional components could capture more variance, if required. However, the first three components often capture most of the meaningful variance, balancing dimensionality reduction with information retention.\n\nThis PCA projection shows that the MNIST digit data has underlying patterns well-represented by the first few components. These findings highlight PCA’s usefulness in compressing high-dimensional data while preserving its structure, making it a valuable tool for visualization, noise reduction, and as a pre-processing step in machine learning tasks.\n\n\n11.1.5.3 PCA in Noise Filtering\nTo demonstrate PCA’s use in noise filtering, we’ll follow these steps:\n\nAdd Random Noise: Add random noise to the original digit images.\nFit PCA to Noisy Data: Apply PCA to the noisy data, selecting enough components to retain 50% of the variance.\nReconstruct the Digits: Use PCA’s inverse transform to reconstruct the digits from the reduced components, effectively filtering out the noise.\nDisplay the Results: Show a side-by-side comparison of the original, perturbed, and reconstructed images for visual assessment.\n\n\ndef plot_digits(datasets, titles):\n    \"\"\"\n    Plots a 2x5 grid of images for each dataset in datasets,\n    using a compact and uniform layout.\n\n    Parameters:\n    - datasets: list of 2D numpy arrays, each array with shape (n_samples, 64),\n                representing different versions of the digit data \n                (e.g., original, noisy, reconstructed).\n    - titles: list of strings, titles for each dataset (e.g., [\"Original\", \"Noisy\", \"Reconstructed\"]).\n    \"\"\"\n    fig, axes = plt.subplots(len(datasets) * 2, 5, figsize=(5, 6),\n                             subplot_kw={'xticks':[], 'yticks':[]},\n                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n    \n    for row, (data, title) in enumerate(zip(datasets, titles)):\n        for i, ax in enumerate(axes[row * 2: row * 2 + 2].flat):\n            ax.imshow(data[i].reshape(8, 8), cmap='binary', interpolation='nearest', clim=(0, 16))\n        axes[row * 2, 0].set_ylabel(title, rotation=0, labelpad=30, fontsize=12, ha='right')\n\n    plt.suptitle(\"PCA Noise Filtering: Original, Noisy, and Reconstructed Digits\", fontsize=16)\n   # plt.tight_layout()\n    plt.show()\n\n# Applying the function to the original, noisy, and reconstructed datasets\n# Load the 8x8 digit dataset\ndigits = load_digits()\nX = digits.data  # Original digit data\n\n# Add random noise to the data\nnp.random.seed(0)\nnoise = np.random.normal(0, 4, X.shape)\nX_noisy = X + noise\n\n# Fit PCA to retain 50% of the variance\npca_50 = PCA(0.50)\nX_pca_50 = pca_50.fit_transform(X_noisy)\nX_reconstructed_50 = pca_50.inverse_transform(X_pca_50)\n\n# Plot the original, noisy, and reconstructed digits using the function\nplot_digits([X, X_noisy, X_reconstructed_50], [\"Original\", \"Noisy\", \"Reconstructed\"])\n\n\n\n\n\n\n\n\nThis visualization demonstrates the noise filtering effect of PCA:\n\nOriginal vs. Noisy Images: The second row shows the effect of added random noise, making the digits less recognizable.\nReconstructed Images: In the third row, PCA has filtered out much of the random noise, reconstructing cleaner versions of the digits while preserving important structural features. This illustrates PCA’s effectiveness in noise reduction by retaining only the principal components that capture meaningful variance.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "unsupervised.html#k-means-clustering",
    "href": "unsupervised.html#k-means-clustering",
    "title": "11  Unsupervised Learning",
    "section": "11.2 K-Means Clustering",
    "text": "11.2 K-Means Clustering\nThis section is presented by ……\n\n11.2.1 Subsection",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "12  Advanced Topics",
    "section": "",
    "text": "12.1 Web Scraping\nThis section was written by Melanie Desroches, a senior majoring in statistics and minoring in computer science. The goal of this section is to introduce web-scraping so that it can be utilized for data science. This will include what web-scraping is, how to web-scrape with Python using examples, and how to web-scrape ethically.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "advanced.html#web-scraping",
    "href": "advanced.html#web-scraping",
    "title": "12  Advanced Topics",
    "section": "",
    "text": "12.1.1 What is Web-Scraping\nAs data scientists, we often want to collect data from a variety of sources. In the age of the internet, a lot of the data we may want to collect is available on a website. However, this data is often times not available in an easily downloadable format. This is where web-scraping becomes valuable. Web-scraping is an automated process used to gather data from websites. This allows us to access and collect large amounts of data directly from web pages if the information is not avalible for download.\nWebsites are primarily structured with HTML (Hypertext Markup Language), which organizes and displays content. Web scrapers parse through this HTML code to identify and extract relevant information. Therefore, it important to have a basic understanding of HTML in order to identify what part of the website you are trying to scrape. The contents of a web page are broken up and identified by elements. Here are some examples of common elements that are important for web-scraping:\n\n&lt;body&gt; : identifies the website body\n&lt;table&gt; : identifies a table\n&lt;tbody&gt; : identifies the body of the table\n&lt;tr&gt; : indentifies the row of a table\n\n\n\n12.1.2 How to Web-Scrape with Python\nThere are many ways to web-scrape with Python. We will cover the two main packages, Beautiful Soup and Selenium.\n\n12.1.2.1 Beautiful Soup\nThe Beautiful Soup Python Library simplifies the process of parsing and navigating HTML and XML documents, making it easier to extract data from websites. Beautiful Soup is ideal for scraping data from static websites. Static websites do not change based on user actions or require server-side interactions to update content dynamically. Basically, what you see is what you get. Static websites tend to be pretty simple so scraping from them is relatively easy.\nBeautiful Soup can be installed by running\npip install beautifulsoup4\nin your terminal.\n\n\n12.1.2.2 Selenium\nSelenium is used for web browser automation and dynamic websites. Dynamic sites often use backend programming to pull data from a database, customize it, and render it in real time based on user requests. This makes Selenium great at performing web-scraping tasks that involve multiple pages or performing actions within those pages. Because dynamic websites tend to be a bit more complex, you need to use a package like Selenium that is more equiped for the complex structure.\nSelenium can be installed by running\npip install selenium\nin your terminal.\n\n\n12.1.2.3 Web Driver\nTo control a web browser, Selenium also requires a WebDriver. We recommend Chrome Driver because it is cross-platform; follow the instructions for developers to set up your Chrome Driver.\nAfter setting up Chrome Driver, you can check its availability from a terminal:\nchromedriver --version\nA commonly seen error on Mac is\n\nError: “chromedriver” cannot be opened because the developer cannot be verified. Unable to launch the chrome browser\n\nThis can be fixed by running:\nxattr -d com.apple.quarantine $(which chromedriver)\nSee explanation from StackOverflow.\n\n\n12.1.2.4 Beautiful Soup vs Selenium\nBoth Beautiful Soup and Selenium are helpful tools in web-scraping. But they both have their strengths and weaknesses. Beautiful Soup is lightweight, easy to learn, and perfect for working with static HTML content. However, Beautiful Soup is more limited when it comes to dynamic websites, which are much more common nowadays. Selenium is better for interacting with dynamic web content that loads JavaScript or requires actions like clicking, scrolling, or filling forms. That said, Selenium can be slower and more resource-intensive since it opens a browser window to simulate real user actions.\n\n\n12.1.2.5 A Step-by Step Guide to Web-Scraping\n\nFind the website URL with the information you want to select\nSend an HTTP request to the URL and confirm you have access to the page. Generally, 200-299 means the request has been granted and 400-499 means that your request is not allowed.\nUse the “Inspect” tool in your browser to identify the tags, classes, or elements associated with the data you want to extract. This can be done by right-clicking on the web page and pressing select. If you hover your clicker over the different sections of HTML, the parts of the website that section is associated with will become highlighted. Use this to find the element that is associated with the data that you want to scrape.\nUse a parsing library like Beautiful Soup or Selenium to process the HTML response. Beautiful Soup requires the use of the requests package in order to send a request. Selenium uses the webdriver to send the request.\nClean and store the relevant infomation.\n\n\n\n\n12.1.3 Examples using NYC Open Data\nSince this class has used the NYC Open Data, let’s build on this data set in order to get some additional information that is not already available.\n\n12.1.3.1 Beautiful Soup and NYPD Precincts\nSay you want to get the adresses of all of the NYPD Precincts in New York City. This information is available in table format on the NYPD website. Since the NYPD Precincts aren’t changed, the website is static, making Beautiful Soup the best package to use to scrape this website.\nStart by making sure you have Beautiful Soup and Requests installed. The requests package can be installed using\npip install requests\nImport the requests package, BeautifulSoup from bs4, and pandas (to create a new data frame). We have already identified the url that will be scraping data from. In the code below, there is a dictionary called headers. This is optional. Headers can help make your requests look more like a browser. If you choose to use a header, include it when you send your request to the url. Otherwise, the request can be sent to the url using requests.get().\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# URL of the NYPD precincts page\nurl = \"https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page\"\n\n# Send a GET request to the page\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 Chrome/87.0\"\n}\nresponse = requests.get(url, headers=headers)\n\n# Check if the request was successful\nprint(response.status_code)\nif response.status_code != 200:\n    print(f\"Failed to retrieve page: Status code {response.status_code}\")\n\n200\n\n\nSince the response to the request was 200, which means the request was successful, we are clear to move onto the next step which is parsing the table.\nTo start parsing, you have to call Beautiful Soup. When you pass response.text into Beautiful Soup, it takesthe raw HTML of the webpage as a string. html.parser specifies the parsing engine used by Beautiful Soup to process the HTML. It is a built-in Python HTML parser that is fast and works well for most cases.\nTo identify which parts of the website you want to webscrape, you can right click on the website and click inspect. This will show you the HTML of the page. The table can be found under the &lt;table&gt; element with the class rt. Using this information, have Beautiful Soup find the table using .find(). Within the table, the rows are indentifies by &lt;tr&gt; within the HTML. In each row, the name of the precinct and address is found in the &lt;td&gt; element with the data labels Precinct and Address respectively. From this, use .find_all('tr') to find all the rows in the table and then within each row, extract the precinct and address.\n\n# Parse the HTML content\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find the table with class \"rt\" which holds the precinct data\ntable = soup.find(\"table\", {\"class\": \"rt\"})\n    \n# Lists to hold the extracted data\nprecinct_names = []\naddresses = []\n    \n# Extract each row of the table (each row corresponds to one precinct)\nfor row in table.find_all(\"tr\"):\n  # Find the \"Precinct\" and \"Address\" columns by data-label attribute\n  precinct_cell = row.find(\"td\", {\"data-label\": \"Precinct\"})\n  address_cell = row.find(\"td\", {\"data-label\": \"Address\"})\n        \n  # If both cells are found, store their text content\n  if precinct_cell and address_cell:\n    precinct_names.append(precinct_cell.get_text(strip=True))\n    addresses.append(address_cell.get_text(strip=True))\n\nThe extracted information can be stripped so that only the relevant text is included and then added to their relevant list. Now that the data has been collected and cleaned, a new dataframe can be created.\n\n# Create a DataFrame with the extracted data\nprecincts_df = pd.DataFrame({\n  \"Precinct\": precinct_names,\n  \"Address\": addresses\n})\n\n# Display the DataFrame\nprint(precincts_df)\n\n          Precinct                   Address\n0     1st Precinct         16 Ericsson Place\n1     5th Precinct       19 Elizabeth Street\n2     6th Precinct        233 West 10 Street\n3     7th Precinct        19 1/2 Pitt Street\n4     9th Precinct         321 East 5 Street\n..             ...                       ...\n72  115th Precinct  92-15 Northern Boulevard\n73  120th Precinct       78 Richmond Terrace\n74  121st Precinct       970 Richmond Avenue\n75  122nd Precinct      2320 Hylan Boulevard\n76  123rd Precinct           116 Main Street\n\n[77 rows x 2 columns]\n\n\n\n\n12.1.3.2 Selenium and Weather Data\nSay you want to see if the weather makes an impact of the number or severity of crashes in New York City. Weather data in New York City can be found on Wundergroud. Since information on weather is always being monitored and collected, the data that we want for a specific time period in being held in the websites database. Therefore, the website is dynamic and Selenium cna be used for web scraping.\nThe first step is to set up Selenium and the WebDriver. In this example, I use Chrome Driver. Options can be initialized with chrome_options = Options() for the Chrome browser. The options I used were --headless (which allows the browser to run without a visible window) and --disable-gpu (which can improve performance in headless mode).\n\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\n\n# Set up ChromeDriver\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  # Run in headless mode (no browser UI)\nchrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\nchrome_options.add_argument(\"--no-sandbox\")  # Required for some environments\n\nNext we need find the path of the Chrome Driver. The following code is a cross-platform solution.\n\n# Path to your ChromeDriver executable\n# config_file_path = \"config.txt\"\n# with open(config_file_path, 'r') as file:\n#         chrome_driver_path = file.read().strip()\n\nimport os\n\ndef find_application_path(app_name):\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        full_path = os.path.join(path, app_name)\n        if os.path.isfile(full_path) and os.access(full_path, os.X_OK):\n            return full_path\n    return None\n\nchrome_driver_path = find_application_path(\"chromedriver\")\n\nThe driver then can be initialized with the path and driver options.\n\nservice = Service(chrome_driver_path)\n\n# Initialize the ChromeDriver\ndriver = webdriver.Chrome(service=service, options=chrome_options)\n\nOnce Selenium and the webdriver is set up, go to the page and find the target data. Same as with the Beautiful Soup example, go to the url and identify the table that you want to webscrape. In this case, I want the table at the bottom of the page that lists the daily observations of the temperature, dew point, humidity, wind speed, pressure, and precipitation. The table is identified as &lt;table&gt; with the class &lt;days&gt;. In Selenium, driver.get(url) opens the webpage in the Edge browser. Once the table has loaded, (By.CSS_SELECTOR, \"table.days\") selects the main data table by its CSS selector “table.days”, ensuring we’re targeting the right element.\n\n# Define the target URL\nurl = f\"https://www.wunderground.com/history/weekly/us/ny/new-york-city/KLGA/date/2024-6-30\"\n\n# Load the page\ndriver.get(url)\n\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Wait for table to load\nwait = WebDriverWait(driver, 15)\ntable = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table.days\")))\n\nWithin the table, the rows are indentified by tr in tbodyand the columns are in td.\n\n# Initialize lists for each data type\ndates = []\nmax_temps = []\nmin_temps = []\nhumidity_values = []\nwind_speeds = []\npressure_values = []\nprecip_values = []\n\n# Get all rows\nrows = table.find_elements(By.CSS_SELECTOR, \"tbody tr\")\nfor row in rows:\n    # Get all 'td' elements in the row\n    columns = row.find_elements(By.TAG_NAME, \"td\")  \n    # Extract text from each column\n    row_data = [col.text.strip() for col in columns]  \n    # Print the content of the row\n    print(\"Row Data:\", row_data)  # This will print the content of each row\n\nRow Data: ['Jun\\n30\\n1\\n2\\n3\\n4\\n5\\n6', 'Jun', '30', '1', '2', '3', '4', '5', '6', 'Max Avg Min\\n101 79.7 73\\n79 72.7 65\\n83 75.1 67\\n83 75.8 68\\n85 77.3 72\\n90 81.0 74\\n92 78.7 72', 'Max', 'Avg', 'Min', '101', '79.7', '73', '79', '72.7', '65', '83', '75.1', '67', '83', '75.8', '68', '85', '77.3', '72', '90', '81.0', '74', '92', '78.7', '72', 'Max Avg Min\\n74 70.3 59\\n56 53.1 46\\n56 49.5 45\\n58 51.5 47\\n69 63.0 52\\n73 70.8 69\\n73 70.9 69', 'Max', 'Avg', 'Min', '74', '70.3', '59', '56', '53.1', '46', '56', '49.5', '45', '58', '51.5', '47', '69', '63.0', '52', '73', '70.8', '69', '73', '70.9', '69', 'Max Avg Min\\n91 74.9 36\\n65 50.9 38\\n55 41.3 29\\n70 43.9 28\\n85 61.8 48\\n87 72.1 55\\n91 78.0 54', 'Max', 'Avg', 'Min', '91', '74.9', '36', '65', '50.9', '38', '55', '41.3', '29', '70', '43.9', '28', '85', '61.8', '48', '87', '72.1', '55', '91', '78.0', '54', 'Max Avg Min\\n20 10.2 5\\n23 14.5 8\\n12 8.7 6\\n18 10.3 0\\n15 9.3 3\\n14 7.4 0\\n17 7.6 0', 'Max', 'Avg', 'Min', '20', '10.2', '5', '23', '14.5', '8', '12', '8.7', '6', '18', '10.3', '0', '15', '9.3', '3', '14', '7.4', '0', '17', '7.6', '0', 'Max Avg Min\\n30.0 29.9 29.8\\n30.1 30.0 29.9\\n30.2 30.2 30.1\\n30.2 30.1 30.0\\n30.0 29.9 29.8\\n29.8 29.8 29.7\\n29.9 29.8 29.8', 'Max', 'Avg', 'Min', '30.0', '29.9', '29.8', '30.1', '30.0', '29.9', '30.2', '30.2', '30.1', '30.2', '30.1', '30.0', '30.0', '29.9', '29.8', '29.8', '29.8', '29.7', '29.9', '29.8', '29.8', 'Total\\n0.06\\n0.11\\n0.00\\n0.00\\n0.00\\n0.03\\n0.11', 'Total', '0.06', '0.11', '0.00', '0.00', '0.00', '0.03', '0.11']\nRow Data: ['Jun']\nRow Data: ['30']\nRow Data: ['1']\nRow Data: ['2']\nRow Data: ['3']\nRow Data: ['4']\nRow Data: ['5']\nRow Data: ['6']\nRow Data: ['Max', 'Avg', 'Min']\nRow Data: ['101', '79.7', '73']\nRow Data: ['79', '72.7', '65']\nRow Data: ['83', '75.1', '67']\nRow Data: ['83', '75.8', '68']\nRow Data: ['85', '77.3', '72']\nRow Data: ['90', '81.0', '74']\nRow Data: ['92', '78.7', '72']\nRow Data: ['Max', 'Avg', 'Min']\nRow Data: ['74', '70.3', '59']\nRow Data: ['56', '53.1', '46']\nRow Data: ['56', '49.5', '45']\nRow Data: ['58', '51.5', '47']\nRow Data: ['69', '63.0', '52']\nRow Data: ['73', '70.8', '69']\nRow Data: ['73', '70.9', '69']\nRow Data: ['Max', 'Avg', 'Min']\nRow Data: ['91', '74.9', '36']\nRow Data: ['65', '50.9', '38']\nRow Data: ['55', '41.3', '29']\nRow Data: ['70', '43.9', '28']\nRow Data: ['85', '61.8', '48']\nRow Data: ['87', '72.1', '55']\nRow Data: ['91', '78.0', '54']\nRow Data: ['Max', 'Avg', 'Min']\nRow Data: ['20', '10.2', '5']\nRow Data: ['23', '14.5', '8']\nRow Data: ['12', '8.7', '6']\nRow Data: ['18', '10.3', '0']\nRow Data: ['15', '9.3', '3']\nRow Data: ['14', '7.4', '0']\nRow Data: ['17', '7.6', '0']\nRow Data: ['Max', 'Avg', 'Min']\nRow Data: ['30.0', '29.9', '29.8']\nRow Data: ['30.1', '30.0', '29.9']\nRow Data: ['30.2', '30.2', '30.1']\nRow Data: ['30.2', '30.1', '30.0']\nRow Data: ['30.0', '29.9', '29.8']\nRow Data: ['29.8', '29.8', '29.7']\nRow Data: ['29.9', '29.8', '29.8']\nRow Data: ['Total']\nRow Data: ['0.06']\nRow Data: ['0.11']\nRow Data: ['0.00']\nRow Data: ['0.00']\nRow Data: ['0.00']\nRow Data: ['0.03']\nRow Data: ['0.11']\n\n\nAs you can see the output is pretty messy. From this step, we need to find the important parts and strip it of the text. This can be done by identifying the indicies of the rows that we want, using find_elements to find the corresponding tag, and then stripping the text to add it to the relevant list.\n\n# Process the first row which contains all the dates\ndate_row = rows[0].text.split('\\n')\ndates = [date for date in date_row if date.isdigit()][:7]  # Get first 7 dates\n\n# Find temperature values (rows 10-16 contain the actual temperature data)\ntemp_rows = rows[10:17]  # Get rows 10-16\nfor row in temp_rows:\n  cells = row.find_elements(By.TAG_NAME, \"td\")\n  if len(cells) &gt;= 3:\n    max_temps.append(cells[0].text.strip())\n    min_temps.append(cells[2].text.strip())\n\n# Find humidity values (rows 18-24)\nhumidity_rows = rows[18:25]\nfor row in humidity_rows:\n  cells = row.find_elements(By.TAG_NAME, \"td\")\n  if len(cells) &gt;= 2:\n    humidity_values.append(cells[1].text.strip())\n\n# Find wind speed values (rows 26-32)\nwind_rows = rows[26:33]\nfor row in wind_rows:\n  cells = row.find_elements(By.TAG_NAME, \"td\")\n  if len(cells) &gt;= 1:\n    wind_speeds.append(cells[0].text.strip())\n\n# Find pressure values (rows 42-48)\npressure_rows = rows[42:49]\nfor row in pressure_rows:\n  cells = row.find_elements(By.TAG_NAME, \"td\")\n  if len(cells) &gt;= 1:\n    pressure_values.append(cells[0].text.strip())\n\n# Find precipitation values (rows 50-56)\nprecip_rows = rows[50:57]\nfor row in precip_rows:\n  cells = row.find_elements(By.TAG_NAME, \"td\")\n  if len(cells) &gt;= 1:\n    precip_values.append(cells[0].text.strip())\n\nOnce all the relevant data has been collected and cleaned, it can be added to a new dataframe.\n\nimport pandas as pd\n\n# Create DataFrame\nweather_data = pd.DataFrame({\n    'Date': dates,\n    'Max Temperature (°F)': max_temps,\n    'Min Temperature (°F)': min_temps,\n    'Humidity (%)': humidity_values,\n    'Wind Speed (mph)': wind_speeds,\n    'Pressure (in)': pressure_values,\n    'Precipitation (in)': precip_values\n})\n\nprint(weather_data)\ndriver.quit()\n\n  Date Max Temperature (°F) Min Temperature (°F) Humidity (%)  \\\n0   30                  101                   73         70.3   \n1    1                   79                   65         53.1   \n2    2                   83                   67         49.5   \n3    3                   83                   68         51.5   \n4    4                   85                   72         63.0   \n5    5                   90                   74         70.8   \n6    6                   92                   72         70.9   \n\n  Wind Speed (mph) Pressure (in) Precipitation (in)  \n0               91          30.0               0.06  \n1               65          30.1               0.11  \n2               55          30.2               0.00  \n3               70          30.2               0.00  \n4               85          30.0               0.00  \n5               87          29.8               0.03  \n6               91          29.9               0.11  \n\n\nLastly, driver.quit() closes the browser.\n\n\n\n12.1.4 A Note on Data Ethics\nWhile web scraping is not explicitly illegal, it can get you in hot water if you are not careful. Web scraping is a powerful tool and it should be treated as such. Just because you can web scrape doesn’t always mean you should.\n\n12.1.4.1 Why Web-Scraping can be un-ethical\nThere are several reasons that web scraping may be deemed unethical.\n\nThe website you are trying to web scrape may not allow it.\nThe information being scraped is considered private information or intellectual property.\nSending too many requests at once can overwhelm the server and crash the website.\n\n\n\n12.1.4.2 Some Tips to Help You Scrape Ethically\n\nYou can check if a website allows web scraping in either the terms of use section of the website or by checking the websites .robots.txt to see who is allowed to use the website and what parts are available for scraping.\nAlways be mindful of what kind of information you are trying to collect and if it is private information/intellectual property\nNever scrape from a website that requires login or payment\nSpread out the time of the requests in order to prevent the website from crashing. If using Selenium, use WebDriverWait from selenium.webdriver.support.ui to wait for the page to load. Otherwise, use the time package to space out the requests.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "advanced.html#reinforcement-learning",
    "href": "advanced.html#reinforcement-learning",
    "title": "12  Advanced Topics",
    "section": "12.2 Reinforcement Learning",
    "text": "12.2 Reinforcement Learning\nThis section was written by Qianruo Tan.\n\n12.2.1 Introduction\n\nReinforcement learning (RL) is a type of machine learning.\nDeep Q-Networks (DQN)\nAgents learn by interacting with the environment.\nThe goal of it is to maximize cumulative reward over time.\nReinforcement Learning (RL) VS Supervised Learning (SL)\n\nSupervised Learning (SL) and Reinforcement Learning (RL) share some similarities in their frameworks, but they differ fundamentally in how they learn and improve over time. In a supervised learning setting, training a neural network, such as learning to play a game, requires a substantial dataset with labeled examples. This involves recording the gameplay process, including inputs like key presses and even eye movements.\nThe network learns by observing how certain actions (inputs) lead to specific outcomes (labels), essentially mimicking human actions to predict what to do next. However, it only imitates what it’s been taught and does not learn to improve on its own beyond the data it’s given.\nIn contrast, reinforcement learning does not rely on a predefined dataset with target labels. Instead, it uses a policy network that transforms input frames from the game into actions. The training process in RL starts with a network that has no prior knowledge. It receives input frames directly from the game environment and must figure out what actions to take to maximize rewards over time. The simplest way to train this policy network is through policy gradients, where the model learns by interacting with the environment and receiving feedback.\nHowever, RL has a downside: if the model encounters a failure, it might incorrectly generalize that a certain strategy is bad, leading to what’s known as the credit assignment problem—it struggles to properly attribute which actions led to success or failure. This means the network may become overly cautious, reducing its exploration of potentially good strategies that initially resulted in failure.\n\n\n12.2.2 Actual usages & Scopes & Limitations\n\n12.2.2.1 Actual usages\n\nGaming and Simulations (AlphaGo)\nCooperate with Bayesian Optimization\nRobotics and Automation\nSelf-driving Cars\nFinance and Trading\nPersonalization and Recommendations\n\n\n\n12.2.2.2 Scopes & Limitations\n\nReinforcement Learning vs. Evolutionary Methods\n\nEvolutionary methods cannot utilize real-time feedback from actions, making them less efficient in dynamic environments where immediate learning is advantageous.\n\nPolicy Gradient Methods\n\nUnlike evolutionary methods, policy gradients interact with the environment to improve performance, allowing more efficient use of detailed feedback from individual interactions.\n\nMisunderstanding of Optimization\n\nOptimization in RL is about improving performance incrementally, not guaranteeing the best possible outcome in every scenario.\n\n\n\n12.2.3 Q-Learning\n\n12.2.3.1 Q-learning Overview\n\nAims to learn the optimal action-value function \\(( q_*(s, a) )\\), regardless of the policy being followed during learning.\nThe main objective is to approximate this function through a learning process where the agent interacts with its environment, receiving rewards and updating its knowledge of state-action pairs.\n\n\n\n12.2.3.2 Q-learning Update Rule\nThe core formula of Q-learning is based on the Bellman equation for the action-value function: \\[ [Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]], \\] where - \\(( Q(s, a) )\\): The current Q-value for taking action \\(( a )\\) in state \\(( s )\\).\n\n\\(( \\alpha )\\): The learning rate, set to 0.1 in your code, which controls how much new information overrides the old information.\n\\(( R )\\): The reward received after taking action \\(( a )\\) in state \\(( s )\\).\n\\(( \\gamma )\\): The discount factor, set to 0.9 in your code, which determines the importance of future rewards.\n\\(( \\max_{a'} Q(s', a') )\\): The maximum Q-value for the next state \\(( s' )\\) across all possible actions \\(( a' )\\). This term represents the best possible future reward from the next state.\n\\(( Q(s, a) )\\) (the initial term on the right side): The old Q-value for the current state-action pair, which is being updated.\n\n\n\n12.2.3.3 Key Components of Q-learning\nOff-policy Learning:\n\nQ-learning is an off-policy method, meaning the agent can learn the optimal policy independently. Used to select actions during training (e.g., \\(( \\epsilon )\\)-greedy strategy).\n\n\n\n12.2.3.4 Common supporting tools\nEnvironment and State Management:\nFunctions get_next_state() and get_reward()\nQ-Table Initialization:\nq_table = np.zeros((grid_size, grid_size, 4)) \n\n\n12.2.3.5 Common supporting tools\nNeural Network: A computational model inspired by the human brain, consisting of layers of interconnected nodes (neurons). It learns to recognize patterns in data by adjusting weights through multiple training iterations.\nDeep Q-Learning (DQN): An algorithm that combines Q-Learning with deep neural networks to approximate the Q-value function, allowing the agent to choose optimal actions in complex environments with high-dimensional state spaces.\nPolicy Network: A neural network designed to output the best action to take in a given state. It maps input states directly to actions, enabling the agent to decide what to do without relying on a value function.\nPolicy Gradient: An optimization technique in reinforcement learning where the agent learns the best policy by directly adjusting its parameters to maximize the cumulative reward, rather than estimating value functions.\n\n\n12.2.3.6 Common supporting tools\nBehavior Policy: The strategy that the agent uses to explore the environment and collect experiences. It often includes some randomness to encourage exploration of different actions.\nTarget Policy: The policy that the agent is trying to optimize. In some algorithms, like Q-learning, it is used to determine the best action to take based on learned values.\nEpsilon-Greedy Policy: A strategy used to balance exploration and exploitation. With a small probability (epsilon), the agent chooses a random action to explore, and with the remaining probability (1 - epsilon), it chooses the best-known action based on current knowledge.\n\n\n\n12.2.4 An Example\n\n12.2.4.1 Grid\nThe environment is a 4×4 grid. The agent starts in (0,0) and aims to reach (3,3).\n\n\n12.2.4.2 Environment Setup\nThe following chunk is used to set up the environment, which the agent is in. I also set up a seed, for the reproduction.\n\n## Import packages\nimport numpy as np\nimport random\n\n## Set the random seed for reproducibility\nrandom.seed(3255)\nnp.random.seed(3255)\n\n## Define the environment\ngrid_size = 4\nstart_state = (0, 0)\ngoal_state = (3, 3)\nobstacles = []\n\n\n\n12.2.4.3 Hyperparameters\nThe following chunk is used to set key reinforcement learning hyperparameters, including the learning rate, discount factor, exploration rate, and the number of training episodes.\n\n## Define the hyperparameters for our value function\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 0.2  # Exploration rate\nnum_episodes = 1000  # Try 1000 times\n\n\n\n12.2.4.4 Q-table Initialization\nThe following chunk initializes a Q-table with zeros to store the value estimates for each state-action pair in a grid, and defines the available actions (up, down, left, right) for the agent to choose from.\n\n## Initialize the Q-table\nq_table = np.zeros((grid_size, grid_size, 4))  \n## 4 possible actions: up, down, left, right\n## The output Q-table follows thie 4 directions\n\n## Define the action space\nactions = [\"up\", \"down\", \"left\", \"right\"]\n\n\n\n12.2.4.5 State Transitions and Reward Calculation\nThe following chunk defines functions to determine the agent’s next state based on an action (preventing it from leaving the grid boundaries) and to assign a numerical reward depending on whether the state is a goal, an obstacle, or a general location.\n\ndef get_next_state(state, action):\n    i, j = state\n    if action == \"up\":\n        return (max(i - 1, 0), j)\n    elif action == \"down\":\n        return (min(i + 1, grid_size - 1), j)\n    elif action == \"left\":\n        return (i, max(j - 1, 0))\n    elif action == \"right\":\n        return (i, min(j + 1, grid_size - 1))\n    \ndef get_reward(state):\n    if state == goal_state:\n        return 10\n    elif state in obstacles:\n        return -5\n    else:\n        return -1\n\nMove up: decrease row index (i) by 1, but don’t go out of bounds (minimum row index is 0) Move down: increase row index (i) by 1, but don’t go out of bounds (maximum row index is grid_size - 1) Move left: decrease column index (j) by 1, but don’t go out of bounds (minimum column index is 0) Move right: increase column index (j) by 1, but don’t go out of bounds (maximum column index is grid_size - 1)\n\n\n12.2.4.6 Action Selection (Epsilon-Greedy Policy)\nThe following chunk implements an epsilon-greedy strategy for action selection, randomly exploring with probability epsilon or exploiting the action with the highest Q-value otherwise.\n\ndef choose_action(state):\n    ## Epsilon-greedy action selection\n    if random.uniform(0, 1) &lt; epsilon:\n        return random.choice(actions)  # Explore\n    else:\n        ## Exploit (choose the action with the highest Q-value)\n        state_q_values = q_table[state[0], state[1], :]\n        return actions[np.argmax(state_q_values)]\n\nstate represents the agent’s current position on the grid (given as a tuple (i, j)). And there are 4 x 4 = 16 possibilities. The function uses this state to decide whether to explore (choose a random action) or exploit (choose the best-known action based on the Q-table). The Q-values for that particular state are retrieved using q_table [state[0], state[1], :].\n\n\n12.2.4.7 Q-L Algorithm (Main Loop)\nThe following chunk runs the Q-learning algorithm over multiple episodes, choosing actions based on an epsilon-greedy policy, observing the resulting reward and next state, then updating the Q-values using the Q-learning equation until it reaches the goal state.\n\n## Q-learning Algorithm\nfor episode in range(num_episodes):\n    state = start_state\n    while state != goal_state:\n        action = choose_action(state)\n        next_state = get_next_state(state, action)\n        \n        ## Get the reward for moving to the next state\n        reward = get_reward(next_state)\n        \n        ## Update Q-value using the Q-learning formula\n        current_q_value = q_table[\n            state[0], state[1], actions.index(action)\n            ]\n        max_future_q_value = np.max(\n            q_table[next_state[0], next_state[1], :]\n            )\n        new_q_value = current_q_value + alpha * (\n            reward + gamma * max_future_q_value - current_q_value\n            )\n        q_table[state[0], state[1], actions.index(action)] = new_q_value\n        \n        state = next_state\n\nstate is initially set to start_state (e.g., (0, 0)). Within the loop, the choose_action(state) function is called to select an action based on the agent’s current position. The agent then moves to a new next_state using the chosen action, and the current state is updated to next_state. Throughout this loop, state always represents the agent’s current position on the grid as it progresses toward the goal_state.\n\n\n12.2.4.8 Result\nThe following chunk prints out the final learned Q-values, showing the agent’s estimated values for each state-action pair after training.\n\n## Display the learned Q-values\nprint(\"Learned Q-Table:\")\nprint(q_table)\n\nLearned Q-Table:\n[[[ 0.61922635  1.7634678   0.61163903  1.8098    ]\n  [ 1.7988677   3.122       0.60561032  3.11191456]\n  [ 1.21191045  4.57986592  0.81220571  0.9359679 ]\n  [ 0.60944922  5.3724181  -0.53983336  0.25476897]]\n\n [[-0.44402797 -0.38478777  0.66294145  3.12143333]\n  [ 1.78608503  4.53127148  1.7430146   4.58      ]\n  [ 3.07526195  6.2         3.0995568   6.18414026]\n  [ 1.73344552  7.99997146  3.22568878  2.73360563]]\n\n [[-0.45222003 -0.77836467 -0.67934652  3.05464906]\n  [ 1.92390397  0.26491207  0.18598014  6.19978945]\n  [ 4.55721914  7.8668745   4.54619388  8.        ]\n  [ 6.16475403 10.          6.18052412  7.95907134]]\n\n [[-0.42453346 -0.3940399  -0.3940399  -0.04672549]\n  [ 0.50001115 -0.12967628 -0.2248309   4.06371215]\n  [-0.1         3.00770286  0.          9.98689979]\n  [ 0.          0.          0.          0.        ]]]\n\n\nThis is the directly output of this example, there are three layers of bracket, each of them have different meanings. First layer of brackets: Represents the rows of the grid. Each layer represents a row in the Q-table(i.e., a row position in the qrid environment). Second layer of brackets: Represents the columns of the grid. Each subarray represents a specificstate in that row (i.e., a specific position in the qrid, such as (0,0), (1,1), etc.). Third layer of brackets: Represents the Q-values for each action in that state. Each elementrepresents the Q-value of a specific action in that state (e.g. the four actions: up, down, left, right)\n\n\n12.2.4.9 Route visualization\nThe following chunk visualizes the learned policy on a grid by drawing arrows in each cell to indicate the best action according to the Q-table, while highlighting the start, goal, and obstacle positions.\n\nimport matplotlib.pyplot as plt\n\n# Define directional arrows for actions: up, down, left, right\ndirections = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n\n# Visualization of the best actions on the grid\nfig, ax = plt.subplots(figsize=(6, 6))\nax.set_xticks(np.arange(0.5, grid_size, 1))\nax.set_yticks(np.arange(0.5, grid_size, 1))\nax.grid(True, which='both')\nax.set_xticklabels([])\nax.set_yticklabels([])\n\n# Highlight Start, Goal, and Obstacles\nax.add_patch(\n    plt.Rectangle(\n        start_state, 1, 1, fill=True, color='yellow', alpha=0.3\n        )\n    )\nax.add_patch(\n    plt.Rectangle(\n        goal_state, 1, 1, fill=True, color='green', alpha=0.3\n        )\n    )\n\n# Highlight obstacles in red\nfor obstacle in obstacles:\n    ax.add_patch(\n        plt.Rectangle(\n            obstacle, 1, 1, fill=True, color='red', alpha=0.5\n            )\n        )\n\n# Add arrows (text-based) for the best actions\nfor i in range(grid_size):\n    for j in range(grid_size):\n        if (i, j) in obstacles:\n            continue  # Skip drawing arrows for obstacle locations\n        q_values = q_table[i, j, :]\n        best_action_idx = np.argmax(q_values)\n        best_action = directions[best_action_idx]  \n        # Text-based direction arrows (↑, ↓, ←, →)\n\n        # Adding text-based arrows at the grid cells\n        ax.text(\n            j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20\n            )\n\n# Draw grid lines and adjust axis\nplt.xlim(0, grid_size)\nplt.ylim(0, grid_size)\nplt.gca().invert_yaxis()  \n# Invert Y-axis to display it in the correct orientation\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12.2.4.10 Grid with obstacles\nBecause of the reusing and stating cause the output become exactly the same. I rewrote the example with obstacles.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n## Set random seed for reproducibility\nrandom.seed(3255)\nnp.random.seed(3255)\n\n## Define the environment\ngrid_size = 4\nstart_state = (0, 0)\ngoal_state = (3, 3)\nobstacles = [(1, 1), (2, 2)]  # Ensure obstacles are unique\n\n## Define directions for visualization\ndirections = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n\n\n\n12.2.4.11 Run the grid with obstacles\nThe following defines reward and next-state functions that incorporate obstacles and goals, and then visualizes the learned Q-values on a grid by marking start, goal, and obstacle cells, as well as displaying the agent’s best action in each non-obstacle cell using directional arrows.\n\n## Function to get reward\ndef get_reward(state):\n    if state == goal_state:\n        return 10\n    elif state in obstacles:\n        return -5\n    else:\n        return -1\n\n## Function to get the next state based on the action\ndef get_next_state(state, action):\n    i, j = state\n    if action == \"up\":\n        next_state = (max(i - 1, 0), j)\n    elif action == \"down\":\n        next_state = (min(i + 1, grid_size - 1), j)\n    elif action == \"left\":\n        next_state = (i, max(j - 1, 0))\n    elif action == \"right\":\n        next_state = (i, min(j + 1, grid_size - 1))\n    \n    ## Prevent moving into obstacles\n    if next_state in obstacles:\n        return state  \n        # Stay in the same position if the next state is an obstacle\n    else:\n        return next_state\n\n## Visualization function\ndef plot_grid_with_obstacles():\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.clear()  # Clear the plot to avoid overlap\n    ax.set_xticks(np.arange(0.5, grid_size, 1))\n    ax.set_yticks(np.arange(0.5, grid_size, 1))\n    ax.grid(True, which='both')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n\n    ## Highlight Start, Goal, and Obstacles\n    ax.add_patch(\n        plt.Rectangle(\n            start_state, 1, 1, fill=True, color='yellow', alpha=0.3\n            )\n        )\n    ax.add_patch(\n        plt.Rectangle(\n            goal_state, 1, 1, fill=True, color='green', alpha=0.3\n            )\n        )\n\n    ## Highlight obstacles in red\n    for obstacle in set(obstacles):  # Use a set to ensure uniqueness\n        ax.add_patch(\n            plt.Rectangle(\n                obstacle, 1, 1, fill=True, color='red', alpha=0.5\n                )\n            )\n\n    ## Add arrows for the best actions from Q-table\n    for i in range(grid_size):\n        for j in range(grid_size):\n            if (i, j) in obstacles:\n                continue  # Skip arrows for obstacle locations\n            q_values = q_table[i, j, :]\n            best_action_idx = np.argmax(q_values)\n            best_action = directions[best_action_idx]  \n            # Directional arrows (↑, ↓, ←, →)\n            ax.text(\n                j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20\n                )\n\n    ## Draw grid lines and adjust axis\n    plt.xlim(0, grid_size)\n    plt.ylim(0, grid_size)\n    plt.gca().invert_yaxis()  \n    # Invert Y-axis for correct orientation\n    plt.show()\n\n## Call the visualization function\nplot_grid_with_obstacles()\n\n\n\n\n\n\n\n\n\n\n\n12.2.5 Rewards and Penalties\nThe following chunk trains a Q-learning agent over multiple episodes, updating Q-values as it navigates from a start state to a goal state, stores the cumulative reward obtained in each episode, and finally plots how the total earned reward per episode evolves over time.\n## Initialize list to store cumulative rewards for each episode\ncumulative_rewards = []\n\nfor episode in range(num_episodes):\n    state = start_state\n    episode_reward = 0  # Track total reward for the current episode\n    \n    while state != goal_state:\n        action = choose_action(state)\n        next_state = get_next_state(state, action)\n        \n        reward = get_reward(next_state)\n        episode_reward += reward  # Accumulate reward for this episode\n        \n        # Update Q-value\n        current_q_value = q_table[\n            state[0], state[1], actions.index(action)\n            ]\n        max_future_q_value = np.max(\n            q_table[next_state[0], next_state[1], :]\n            )\n        new_q_value = current_q_value + alpha * \n                        (\n                            reward + gamma * max_future_q_value - current_q_value\n                            )\n        q_table[state[0], state[1], actions.index(action)] = new_q_value\n        \n        state = next_state  # Move to the next state\n    \n    cumulative_rewards.append(episode_reward)  \n    # Store cumulative reward for this episode\n\n## Visualization of Cumulative Rewards\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(num_episodes), cumulative_rewards, \n            label='Cumulative Reward per Episode')\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Cumulative Reward\")\nplt.title(\"Cumulative Rewards and Penalties Over Episodes\")\nplt.legend()\nplt.show()\n\n12.2.5.1 Track Cumulative Rewards Over Episodes\nThe following chunk trains a Q-learning agent by repeatedly choosing actions, updating its Q-values based on observed rewards, recording total rewards per episode, and then visualizes how cumulative rewards evolve over the episodes.\n\n## Initialize list to store cumulative rewards for each episode\ncumulative_rewards = []\n\nfor episode in range(num_episodes):\n    state = start_state\n    episode_reward = 0  # Track total reward for the current episode\n    \n    while state != goal_state:\n        action = choose_action(state)\n        next_state = get_next_state(state, action)\n        \n        reward = get_reward(next_state)\n        episode_reward += reward  # Accumulate reward for this episode\n        \n        # Update Q-value\n        current_q_value = q_table[\n            state[0], state[1], actions.index(action)\n            ]\n        max_future_q_value = np.max(\n            q_table[next_state[0], next_state[1], :]\n            )\n        new_q_value = current_q_value + alpha * (\n            reward + gamma * max_future_q_value - current_q_value\n            )\n        q_table[state[0], state[1], actions.index(action)] = new_q_value\n        \n        state = next_state  # Move to the next state\n    \n    cumulative_rewards.append(episode_reward)  \n    # Store cumulative reward for this episode\n\n## Visualization of Cumulative Rewards\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(\n    range(num_episodes), cumulative_rewards, label='Cumulative Reward per Episode'\n    )\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Cumulative Reward\")\nplt.title(\"Cumulative Rewards and Penalties Over Episodes\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "13  Exercises",
    "section": "",
    "text": "Quarto and Git setup Quarto and Git are two important tools for data science. Get familiar with them through the following tasks. Please use the templates/hw.qmd template.\n\nInstall Quarto onto your computer following the instructions of Get Started. Document the obstacles you encountered and how you overcame them.\nPick a tool of your choice (e.g., VS Code, Jupyter Notebook, Emacs, etc.), follow the instructions to reproduce the example of line plot on polar axis.\nRender the homework into a pdf file and put the file into a release in your GitHub repo. Document any obstacles you have and how you overcome them.\n\nGit basics and GitHub setup Learn the Git basics and set up an account on GitHub if you do not already have one. Practice the tips on Git in the notes. By going through the following tasks, ensure your repo has at least 10 commits, each with an informative message. Regularly check the status of your repo using git status. The specific tasks are:\n\nClone the class notes repo to an appropriate folder on your computer.\nAdd all the files to your designated homework repo from GitHub Classroom and work on that repo for the rest of the problem.\nAdd your name and wishes to the Wishlist; commit.\nRemove the Last, First entry from the list; commit.\nCreate a new file called add.qmd containing a few lines of texts; commit.\nRemove add.qmd (pretending that this is by accident); commit.\nRecover the accidentally removed file add.qmd; add a long line (a paragraph without a hard break); add a short line (under 80 characters); commit.\nChange one word in the long line and one word in the short line; use git diff to see the difference from the last commit; commit.\nPlay with other git operations and commit.\n\nContributing to the Class Notes To contribute to the classnotes, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file as if you are explaining them to someone who want to contribute too.\n\nCreate a fork of the notes repo into your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer; document the obstacles and solutions.\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; commit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes.\n\nMonty Hall Consider a generalized Monty Hall experiment. Suppose that the game start with \\(n\\) doors; after you pick one, the host opens \\(m \\le n - 2\\) doors, that show no award. Include sufficient text around the code chunks to explain them.\n\nWrite a function to simulate the experiment once. The function takes two arguments ndoors and nempty, which represent the number of doors and the number of empty doors showed by the host, respectively, It returns the result of two strategies, switch and no-switch, from playing this game.\nPlay this game with 3 doors and 1 empty a few times.\nPlay this game with 10 doors and 8 empty a few times.\nWrite a function to play this game ntrial times and return the proportion of wins for both strategies.\nApply your function to play this game 1000 times, with 3 doors and 10 doors, and summarize your results.\nWrite a function to demonstrate the Monty Hall problem through simulation. The function takes two arguments ndoors and ntrials, representing the number of doors in the experiment and the number of trials in a simulation, respectively. The function should return the proportion of wins for both the switch and no-switch strategy.\nApply your function with 3 doors and 5 doors, both with 1000 trials. Summarize your results.\n\nApproximating \\(\\pi\\) Write a function to do a Monte Carlo approximation of \\(\\pi\\). The function takes a Monte Carlo sample size n as input, and returns a point estimate of \\(\\pi\\) and a 95% confidence interval. Apply your function with sample size 1000, 2000, 4000, and 8000. Repeat the experiment 1000 times for each sample size and check the empirical probability that the confidence intervals cover the true value of \\(\\pi\\). Comment on the results.\nGoogle Billboard Ad Find the first 10-digit prime number occurring in consecutive digits of \\(e\\). This was a Google recruiting ad.\nGame 24 The math game 24 is one of the addictive games among number lovers. With four randomly selected cards form a deck of poker cards, use all four values and elementary arithmetic operations (\\(+-\\times /\\)) to come up with 24. Let \\(\\square\\) be one of the four numbers. Let \\(\\bigcirc\\) represent one of the four operators. For example, \\[\\begin{equation*}\n(\\square \\bigcirc \\square) \\bigcirc (\\square \\bigcirc \\square)\n\\end{equation*}\\] is one way to group the the operations.\n\nList all the possible ways to group the four numbers.\nHow many possible ways are there to check for a solution?\nWrite a function to solve the problem in a brutal force way. The inputs of the function are four numbers. The function returns a list of solutions. Some of the solutions will be equivalent, but let us not worry about that for now.\n\nNYC Crash Data Cleaning The NYC motor vehicle collisions data with documentation is available from NYC Open Data. The raw data needs some cleaning.\n\nUse the filter from the website to download the crash data of the week of June 30, 2024 in CSV format; save it under a directory data with an informative name (e.g., nyccrashes_2024w0630_by20240916.csv); read the data into a Panda data frame with careful handling of the date time variables.\nClean up the variable names. Use lower cases and replace spaces with underscores.\nGet the basic summaries of each variables: missing percentage; descriptive statistics for continuous variables; frequency tables for discrete variables.\nAre their invalid longitude and latitude in the data? If so, replace them with NA.\nAre there zip_code values that are not legit NYC zip codes? If so, replace them with NA.\nAre there missing in zip_code and borough? Do they always co-occur?\nAre there cases where zip_code and borough are missing but the geo codes are not missing? If so, fill in zip_code and borough using the geo codes.\nIs it redundant to keep both location and the longitude/latitude at the NYC Open Data server?\nCheck the frequency of crash_time by hour. Is there a matter of bad luck at exactly midnight? How would you interpret this?\nAre the number of persons killed/injured the summation of the numbers of pedestrians, cyclist, and motorists killed/injured? If so, is it redundant to keep these two columns at the NYC Open Data server?\nPrint the whole frequency table of contributing_factor_vehicle_1. Convert lower cases to uppercases and check the frequencies again.\nProvided an opportunity to meet the data provider, what suggestions would you make based on your data exploration experience?\n\nNYC Crash Data Exploration Except for the first question, use the cleaned crash data in feather format.\n\nConstruct a contigency table for missing in geocode (latitude and longitude) by borough. Is the missing pattern the same across boroughs? Formulate a hypothesis and test it.\nConstruct a hour variable with integer values from 0 to 23. Plot the histogram of the number of crashes by hour. Plot it by borough.\nOverlay the locations of the crashes on a map of NYC. The map could be a static map or Google map.\nCreate a new variable severe which is one if the number of persons injured or deaths is 1 or more; and zero otherwise. Construct a cross table for severe versus borough. Is the severity of the crashes the same across boroughs? Test the null hypothesis that the two variables are not associated with an appropriate test.\nMerge the crash data with the zip code database.\nFit a logistic model with severe as the outcome variable and covariates that are available in the data or can be engineered from the data. For example, zip code level covariates can be obtained by merging with the zip code database; crash hour; number of vehicles involved.\n\nNYC Crash severity modeling Using the cleaned NYC crash data, merged with zipcode level information, predict severe of a crash.\n\nSet random seed to 1234. Randomly select 20% of the crashes as testing data and leave the rest 80% as training data.\nFit a logistic model on the training data and validate the performance on the testing data. Explain the confusion matrix result from the testing data. Compute the F1 score.\nFit a logistic model on the training data with \\(L_1\\) regularization. Select the tuning parameter with 5-fold cross-validation in F1 score\nApply the regularized logistic regression to predict the severity of the crashes in the testing data. Compare the performance of the two logistic models in terms of accuracy, precision, recall, F1-score, and AUC.\n\nMidterm project: Noise complaints in NYC The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with requests to NYPD on noise complaints that are created between 00:00:00 06/30/2024 and 24:00:00 07/06/2024. The subset is available in CSV format as data/nypd311w063024noise_by100724.csv. Read the data dictionary online to understand the meaning of the variables.\n\nData cleaning.\n\nImport the data, rename the columns with our preferred styles.\nSummarize the missing information. Are there variables that are close to completely missing?\nAre there redundant information in the data? Try storing the data using the Arrow format and comment on the efficiency gain.\nAre there invalid NYC zipcode or borough? Justify and clean them if yes.\nAre there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second; action_update_date after closed_date.\nSummarize your suggestions to the data curator in several bullet points.\n\nData exploration.\n\nIf we suspect that response time may depend on the time of day when a complaint is made, we can compare the response times for complaints submitted during nighttime and daytime. To do this, we can visualize the comparison by complaint type, borough, and weekday (vs weekend/holiday).\nPerform a formal hypothesis test to confirm the observations from your visualization. Formally state your hypotheses and summarize your conclusions in plain English.\nCreate a binary variable over2h to indicate that a service request took two hours or longer to close.\nDoes over2h depend on the complaint type, borough, or weekday (vs weekend/holiday)? State your hypotheses and summarize your conclusions in plain English.\n\nData analysis.\n\nThe addresses of NYC police precincts are stored in data/nypd_precincts.csv. Use geocoding tools to find their geocode (longitude and latitude) from the addresses.\nCreate a variable dist2pp which represent the distance from each request incidence to the nearest police precinct.\nCreate zip code level variables by merging with data from package uszipcode.\nRandomly select 20% of the complaints as testing data with seeds 1234. Build a logistic model to predict over2h for the noise complaints with the training data, using all the variables you can engineer from the available data. If you have tuning parameters, justify how they were selected.\nAssess the performance of your model in terms of commonly used metrics. Summarize your results to a New Yorker who is not data science savvy.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Breiman, L., Friedman, J. H., Olshen, R., & Stone, C. J. (1984).\nClassification and regression trees. Wadsworth.\n\n\nCollaboration, E. H. T. (2019). First M87 event horizon telescope\nresults. I. The shadow of the supermassive black hole. The\nAstrophysical Journal Letters, 875(1), L1. https://doi.org/10.3847/2041-8213/ab0c57\n\n\nDomingos, P., & Pazzani, M. (1997). On the optimality of the simple\nBayesian classifier under zero-one loss. Machine\nLearning, 29(2-3), 103–130.\n\n\nFayaz, O. (2022). Principles of visual communication.\n\n\nGeeksforGeeks. (2023). Complement naive bayes (CNB) algorithm.\nhttps://www.geeksforgeeks.org/complement-naive-bayes-cnb-algorithm/\n\n\nGrillo, H. M., & Enesi, M. (2022). The impact, importance, types,\nand use of non-verbal communication in social relations. Linguistics\nand Culture Review, 6, 291–307.\n\n\nHunter, J. D., & Matplotlib Development Team, the. (2023a).\nMatplotlib: Visualization with python. https://matplotlib.org/stable/index.html\n\n\nHunter, J. D., & Matplotlib Development Team, the. (2023b).\nMatplotlib.animation.FuncAnimation. https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html\n\n\nIBM. (2024). What are naïve bayes classifiers? https://www.ibm.com/topics/naive-bayes\n\n\nKharlamov, M. (2023). Contextily: Adding basemaps to geospatial\ndata. https://contextily.readthedocs.io/en/latest/intro_guide.html\n\n\nNg, A., & Jordan, M. (2001). On discriminative vs. Generative\nclassifiers: A comparison of logistic regression and naive\nBayes. Proceedings of the 14th International Conference\non Neural Information Processing Systems: Natural and Synthetic,\n841–848.\n\n\nPrabavathi, R., & Nagasubramani, P. C. (2018). Effective oral and\nwritten communication. Journal of Applied and Advanced\nResearch, 10, 29–32.\n\n\nRadovilsky, Z., Hegde, V., Acharya, A., & Uma, U. (2018). Skills\nrequirements of business data analytics and data science jobs: A\ncomparative analysis. Journal of Supply Chain and Operations\nManagement, 16, 82–101.\n\n\nRish, I. (2001). An empirical study of the naive Bayes\nclassifier. IJCAI 2001 Workshop on Empirical Methods in Artificial\nIntelligence, 3, 41–46.\n\n\nTeam, G. D. (2024). GeoPandas documentation. https://geopandas.org/en/stable/index.html\n\n\nTibshirani, R. (1996). Regression shrinkage and selection via the\nLASSO. Journal of the Royal Statistical Society: Series\nB (Methodological), 58(1), 267–288.\n\n\nVandemeulebroecke, M., Baillie, M., Margolskee, A., & Magnusson, B.\n(2019). Effective visual communication for the quantitative scientist.\nCPT Pharmacometrics Syst Pharmacol, 10, 705–719.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nWilkinson, L. (2012). The grammar of graphics. Springer.\n\n\nYer, S. (2018). Verbal communication as a two-way process in connecting\npeople. Social Science Research Network.",
    "crumbs": [
      "References"
    ]
  }
]