{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Naive Bayes\"\n",
        "subtitle: \"STAT 3255 - Introduction to Data Science\"\n",
        "author: Suha Akach\n",
        "format:\n",
        "  revealjs:\n",
        "    slide-number: true\n",
        "    preview-links: true\n",
        "    theme: serif\n",
        "    transition: slide\n",
        "execute: \n",
        "  cache: true    \n",
        "echo: true\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Topics to be covered:\n",
        "::: {.incremental}\n",
        "\n",
        "- What is Naive Bayes?\n",
        "- Implementing our NY Crash Data analysis.\n",
        "- Testing Naive Bayes in Python.\n",
        ":::\n",
        "\n",
        "# What is Naive Bayes?\n",
        "- A classification algorithm based on Bayes Theorem.\n",
        "- \"Naive\" because it assumes conditional independence.\n",
        "- Used for solving text categorization problems such as email spam detection, weather detection, recommender systems, etc.\n",
        "\n",
        "::: {.callout-tip title=\"Bayes Rule\"}\n",
        "The **Bayes Theorem** takes a test result and relates it to the conditional probability of that test result given other related events. The theorem gives a more generilized likelihood of an outcome.\n",
        ":::\n",
        "\n",
        "\n",
        "# Implementing our NY Crash Data analysis\n",
        "## {.scrollable}\n",
        "::: {.incremental}\n",
        "- We want to predict whether a crash will be severe based on our independent X predictors: \n",
        "    - Borough, location, crash time, contributing factor vehicle 1, contributing factor vehicle 2, vehicle type code 1, vehicle type code 2.\n",
        "- Classification Target: \n",
        "    - Severe Crash: (1 = if either number of persons killed > 0 or number of persons injured > 1) , and 0 = otherwise.\n",
        ":::\n",
        "\n",
        "#  Naive Bayes Formula:\n",
        "## {.scrollable}\n",
        "$$\n",
        "P(Y | X) = \\frac{P(X | Y) \\cdot P(Y)}{P(X)} \n",
        "$$\n",
        "$$\n",
        "P(X | Y) = P(X_1 | Y) \\cdot P(X_2 | Y) \\cdot P(X_3 | Y) \\cdots P(X_n | Y)\n",
        "$$\n",
        "\n",
        "##### Where:\n",
        "- \\( P(Y | X) \\): The probability of the target \\( Y \\) (severe crash or not) given the chosen predictors \\( X \\).\n",
        "- \\( P(X | Y) \\): The likelihood of the predictors \\( X \\) given the target \\( Y \\). \n",
        "- \\( P(Y) \\): The prior probability of the target \\( Y \\). \n",
        "- \\( P(X) \\): The total probability of the predictors \\( X \\).\n",
        "\n",
        "\n",
        "# NY Crash Data Analysis w/ Python\n",
        "## {.scrollable}\n",
        "##### 1. Importing packages"
      ],
      "id": "350cb71f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split # A function to split the dataset into training and testing sets.\n",
        "from sklearn.naive_bayes import GaussianNB # The Naive Bayes classifier for Gaussian-distributed data. Itâ€™s suitable for continuous data, but it can also handle categorical data.\n",
        "from sklearn.preprocessing import OneHotEncoder # Converts categorical variables into a format that can be provided to ML algorithms.\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score # Metrics used to evaluate the model's performance.\n",
        "from imblearn.over_sampling import SMOTE # Handels class imbalance\n",
        "from sklearn.impute import SimpleImputer # If there are any missing values in dataset, use imputation techniques before applying SMOTE.\n",
        "\n",
        "# Load the dataset.\n",
        "data = pd.read_csv('nyccrashes_2024w0630_by20240916.csv')"
      ],
      "id": "f6fbf75e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip title=\"Training and Testing Sets\"}\n",
        "- During **Training**, the algorithm runs our chosen predictors in the training set. \n",
        "- The **Testing** set helps assess how well the model generalizes to unseen data. It would include different data points (also with crash characteristics and severity outcomes) that were not included in our chosen training set.\n",
        ":::\n",
        "\n",
        "# 2. Predictors\n",
        "## {.scrollable}"
      ],
      "id": "38660e7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Target variable for severe crashes.\n",
        "data['Severe Crash'] = ((data['NUMBER OF PERSONS KILLED'] > 0) | (data['NUMBER OF PERSONS INJURED'] > 1)).astype(int)\n",
        "\n",
        "# Select predictors.\n",
        "predictors = ['BOROUGH', 'LOCATION', 'CRASH TIME', 'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']\n",
        "\n",
        "# Initialize data.\n",
        "X = pd.get_dummies(data[predictors], drop_first=True)\n",
        "y = data['Severe Crash']\n",
        "\n",
        "# Impute any missing values (if any)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "id": "4ca19a9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip title=\"Functions\"}\n",
        "- **pd.get_dummies** creates new columns for each category in the categorical variables. Assigns a 1 or 0 to indicate the presence of each category.\n",
        "- **drop_first=True** removes the initial category, which helps avoid multicollinearity.\n",
        "- **test_size=0.3** 30% of the data will be used for testing.\n",
        "- **SMOTE** Synthetic Minority Over-sampling Technique generates new synthetic samples. This helps create a more balanced dataset while maintaining the underlying distribution of the minority class.\n",
        ":::\n",
        "\n",
        "\n",
        "# 3. Initialize Naive  Bayes\n",
        "## {.scrollable}"
      ],
      "id": "689d2365"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train) # Trains the model \n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test) # Stores the predicted test labels\n",
        "\n",
        "# Evaluate the model\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)"
      ],
      "id": "09d7570c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip title=\"Confusion & Classification\"}\n",
        "- **Confusion Matrix** makes predictions of false and true negatives and positives.\n",
        "- **Classification** summarizes the accuracy for each class prediction (severe and not severe).\n",
        ":::\n",
        "\n",
        "#  Results & Validation\n",
        "## {.scrollable}"
      ],
      "id": "de22736e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('nyccrashes_2024w0630_by20240916.csv')\n",
        "\n",
        "# Target variable for severe crashes\n",
        "data['Severe Crash'] = ((data['NUMBER OF PERSONS KILLED'] > 0) | (data['NUMBER OF PERSONS INJURED'] > 1)).astype(int)\n",
        "\n",
        "# Select predictors\n",
        "predictors = ['BOROUGH', 'LOCATION', 'CRASH TIME', 'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2']\n",
        "\n",
        "# Create dummy variables\n",
        "X = pd.get_dummies(data[predictors], drop_first=True)\n",
        "y = data['Severe Crash']\n",
        "\n",
        "# Impute any missing values (if any)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Initialize and fit the Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Cross-Validation Average\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv = 10, scoring='accuracy')\n",
        "\n",
        "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
      ],
      "id": "6864a39f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 25px;\"></p>\n",
        "<ul style=\"font-size: 25px;\">\n",
        "  <li>True Negatives (TN): 315 (correctly predicted as class 0)\n",
        "  <li>False Positives (FP): 187 (incorrectly predicted as class 1)</li>\n",
        "  <li>False Negatives (FN): 28 (incorrectly predicted as class 0)</li>\n",
        "  <li>True Positives (TP): 33 (correctly predicted as class 1)</li>\n",
        "</ul>\n",
        "<p style=\"font-size: 25px;\"></p>\n",
        "<ul style=\"font-size: 25px;\">\n",
        " <li> We can say that the model is relatively good at predicting non-severe crashes with 92% precision and moderate recall of 62%, but it performs poorly in identifying severe crashes with 0.15 precision (very low precision).</li>\n",
        " <li> Our original model accuracy is 0.62, but the mean cross-validation accuracy is 0.60. So, the 10-fold cross-validation accuracy test does not result in performance improvement for this model.</li>\n",
        " <li> Based on this model, approximately 60% of crashes are predicted to be not severe.</li>\n",
        " </ul>\n",
        " \n",
        "#  Receiver Operating Characteristic (ROC)\n",
        "## {.scrollable}\n",
        "::: {.callout-tip title=\"Receiver Operating Characteristic\"}\n",
        "(ROC) curve is a graphical representation used to evaluate the performance of a binary classification model. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
        ":::"
      ],
      "id": "271c84c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate probabilities and ROC\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs) # calculates the FPR and TPR at different threshold levels based on the true labels (y_test) and the predicted probabilities (y_probs)\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "id": "64e35397",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"font-size: 25px;\"></p>\n",
        "<ul style=\"font-size: 25px;\">\n",
        " <li> An AUC (Area Under the Curve) of 0.58 indicates that the model has a slight ability to distinguish between the two classes (severe crash vs. not severe crash), but it is not performing well. This might be due to the fact that Naive Bayes does not capture non-linear relationships effectively. </li>\n",
        " </ul>\n",
        "\n",
        "# Summary of Results\n",
        "## {.scrollable}\n",
        "- While the Naive Bayes model demonstrates strong precision for non-severe crashes, it struggles significantly with identifying severe crashes. \n",
        "- The overall accuracy of 60% and AUC of 58% indicates that the model's performance could be improved.\n",
        "- Naive Bayes classifiers does not work best with our predictors.\n",
        "- The presence of non-linear relationships among the predictors suggests that a model capable of capturing such complexity such as Random Forest and Logical Regression would be more effective than simpler models like Naive Bayes.\n",
        "\n",
        "# Conclusion\n",
        "::: {.incremental}\n",
        "- Defined Naive Bayes\n",
        "- Implemented our NY Crash Data analysis.\n",
        "- Tested Naive Bayes w/ Python.\n",
        "- Preformed Accuracy tests.\n",
        ":::\n",
        "\n",
        "# References\n",
        "<p style=\"font-size: 25px;\"></p>\n",
        "<ul style=\"font-size: 25px;\">\n",
        "@website{towardsdatascience2021,\n",
        "  title = {Introduction to NaÃ¯ve Bayes Classifier},\n",
        "  author = {Towards Data Science},\n",
        "  year = {2021},\n",
        "  url = {https://towardsdatascience.com/introduction-to-naÃ¯ve-bayes-classifier-fa59e3e24aaf},\n",
        "  note = {Accessed: 2024-10-2}\n",
        "}\n",
        "\n",
        "@misc{mlbook2021,\n",
        "  title = {NaÃ¯ve Bayes and Logistic Regression},\n",
        "  author = {Tom Mitchell},\n",
        "  year = {2021},\n",
        "  url = {https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf},\n",
        "  note = {Accessed: 2024-10-2}\n",
        "}\n",
        "\n",
        "@website{kaggle2021,\n",
        "  title = {Naive Bayes Classifier in Python},\n",
        "  author = {Prashant},\n",
        "  year = {2021},\n",
        "  url = {https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python/notebook},\n",
        "  note = {Accessed: 2024-10-2}\n",
        "}\n",
        "</ul>\n",
        "\n",
        "# THANK YOU!"
      ],
      "id": "3a111fc6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Suhak\\AppData\\Local\\Programs\\Python\\Python310\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}