<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Supervised Learning – Introduction to Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./unsupervised.html" rel="next">
<link href="./machinelearning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./supervised.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./images/ids-f24.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Data Science</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Project Management</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quarto.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Reproducible Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python Refreshment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./communication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Communication in Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./manipulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Statistical Tests and Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Machine Learning: Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Advanced Topics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#decision-trees-foundation" id="toc-decision-trees-foundation" class="nav-link active" data-scroll-target="#decision-trees-foundation"><span class="header-section-number">10.1</span> Decision Trees: Foundation</a>
  <ul class="collapse">
  <li><a href="#algorithm-formulation" id="toc-algorithm-formulation" class="nav-link" data-scroll-target="#algorithm-formulation"><span class="header-section-number">10.1.1</span> Algorithm Formulation</a></li>
  <li><a href="#search-space-for-possible-splits" id="toc-search-space-for-possible-splits" class="nav-link" data-scroll-target="#search-space-for-possible-splits"><span class="header-section-number">10.1.2</span> Search Space for Possible Splits</a></li>
  <li><a href="#metrics" id="toc-metrics" class="nav-link" data-scroll-target="#metrics"><span class="header-section-number">10.1.3</span> Metrics</a></li>
  </ul></li>
  <li><a href="#decision-trees-demonstration" id="toc-decision-trees-demonstration" class="nav-link" data-scroll-target="#decision-trees-demonstration"><span class="header-section-number">10.2</span> Decision Trees: Demonstration</a></li>
  <li><a href="#decision-trees-demonstration-1" id="toc-decision-trees-demonstration-1" class="nav-link" data-scroll-target="#decision-trees-demonstration-1"><span class="header-section-number">10.3</span> Decision Trees: Demonstration</a>
  <ul class="collapse">
  <li><a href="#about-the-trees" id="toc-about-the-trees" class="nav-link" data-scroll-target="#about-the-trees"><span class="header-section-number">10.3.1</span> About the Trees</a></li>
  <li><a href="#classification-trees-overview" id="toc-classification-trees-overview" class="nav-link" data-scroll-target="#classification-trees-overview"><span class="header-section-number">10.3.2</span> Classification Trees Overview</a></li>
  <li><a href="#splitting-the-variables-at-each-node" id="toc-splitting-the-variables-at-each-node" class="nav-link" data-scroll-target="#splitting-the-variables-at-each-node"><span class="header-section-number">10.3.3</span> Splitting the Variables at Each Node</a></li>
  <li><a href="#classification-tree-implementation" id="toc-classification-tree-implementation" class="nav-link" data-scroll-target="#classification-tree-implementation"><span class="header-section-number">10.3.4</span> Classification Tree Implementation</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation"><span class="header-section-number">10.3.5</span> Data Preparation</a></li>
  <li><a href="#data-preparation-1" id="toc-data-preparation-1" class="nav-link" data-scroll-target="#data-preparation-1"><span class="header-section-number">10.3.6</span> Data Preparation</a></li>
  <li><a href="#entropy-based-model" id="toc-entropy-based-model" class="nav-link" data-scroll-target="#entropy-based-model"><span class="header-section-number">10.3.7</span> Entropy-based Model</a></li>
  <li><a href="#gini-based-model" id="toc-gini-based-model" class="nav-link" data-scroll-target="#gini-based-model"><span class="header-section-number">10.3.8</span> Gini-based Model</a></li>
  <li><a href="#regression-trees-overview" id="toc-regression-trees-overview" class="nav-link" data-scroll-target="#regression-trees-overview"><span class="header-section-number">10.3.9</span> Regression Trees Overview</a></li>
  <li><a href="#regression-trees-overview-1" id="toc-regression-trees-overview-1" class="nav-link" data-scroll-target="#regression-trees-overview-1"><span class="header-section-number">10.3.10</span> Regression Trees Overview</a></li>
  <li><a href="#regression-tree-implementation" id="toc-regression-tree-implementation" class="nav-link" data-scroll-target="#regression-tree-implementation"><span class="header-section-number">10.3.11</span> Regression Tree Implementation</a></li>
  <li><a href="#data-preparation-2" id="toc-data-preparation-2" class="nav-link" data-scroll-target="#data-preparation-2"><span class="header-section-number">10.3.12</span> Data Preparation</a></li>
  <li><a href="#data-preparation-3" id="toc-data-preparation-3" class="nav-link" data-scroll-target="#data-preparation-3"><span class="header-section-number">10.3.13</span> Data Preparation</a></li>
  <li><a href="#data-preparation-4" id="toc-data-preparation-4" class="nav-link" data-scroll-target="#data-preparation-4"><span class="header-section-number">10.3.14</span> Data Preparation</a></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training"><span class="header-section-number">10.3.15</span> Model Training</a></li>
  <li><a href="#residual-plot" id="toc-residual-plot" class="nav-link" data-scroll-target="#residual-plot"><span class="header-section-number">10.3.16</span> Residual plot</a></li>
  <li><a href="#tree-visualization" id="toc-tree-visualization" class="nav-link" data-scroll-target="#tree-visualization"><span class="header-section-number">10.3.17</span> Tree Visualization</a></li>
  <li><a href="#export-tree-to-text" id="toc-export-tree-to-text" class="nav-link" data-scroll-target="#export-tree-to-text"><span class="header-section-number">10.3.18</span> Export Tree to Text</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10.3.19</span> Conclusion</a></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings"><span class="header-section-number">10.3.20</span> Further Readings</a></li>
  </ul></li>
  <li><a href="#boosted-trees" id="toc-boosted-trees" class="nav-link" data-scroll-target="#boosted-trees"><span class="header-section-number">10.4</span> Boosted Trees</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">10.4.1</span> Introduction</a></li>
  <li><a href="#boosting-process" id="toc-boosting-process" class="nav-link" data-scroll-target="#boosting-process"><span class="header-section-number">10.4.2</span> Boosting Process</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts"><span class="header-section-number">10.4.3</span> Key Concepts</a></li>
  <li><a href="#why-boosting-works" id="toc-why-boosting-works" class="nav-link" data-scroll-target="#why-boosting-works"><span class="header-section-number">10.4.4</span> Why Boosting Works</a></li>
  <li><a href="#applications-and-popular-implementations" id="toc-applications-and-popular-implementations" class="nav-link" data-scroll-target="#applications-and-popular-implementations"><span class="header-section-number">10.4.5</span> Applications and Popular Implementations</a></li>
  </ul></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="header-section-number">10.5</span> Naive Bayes</a>
  <ul class="collapse">
  <li><a href="#theoretical-foundations" id="toc-theoretical-foundations" class="nav-link" data-scroll-target="#theoretical-foundations"><span class="header-section-number">10.5.1</span> Theoretical Foundations</a></li>
  <li><a href="#types-of-naive-bayes" id="toc-types-of-naive-bayes" class="nav-link" data-scroll-target="#types-of-naive-bayes"><span class="header-section-number">10.5.2</span> Types of Naive Bayes:</a></li>
  <li><a href="#naive-bayes-w-nyc-crash-data" id="toc-naive-bayes-w-nyc-crash-data" class="nav-link" data-scroll-target="#naive-bayes-w-nyc-crash-data"><span class="header-section-number">10.5.3</span> Naive Bayes w/ NYC Crash Data</a></li>
  </ul></li>
  <li><a href="#handling-imbalanced-data-with-smote" id="toc-handling-imbalanced-data-with-smote" class="nav-link" data-scroll-target="#handling-imbalanced-data-with-smote"><span class="header-section-number">10.6</span> Handling Imbalanced Data with SMOTE</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">10.6.1</span> Introduction</a></li>
  <li><a href="#class-imbalance" id="toc-class-imbalance" class="nav-link" data-scroll-target="#class-imbalance"><span class="header-section-number">10.6.2</span> Class Imbalance</a></li>
  <li><a href="#synthetic-model-oversampling-technique" id="toc-synthetic-model-oversampling-technique" class="nav-link" data-scroll-target="#synthetic-model-oversampling-technique"><span class="header-section-number">10.6.3</span> Synthetic Model Oversampling Technique</a></li>
  <li><a href="#smote-versus-traditional-methods" id="toc-smote-versus-traditional-methods" class="nav-link" data-scroll-target="#smote-versus-traditional-methods"><span class="header-section-number">10.6.4</span> SMOTE versus Traditional Methods</a></li>
  <li><a href="#installation-and-setup" id="toc-installation-and-setup" class="nav-link" data-scroll-target="#installation-and-setup"><span class="header-section-number">10.6.5</span> Installation and Setup</a></li>
  <li><a href="#data-preparation-5" id="toc-data-preparation-5" class="nav-link" data-scroll-target="#data-preparation-5"><span class="header-section-number">10.6.6</span> Data Preparation</a></li>
  <li><a href="#data-visualization" id="toc-data-visualization" class="nav-link" data-scroll-target="#data-visualization"><span class="header-section-number">10.6.7</span> Data Visualization</a></li>
  <li><a href="#implementing-smote" id="toc-implementing-smote" class="nav-link" data-scroll-target="#implementing-smote"><span class="header-section-number">10.6.8</span> Implementing SMOTE</a></li>
  <li><a href="#visualization-after-smote" id="toc-visualization-after-smote" class="nav-link" data-scroll-target="#visualization-after-smote"><span class="header-section-number">10.6.9</span> Visualization after SMOTE</a></li>
  <li><a href="#model-training-using-smote" id="toc-model-training-using-smote" class="nav-link" data-scroll-target="#model-training-using-smote"><span class="header-section-number">10.6.10</span> Model Training using SMOTE</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation"><span class="header-section-number">10.6.11</span> Model Evaluation</a></li>
  <li><a href="#another-example-using-nyc-crash-data" id="toc-another-example-using-nyc-crash-data" class="nav-link" data-scroll-target="#another-example-using-nyc-crash-data"><span class="header-section-number">10.6.12</span> Another Example Using NYC Crash Data</a></li>
  <li><a href="#nyc-crash-data-part-2" id="toc-nyc-crash-data-part-2" class="nav-link" data-scroll-target="#nyc-crash-data-part-2"><span class="header-section-number">10.6.13</span> NYC Crash Data Part 2</a></li>
  <li><a href="#limitations-and-challenges-of-smote" id="toc-limitations-and-challenges-of-smote" class="nav-link" data-scroll-target="#limitations-and-challenges-of-smote"><span class="header-section-number">10.6.14</span> Limitations and Challenges of SMOTE</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">10.6.15</span> Conclusion</a></li>
  <li><a href="#further-readings-1" id="toc-further-readings-1" class="nav-link" data-scroll-target="#further-readings-1"><span class="header-section-number">10.6.16</span> Further Readings</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Supervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="decision-trees-foundation" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="decision-trees-foundation"><span class="header-section-number">10.1</span> Decision Trees: Foundation</h2>
<p>Decision trees are widely used supervised learning models that predict the value of a target variable by iteratively splitting the dataset based on decision rules derived from input features. The model functions as a piecewise constant approximation of the target function, producing clear, interpretable rules that are easily visualized and analyzed <span class="citation" data-cites="breiman1984classification">(<a href="references.html#ref-breiman1984classification" role="doc-biblioref">Breiman et al., 1984</a>)</span>. Decision trees are fundamental in both classification and regression tasks, serving as the building blocks for more advanced ensemble models such as Random Forests and Gradient Boosting Machines.</p>
<section id="algorithm-formulation" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="algorithm-formulation"><span class="header-section-number">10.1.1</span> Algorithm Formulation</h3>
<p>The core mechanism of a decision tree algorithm is the identification of optimal splits that partition the data into subsets that are increasingly homogeneous with respect to the target variable. At any node <span class="math inline">\(m\)</span>, the data subset is denoted as <span class="math inline">\(Q_m\)</span> with a sample size of <span class="math inline">\(n_m\)</span>. The objective is to find a candidate split <span class="math inline">\(\theta\)</span>, defined as a threshold for a given feature, that minimizes an impurity or loss measure <span class="math inline">\(H\)</span>.</p>
<p>When a split is made at node <span class="math inline">\(m\)</span>, the data is divided into two subsets: <span class="math inline">\(Q_{m,l}\)</span> (left node) with sample size <span class="math inline">\(n_{m,l}\)</span>, and <span class="math inline">\(Q_{m,r}\)</span> (right node) with sample size <span class="math inline">\(n_{m,r}\)</span>. The split quality, measured by <span class="math inline">\(G(Q_m, \theta)\)</span>, is given by:</p>
<p><span class="math display">\[
G(Q_m, \theta) = \frac{n_{m,l}}{n_m} H(Q_{m,l}(\theta)) +
\frac{n_{m,r}}{n_m} H(Q_{m,r}(\theta)).
\]</span></p>
<p>The algorithm aims to identify the split that minimizes the impurity:</p>
<p><span class="math display">\[
\theta^* = \arg\min_{\theta} G(Q_m, \theta).
\]</span></p>
<p>This process is applied recursively at each child node until a stopping condition is met.</p>
<ul>
<li>Stopping Criteria: The algorithm stops when the maximum tree depth is reached or when the node sample size falls below a preset threshold.</li>
<li>Pruning: Reduce the complexity of the final tree by removing branches that add little predictive value. This reduces overfitting and improves the generalization accuracy of the model.</li>
</ul>
</section>
<section id="search-space-for-possible-splits" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="search-space-for-possible-splits"><span class="header-section-number">10.1.2</span> Search Space for Possible Splits</h3>
<p>At each node in the decision tree, the search space for possible splits comprises all features in the dataset and potential thresholds derived from the values of each feature. For a given feature, the algorithm considers each unique value in the current node’s subset as a possible split point. The potential thresholds are typically set as midpoints between consecutive unique values, ensuring the data is partitioned effectively.</p>
<p>Formally, let the feature set be <span class="math inline">\(\{X_1, X_2, \ldots, X_p\}\)</span>, where <span class="math inline">\(p\)</span> is the total number of features, and let the unique values of feature $ X_j $ at node $ m $ be denoted by ${v_{j,1}, v_{j,2}, , v_{j,k_j}} <span class="math inline">\(. The search space at node\)</span>m$ includes:</p>
<ul>
<li>Feature candidates: <span class="math inline">\(\{X_1, X_2, \ldots, X_p\}\)</span>.</li>
<li>Threshold candidates for <span class="math inline">\(X_j\)</span>: <span class="math display">\[
\left\{ \frac{v_{j,i} + v_{j,i+1}}{2} \mid 1 \leq i &lt; k_j \right\}.
\]</span></li>
</ul>
<p>The search space therefore encompasses all combinations of features and their respective thresholds. While the complexity of this search can be substantial, particularly for high-dimensional data or features with numerous unique values, efficient algorithms use sorting and single-pass scanning techniques to mitigate the computational cost.</p>
</section>
<section id="metrics" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="metrics"><span class="header-section-number">10.1.3</span> Metrics</h3>
<section id="classification" class="level4" data-number="10.1.3.1">
<h4 data-number="10.1.3.1" class="anchored" data-anchor-id="classification"><span class="header-section-number">10.1.3.1</span> Classification</h4>
<p>In decision tree classification, several criteria can be used to measure the quality of a split at each node. These criteria are based on how “pure” the resulting nodes are after the split. A pure node contains samples that predominantly belong to a single class. The goal is to minimize impurity, leading to nodes that are as homogeneous as possible.</p>
<ul>
<li><p>Gini Index: The Gini index measures the impurity of a node by calculating the probability of randomly choosing two different classes. A perfect split (all instances belong to one class) has a Gini index of 0. At node <span class="math inline">\(m\)</span>, the Gini index is <span class="math display">\[
H(Q_m) = \sum_{k=1}^{K} p_{mk} (1 - p_{mk}),
\]</span> where <span class="math inline">\(p_{mk}\)</span> is the proportion of samples of class <span class="math inline">\(k\)</span> at node <span class="math inline">\(m\)</span>; and<span class="math inline">\(K\)</span> is the total number of classes The Gini index is often preferred for its speed and simplicity, and it’s used by default in many implementations of decision trees, including <code>sklearn</code>.</p></li>
<li><p>Entropy (Information Gain): Entropy is another measure of impurity, derived from information theory. It quantifies the “disorder” of the data at a node. Lower entropy means higher purity. At node <span class="math inline">\(m\)</span>, it is defined as <span class="math display">\[
H(Q_m) = - \sum_{k=1}^{K} p_{mk} \log p_{mk}
\]</span> Entropy is commonly used in decision tree algorithms like ID3 and C4.5. The choice between Gini and entropy often depends on specific use cases, but both perform similarly in practice.</p></li>
<li><p>Misclassification Error: Misclassification error focuses solely on the most frequent class in the node. It measures the proportion of samples that do not belong to the majority class. Although less sensitive than Gini and entropy, it can be useful for classification when simplicity is preferred. At node <span class="math inline">\(m\)</span>, it is defined as <span class="math display">\[
H(Q_m) = 1 - \max_k p_{mk},
\]</span> where <span class="math inline">\(\max_k p_{mk}\)</span> is the largest proportion of samples belonging to any class <span class="math inline">\(k\)</span>.</p></li>
</ul>
</section>
<section id="regression-criteria" class="level4" data-number="10.1.3.2">
<h4 data-number="10.1.3.2" class="anchored" data-anchor-id="regression-criteria"><span class="header-section-number">10.1.3.2</span> Regression Criteria</h4>
<p>In decision tree regression, different criteria are used to assess the quality of a split. The goal is to minimize the spread or variance of the target variable within each node.</p>
<ul>
<li><p>Mean Squared Error (MSE): Mean squared error is the most common criterion used in regression trees. It measures the average squared difference between the actual values and the predicted values (mean of the target in the node). The smaller the MSE, the better the fit. At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} (y_i - \bar{y}_m)^2,
\]</span> where</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the actual value for sample <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(\bar{y}_m\)</span> is the mean value of the target at node <span class="math inline">\(m\)</span>;</li>
<li><span class="math inline">\(n_m\)</span> is the number of samples at node <span class="math inline">\(m\)</span>.</li>
</ul>
<p>MSE works well when the target is continuous and normally distributed.</p></li>
<li><p>Half Poisson Deviance (for count targets): When dealing with count data, the Poisson deviance is used to model the variance in the number of occurrences of an event. It is well-suited for target variables representing counts (e.g., number of occurrences of an event). At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \sum_{i=1}^{n_m} \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right),
\]</span> where <span class="math inline">\(\hat{y}_i\)</span> is the predicted count. This criterion is especially useful when the target variable represents discrete counts, such as predicting the number of occurrences of an event.</p></li>
<li><p>Mean Absolute Error (MAE): Mean absolute error is another criterion that minimizes the absolute differences between actual and predicted values. While it is more robust to outliers than MSE, it is slower computationally due to the lack of a closed-form solution for minimization. At node <span class="math inline">\(m\)</span>, it is <span class="math display">\[
H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} |y_i - \bar{y}_m|
\]</span> MAE is useful when you want to minimize large deviations and can be more robust in cases where outliers are present in the data.</p></li>
</ul>
</section>
<section id="summary" class="level4" data-number="10.1.3.3">
<h4 data-number="10.1.3.3" class="anchored" data-anchor-id="summary"><span class="header-section-number">10.1.3.3</span> Summary</h4>
<p>In decision trees, the choice of splitting criterion depends on the type of task (classification or regression) and the nature of the data. For classification tasks, the Gini index and entropy are the most commonly used, with Gini offering simplicity and speed, and entropy providing a more theoretically grounded approach. Misclassification error can be used for simpler cases. For regression tasks, MSE is the most popular choice, but Poisson deviance and MAE are useful for specific use cases such as count data and robust models, respectively.</p>
</section>
</section>
</section>
<section id="decision-trees-demonstration" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="decision-trees-demonstration"><span class="header-section-number">10.2</span> Decision Trees: Demonstration</h2>
</section>
<section id="decision-trees-demonstration-1" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="decision-trees-demonstration-1"><span class="header-section-number">10.3</span> Decision Trees: Demonstration</h2>
<p>This section was presented by Jaden Astle.</p>
<section id="about-the-trees" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="about-the-trees"><span class="header-section-number">10.3.1</span> About the Trees</h3>
<ul>
<li>Supervised learning algorithm</li>
<li>Both classification &amp; regression methods</li>
<li>Goal is to sort data into specific groups, one characteristic at a time</li>
<li>Splitting follows a tree-like structure with binary splits</li>
</ul>
</section>
<section id="classification-trees-overview" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="classification-trees-overview"><span class="header-section-number">10.3.2</span> Classification Trees Overview</h3>
<ul>
<li>Splits observations based on binary classifications (categorical variables)</li>
<li>ex. Contains “Congratulations!”, has fur, has a pool, etc.</li>
<li>Note: categorical variables that are <em>not</em> binary need to be processed with One-Hot Encoding before training</li>
</ul>
<p><strong>How exactly are these splits chosen?</strong></p>
</section>
<section id="splitting-the-variables-at-each-node" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="splitting-the-variables-at-each-node"><span class="header-section-number">10.3.3</span> Splitting the Variables at Each Node</h3>
<section id="entropy" class="level4" data-number="10.3.3.1">
<h4 data-number="10.3.3.1" class="anchored" data-anchor-id="entropy"><span class="header-section-number">10.3.3.1</span> Entropy</h4>
<ul>
<li><strong>Measure of randomness</strong> or disorder in the given environment.</li>
<li>Observes <strong>uncertainty</strong> of data based on distribution of classes at any given step.</li>
<li>A lower entropy value means less disorder, therefore a <strong>better split</strong>.</li>
</ul>
</section>
<section id="gini-index" class="level4" data-number="10.3.3.2">
<h4 data-number="10.3.3.2" class="anchored" data-anchor-id="gini-index"><span class="header-section-number">10.3.3.2</span> Gini Index</h4>
<ul>
<li>Utilizes the <strong>probability</strong> that a random element is incorrectly labeled based on labeling from the original dataset distribution.</li>
<li>A lower Gini index means a <strong>better split</strong>.</li>
</ul>
</section>
</section>
<section id="classification-tree-implementation" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="classification-tree-implementation"><span class="header-section-number">10.3.4</span> Classification Tree Implementation</h3>
</section>
<section id="data-preparation" class="level3" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="data-preparation"><span class="header-section-number">10.3.5</span> Data Preparation</h3>
<div id="a4662cad" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_feather(<span class="st">'data/nyccrashes_cleaned.feather'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>vehicle_columns <span class="op">=</span> [<span class="st">'vehicle_type_code_1'</span>, <span class="st">'vehicle_type_code_2'</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a> <span class="st">'vehicle_type_code_3'</span>, <span class="st">'vehicle_type_code_4'</span>, <span class="st">'vehicle_type_code_5'</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'number_of_vehicles_involved'</span>] <span class="op">=</span> df[vehicle_columns].notna().<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df[[<span class="st">'latitude'</span>, <span class="st">'longitude'</span>]] <span class="op">=</span> scaler.fit_transform(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    df[[<span class="st">'latitude'</span>, <span class="st">'longitude'</span>]])</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'contributing_factor_vehicle_1'</span>] <span class="op">=</span> df[<span class="st">'contributing_factor_vehicle_1'</span>].<span class="bu">str</span>.lower().<span class="bu">str</span>.replace(<span class="st">' '</span>, <span class="st">'_'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.get_dummies(df, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>columns<span class="op">=</span>[<span class="st">'borough'</span>, <span class="st">'contributing_factor_vehicle_1'</span>],</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>drop_first<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data-preparation-1" class="level3" data-number="10.3.6">
<h3 data-number="10.3.6" class="anchored" data-anchor-id="data-preparation-1"><span class="header-section-number">10.3.6</span> Data Preparation</h3>
<div id="deb82b9a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'latitude'</span>, <span class="st">'longitude'</span>, <span class="st">'number_of_vehicles_involved'</span>] </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'borough_'</span> <span class="kw">in</span> col] </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'contributing_factor_vehicle_1_'</span> <span class="kw">in</span> col]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'severe'</span>] <span class="op">=</span> (</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    (df[<span class="st">'number_of_persons_killed'</span>] <span class="op">&gt;=</span> <span class="dv">1</span>) <span class="op">|</span> (df[<span class="st">'number_of_persons_injured'</span>] <span class="op">&gt;=</span> <span class="dv">1</span>)).astype(<span class="bu">int</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset<span class="op">=</span>features)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[features]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'severe'</span>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">1234</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="entropy-based-model" class="level3" data-number="10.3.7">
<h3 data-number="10.3.7" class="anchored" data-anchor-id="entropy-based-model"><span class="header-section-number">10.3.7</span> Entropy-based Model</h3>
<div id="13dbd869" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier <span class="im">as</span> DTC</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DTC(criterion<span class="op">=</span><span class="st">'entropy'</span>, max_depth<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>tree.fit(X_train, y_train)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> tree.predict(X_test)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[143 147]
 [114 113]]
              precision    recall  f1-score   support

           0       0.56      0.49      0.52       290
           1       0.43      0.50      0.46       227

    accuracy                           0.50       517
   macro avg       0.50      0.50      0.49       517
weighted avg       0.50      0.50      0.50       517
</code></pre>
</div>
</div>
</section>
<section id="gini-based-model" class="level3" data-number="10.3.8">
<h3 data-number="10.3.8" class="anchored" data-anchor-id="gini-based-model"><span class="header-section-number">10.3.8</span> Gini-based Model</h3>
<div id="ae7a9dcf" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier <span class="im">as</span> DTC</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DTC(criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>tree.fit(X_train, y_train)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> tree.predict(X_test)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[170 120]
 [136  91]]
              precision    recall  f1-score   support

           0       0.56      0.59      0.57       290
           1       0.43      0.40      0.42       227

    accuracy                           0.50       517
   macro avg       0.49      0.49      0.49       517
weighted avg       0.50      0.50      0.50       517
</code></pre>
</div>
</div>
</section>
<section id="regression-trees-overview" class="level3" data-number="10.3.9">
<h3 data-number="10.3.9" class="anchored" data-anchor-id="regression-trees-overview"><span class="header-section-number">10.3.9</span> Regression Trees Overview</h3>
<ul>
<li>Built for continuous variables; predicts continuous values</li>
<li>Rather than have the split based on Gini or Entropy, the split is based on MSE</li>
<li>Calculate all possible splits of all features; minimize total MSE</li>
</ul>
<p><span class="math display">\[
\text{MSE} = \frac{1}{n} \sum (y_i - \bar{y})^2
\]</span></p>
<p><span class="math display">\[
\text{Total MSE} = \frac{n_{\text{total}}}{n_{\text{left}}} \cdot \text{MSE}_{\text{left}} + \frac{n_{\text{total}}}{n_{\text{right}}} \cdot \text{MSE}_{\text{right}}
\]</span></p>
</section>
<section id="regression-trees-overview-1" class="level3" data-number="10.3.10">
<h3 data-number="10.3.10" class="anchored" data-anchor-id="regression-trees-overview-1"><span class="header-section-number">10.3.10</span> Regression Trees Overview</h3>
<ul>
<li>What about leaf nodes? What are the final classifications?</li>
<li>Average y of all remaining observations in that node becomes prediction for future observations</li>
</ul>
</section>
<section id="regression-tree-implementation" class="level3" data-number="10.3.11">
<h3 data-number="10.3.11" class="anchored" data-anchor-id="regression-tree-implementation"><span class="header-section-number">10.3.11</span> Regression Tree Implementation</h3>
</section>
<section id="data-preparation-2" class="level3" data-number="10.3.12">
<h3 data-number="10.3.12" class="anchored" data-anchor-id="data-preparation-2"><span class="header-section-number">10.3.12</span> Data Preparation</h3>
<div id="5f41c732" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/nypd311w063024noise_by100724.csv'</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>df.columns <span class="op">=</span> df.columns.<span class="bu">str</span>.lower().<span class="bu">str</span>.replace(<span class="st">' '</span>, <span class="st">'_'</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'created_date'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'created_date'</span>])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'closed_date'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'closed_date'</span>])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'hour'</span>] <span class="op">=</span> df[<span class="st">'created_date'</span>].dt.hour</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'tod'</span>] <span class="op">=</span> np.where((df[<span class="st">'hour'</span>] <span class="op">&gt;=</span> <span class="dv">20</span>) <span class="op">|</span> (df[<span class="st">'hour'</span>] <span class="op">&lt;</span> <span class="dv">6</span>), <span class="st">'Nighttime'</span>, <span class="st">'Daytime'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'dow'</span>] <span class="op">=</span> df[<span class="st">'created_date'</span>].dt.weekday</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'day_type'</span>] <span class="op">=</span> np.where(df[<span class="st">'dow'</span>] <span class="op">&lt;</span> <span class="dv">5</span>, <span class="st">'Weekday'</span>, <span class="st">'Weekend'</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'response_time'</span>] <span class="op">=</span> (df[<span class="st">'closed_date'</span>] <span class="op">-</span> df[<span class="st">'created_date'</span>]).dt.total_seconds() <span class="op">/</span> <span class="dv">3600</span> <span class="co">#this will be the y-variable</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>df.columns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_46954/2261056809.py:9: UserWarning:

Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.

/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_46954/2261056809.py:10: UserWarning:

Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>Index(['unique_key', 'created_date', 'closed_date', 'agency', 'agency_name',
       'complaint_type', 'descriptor', 'location_type', 'incident_zip',
       'incident_address', 'street_name', 'cross_street_1', 'cross_street_2',
       'intersection_street_1', 'intersection_street_2', 'address_type',
       'city', 'landmark', 'facility_type', 'status', 'due_date',
       'resolution_description', 'resolution_action_updated_date',
       'community_board', 'bbl', 'borough', 'x_coordinate_(state_plane)',
       'y_coordinate_(state_plane)', 'open_data_channel_type',
       'park_facility_name', 'park_borough', 'vehicle_type',
       'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name',
       'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment',
       'latitude', 'longitude', 'location', 'hour', 'tod', 'dow', 'day_type',
       'response_time'],
      dtype='object')</code></pre>
</div>
</div>
</section>
<section id="data-preparation-3" class="level3" data-number="10.3.13">
<h3 data-number="10.3.13" class="anchored" data-anchor-id="data-preparation-3"><span class="header-section-number">10.3.13</span> Data Preparation</h3>
<div id="d1d8227f" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>[<span class="st">'complaint_type'</span>, <span class="st">'borough'</span>, <span class="st">'location_type'</span>, <span class="st">'address_type'</span>, <span class="st">'tod'</span>, <span class="st">'day_type'</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'sin_hour'</span>] <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> df[<span class="st">'hour'</span>] <span class="op">/</span> <span class="dv">24</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'cos_hour'</span>] <span class="op">=</span> np.cos(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> df[<span class="st">'hour'</span>] <span class="op">/</span> <span class="dv">24</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>df[[<span class="st">'latitude'</span>, <span class="st">'longitude'</span>]] <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'latitude'</span>, <span class="st">'longitude'</span>]])</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'sin_hour'</span>, <span class="st">'cos_hour'</span>, <span class="st">'latitude'</span>, <span class="st">'longitude'</span>, <span class="st">'dow'</span>, <span class="st">'tod_Nighttime'</span>, <span class="st">'day_type_Weekend'</span>]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'complaint_type_'</span> <span class="kw">in</span> col]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'borough_'</span> <span class="kw">in</span> col]</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'location_type_'</span> <span class="kw">in</span> col]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>features <span class="op">+=</span> [col <span class="cf">for</span> col <span class="kw">in</span> df.columns <span class="cf">if</span> <span class="st">'address_type_'</span> <span class="kw">in</span> col]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna(subset<span class="op">=</span>[<span class="st">'latitude'</span>, <span class="st">'longitude'</span>])</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>training_df <span class="op">=</span> df[features]</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>training_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sin_hour</th>
<th data-quarto-table-cell-role="th">cos_hour</th>
<th data-quarto-table-cell-role="th">latitude</th>
<th data-quarto-table-cell-role="th">longitude</th>
<th data-quarto-table-cell-role="th">dow</th>
<th data-quarto-table-cell-role="th">tod_Nighttime</th>
<th data-quarto-table-cell-role="th">day_type_Weekend</th>
<th data-quarto-table-cell-role="th">complaint_type_Noise - House of Worship</th>
<th data-quarto-table-cell-role="th">complaint_type_Noise - Park</th>
<th data-quarto-table-cell-role="th">complaint_type_Noise - Residential</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">borough_STATEN ISLAND</th>
<th data-quarto-table-cell-role="th">borough_Unspecified</th>
<th data-quarto-table-cell-role="th">location_type_House of Worship</th>
<th data-quarto-table-cell-role="th">location_type_Park/Playground</th>
<th data-quarto-table-cell-role="th">location_type_Residential Building/House</th>
<th data-quarto-table-cell-role="th">location_type_Store/Commercial</th>
<th data-quarto-table-cell-role="th">location_type_Street/Sidewalk</th>
<th data-quarto-table-cell-role="th">address_type_BLOCKFACE</th>
<th data-quarto-table-cell-role="th">address_type_INTERSECTION</th>
<th data-quarto-table-cell-role="th">address_type_UNRECOGNIZED</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-0.258819</td>
<td>0.965926</td>
<td>-0.225836</td>
<td>3.014662</td>
<td>5</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>...</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-0.258819</td>
<td>0.965926</td>
<td>-1.468384</td>
<td>-0.396220</td>
<td>5</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>...</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-0.258819</td>
<td>0.965926</td>
<td>0.073297</td>
<td>-0.119805</td>
<td>5</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>...</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-0.258819</td>
<td>0.965926</td>
<td>1.259327</td>
<td>0.116306</td>
<td>5</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>...</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>-0.258819</td>
<td>0.965926</td>
<td>-0.873509</td>
<td>0.723039</td>
<td>5</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>...</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p>5 rows × 25 columns</p>
</div>
</div>
</div>
</section>
<section id="data-preparation-4" class="level3" data-number="10.3.14">
<h3 data-number="10.3.14" class="anchored" data-anchor-id="data-preparation-4"><span class="header-section-number">10.3.14</span> Data Preparation</h3>
<div id="8b5e14c3" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[features]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'response_time'</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1234</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-training" class="level3" data-number="10.3.15">
<h3 data-number="10.3.15" class="anchored" data-anchor-id="model-training"><span class="header-section-number">10.3.15</span> Model Training</h3>
<div id="a02ba9fd" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor <span class="im">as</span> DTR</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> DTR(max_depth<span class="op">=</span><span class="dv">20</span>, min_samples_split<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>tree.fit(X_train, y_train)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> tree.predict(X_test)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_squared_error(y_test, y_pred))</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(r2_score(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3.84329959566305
0.6251134474207172</code></pre>
</div>
</div>
</section>
<section id="residual-plot" class="level3" data-number="10.3.16">
<h3 data-number="10.3.16" class="anchored" data-anchor-id="residual-plot"><span class="header-section-number">10.3.16</span> Residual plot</h3>
<div id="d0c859b8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>residuals <span class="op">=</span> y_test <span class="op">-</span> y_pred</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_pred, residuals)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.hlines(<span class="dv">0</span>, <span class="bu">min</span>(y_pred), <span class="bu">max</span>(y_pred), colors<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted Values (response time)"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Residuals"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Residual Plot"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="supervised_files/figure-html/cell-10-output-1.png" width="596" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tree-visualization" class="level3" data-number="10.3.17">
<h3 data-number="10.3.17" class="anchored" data-anchor-id="tree-visualization"><span class="header-section-number">10.3.17</span> Tree Visualization</h3>
<div id="eaa3bb6b" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> plot_tree</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">6</span>)) </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plot_tree(tree, feature_names<span class="op">=</span>X_train.columns, filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="supervised_files/figure-html/cell-11-output-1.png" width="703" height="463" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="export-tree-to-text" class="level3" data-number="10.3.18">
<h3 data-number="10.3.18" class="anchored" data-anchor-id="export-tree-to-text"><span class="header-section-number">10.3.18</span> Export Tree to Text</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> export_text</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>tree_structure <span class="op">=</span> export_text(tree, feature_names<span class="op">=</span><span class="bu">list</span>(X_train.columns))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tree_structure)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will print a text version of the decision tree in a tree-like format, with each indent representing a new split.</p>
</section>
<section id="conclusion" class="level3" data-number="10.3.19">
<h3 data-number="10.3.19" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10.3.19</span> Conclusion</h3>
<ul>
<li>Decision trees are effective for both classification &amp; regression tasks
<ul>
<li>classification vs.&nbsp;regression trees</li>
</ul></li>
<li>Very flexible &amp; easy to interpret</li>
<li>Easily adjustable parameters to help prevent overfitting (max tree depth, min sample split)</li>
</ul>
</section>
<section id="further-readings" class="level3" data-number="10.3.20">
<h3 data-number="10.3.20" class="anchored" data-anchor-id="further-readings"><span class="header-section-number">10.3.20</span> Further Readings</h3>
<ol type="1">
<li><a href="https://scikit-learn.org/1.5/modules/tree.html">Scikit-Learn Decision Tree Documentation</a></li>
<li><a href="https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef">“Decision Tree Regressor, Explained: A Visual Guide with Code Examples” by Samy Baladram</a></li>
<li><a href="https://www.geeksforgeeks.org/decision-tree/">GeeksforGeeks Decision Tree Overview</a></li>
</ol>
</section>
</section>
<section id="boosted-trees" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="boosted-trees"><span class="header-section-number">10.4</span> Boosted Trees</h2>
<p>Boosted trees are a powerful ensemble technique in machine learning that combine multiple weak learners, typically decision trees, into a strong learner. Unlike bagging methods, which train trees independently, boosting fits models sequentially, with each new model correcting the errors of the previous ensemble. Gradient boosting, one of the most popular variants, optimizes a loss function by iteratively adding trees that reduce the residual errors of the current ensemble.</p>
<section id="introduction" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">10.4.1</span> Introduction</h3>
<p>Boosted trees build on the general concept of boosting, which aims to create a strong predictor from a series of weak predictors. In boosted trees, the weak learners are shallow decision trees, often referred to as “stumps,” and they are added sequentially to the model. At each step, a new tree focuses on the training instances that are hardest to predict, improving overall accuracy. This iterative focus on “hard-to- predict” instances is the defining characteristic of boosting.</p>
<p>The effectiveness of boosted trees has made them popular for various tasks, including classification, regression, and ranking. They also form the foundation for algorithms like XGBoost, LightGBM, and CatBoost, known for their speed and scalability.</p>
</section>
<section id="boosting-process" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="boosting-process"><span class="header-section-number">10.4.2</span> Boosting Process</h3>
<p>The boosting process in gradient boosted trees builds an ensemble by adding trees iteratively, each designed to minimize the residual errors from the combined predictions of the previous trees. This iterative approach allows the model to refine its predictions by optimizing a loss function, denoted as <span class="math inline">\(L(y, F(x))\)</span>, where <span class="math inline">\(y\)</span> is the true value and <span class="math inline">\(F(x)\)</span> is the model’s prediction.</p>
<section id="model-iteration" class="level4" data-number="10.4.2.1">
<h4 data-number="10.4.2.1" class="anchored" data-anchor-id="model-iteration"><span class="header-section-number">10.4.2.1</span> Model Iteration</h4>
<p>The boosting process can be delineated as follows:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Start with a base model <span class="math inline">\(F_0(x)\)</span>, which is usually the mean of the target variable in regression or the log odds in classification:</p>
<ul>
<li>For regression: <span class="math inline">\(F_0(x) = \text{mean}(y_i)\)</span>.</li>
<li>For classification: <span class="math inline">\(F_0(x) = \log \left( \frac{P(y=1)}{1-P(y=1)} \right)\)</span>.</li>
</ul></li>
<li><p><strong>Iterative Boosting</strong>:</p>
<p>At each iteration <span class="math inline">\(m\)</span>:</p>
<ul>
<li><p>Compute the pseudo-residuals, representing the negative gradient of the loss function with respect to the current model predictions. The residuals at iteration <span class="math inline">\(m\)</span> are defined as:</p>
<p><span class="math display">\[
r_i^{(m)} = -\left. \frac{\partial L(y_i, F(x_i))}
{\partial F(x_i)} \right|_{F(x) = F_{m-1}(x)}.
\]</span></p>
<p>The residuals guide the next tree to focus on reducing the largest errors from the previous iteration.</p></li>
<li><p>Fit a new tree <span class="math inline">\(h_m(x)\)</span> to the pseudo-residuals. The new tree is trained to predict the residuals of the current ensemble model, identifying where the model needs the most improvement.</p></li>
<li><p>Update the model as the sum of the previous model and the newly added tree, scaled by a learning rate <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math display">\[
F_m(x) = F_{m-1}(x) + \eta \, h_m(x).
\]</span></p>
<p>The learning rate, a small positive number (e.g., 0.01 to 0.1), controls the contribution of each tree, ensuring incremental improvements and reducing the risk of overfitting.</p></li>
</ul></li>
<li><p><strong>Final Model</strong>:</p>
<p>After <span class="math inline">\(M\)</span> iterations, the ensemble model is given by:</p>
<p><span class="math display">\[
F_M(x) = F_0(x) + \sum_{m=1}^M \eta \, h_m(x).
\]</span></p>
<p>The final model <span class="math inline">\(F_M(x)\)</span> represents the sum of the initial model and the incremental improvements made by each of the <span class="math inline">\(M\)</span> trees, with each tree trained to correct the residuals of the ensemble up to that point.</p></li>
</ol>
</section>
</section>
<section id="key-concepts" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="key-concepts"><span class="header-section-number">10.4.3</span> Key Concepts</h3>
<ol type="1">
<li><p><strong>Loss Function</strong>: The loss function measures the discrepancy between the actual and predicted values. It guides the model updates. Common choices include:</p>
<ul>
<li>Squared error for regression: <span class="math inline">\(L(y, F(x)) = \frac{1}{2} (y - F(x))^2\)</span>.</li>
<li>Logistic loss for binary classification: <span class="math inline">\(L(y, F(x)) = \log(1 + \exp(-y \, F(x)))\)</span>.</li>
</ul></li>
<li><p><strong>Learning Rate</strong>: The learning rate scales the contribution of each tree and helps control the speed of learning. A smaller learning rate typically requires more trees but results in a more robust model with better generalization.</p></li>
<li><p><strong>Regularization</strong>: Boosted trees incorporate regularization to avoid overfitting, including:</p>
<ul>
<li>Tree depth: Limits the maximum depth of each tree, reducing model complexity.</li>
<li>L1/L2 penalties: Regularize the weights of the trees, similar to Lasso and Ridge regression.</li>
<li>Subsampling: Uses a fraction of the training data at each iteration, making the model more robust to overfitting and improving generalization.</li>
</ul></li>
</ol>
</section>
<section id="why-boosting-works" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="why-boosting-works"><span class="header-section-number">10.4.4</span> Why Boosting Works</h3>
<p>The iterative approach of boosting, focusing on correcting the errors of the ensemble at each step, distinguishes gradient boosting from other ensemble methods like bagging or random forests. Key reasons for its effectiveness include:</p>
<ol type="1">
<li><p><strong>Error Correction</strong>: By focusing on the hardest-to-predict instances, boosting gradually improves model accuracy, leading to better performance than models trained independently.</p></li>
<li><p><strong>Weighted Learning</strong>: Boosting adjusts the weights of training samples based on errors, ensuring that the model learns disproportionately from difficult cases, reducing bias.</p></li>
<li><p><strong>Flexibility</strong>: Boosted trees can handle various loss functions, making them suitable for different types of tasks, including regression, classification, and ranking.</p></li>
</ol>
</section>
<section id="applications-and-popular-implementations" class="level3" data-number="10.4.5">
<h3 data-number="10.4.5" class="anchored" data-anchor-id="applications-and-popular-implementations"><span class="header-section-number">10.4.5</span> Applications and Popular Implementations</h3>
<p>Boosted trees are widely used in real-world applications, ranging from financial risk modeling to predictive maintenance. They are also favored in machine learning competitions due to their interpretability and robustness. Popular implementations include:</p>
<ul>
<li>XGBoost: Known for its speed and performance, with features like regularization, column sampling, and advanced tree pruning.</li>
<li>LightGBM: Optimized for speed and scalability, using histogram- based algorithms to handle large datasets efficiently.</li>
<li>CatBoost: Effective with categorical features, using advanced encoding techniques and built-in support for categorical variables.</li>
</ul>
</section>
</section>
<section id="naive-bayes" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">10.5</span> Naive Bayes</h2>
<p>This section was contributed by Suha Akach.</p>
<p>Naive Bayes is a probabilistic classification algorithm based on Bayes’ Theorem, which is used for both binary and multiclass classification problems. It is particularly effective for high-dimensional datasets and is commonly applied in tasks like text classification, spam detection, and sentiment analysis. The algorithm is called “naive” because it assumes that all features are conditionally independent given the class label, an assumption that rarely holds in real-world data but still performs well in many cases.</p>
<section id="theoretical-foundations" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="theoretical-foundations"><span class="header-section-number">10.5.1</span> Theoretical Foundations</h3>
<p>The foundation of the Naive Bayes classifier is Bayes’ Theorem, which is used to update the probability estimate of a hypothesis given new evidence. Mathematically, Bayes’ Theorem is expressed as:</p>
<p><span class="math display">\[
P(y \mid X) = \frac{P(X \mid y) \, P(y)}{P(X)},
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(P(y \mid X)\)</span>: Posterior probability of class <span class="math inline">\(y\)</span> given the input features <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(P(X \mid y)\)</span>: Likelihood of observing <span class="math inline">\(X\)</span> given that the class is <span class="math inline">\(y\)</span>.</li>
<li><span class="math inline">\(P(y)\)</span>: Prior probability of the class <span class="math inline">\(y\)</span>.</li>
<li><span class="math inline">\(P(X)\)</span>: Marginal probability of the feature vector <span class="math inline">\(X\)</span>.</li>
</ul>
<section id="naive-assumption-and-likelihood-decomposition" class="level4" data-number="10.5.1.1">
<h4 data-number="10.5.1.1" class="anchored" data-anchor-id="naive-assumption-and-likelihood-decomposition"><span class="header-section-number">10.5.1.1</span> Naive Assumption and Likelihood Decomposition</h4>
<p>The algorithm makes the simplifying assumption that features in <span class="math inline">\(X\)</span> are conditionally independent given the class <span class="math inline">\(y\)</span>. This assumption enables the likelihood <span class="math inline">\(P(X \mid y)\)</span> to be decomposed as:</p>
<p><span class="math display">\[
P(X \mid y) = \prod_{i=1}^n P(x_i \mid y),
\]</span></p>
<p>where <span class="math inline">\(X = \{x_1, x_2, \ldots, x_n\}\)</span> represents the feature vector with <span class="math inline">\(n\)</span> features, and <span class="math inline">\(P(x_i \mid y)\)</span> is the conditional probability of feature <span class="math inline">\(x_i\)</span> given the class <span class="math inline">\(y\)</span>.</p>
<p>The model parameters are the prior probabilities <span class="math inline">\(P(y)\)</span> and the conditional probabilities <span class="math inline">\(P(x_i \mid y)\)</span>. These are estimated from the training data using the maximum likelihood estimation (MLE):</p>
<ol type="1">
<li><p>Prior Estimation: The prior probability <span class="math inline">\(P(y)\)</span> is estimated as the proportion of training samples in class <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\hat{P}(y) = \frac{\text{count}(y)}{N},
\]</span></p>
<p>where <span class="math inline">\(\text{count}(y)\)</span> is the number of instances belonging to class <span class="math inline">\(y\)</span>, and <span class="math inline">\(N\)</span> is the total number of training samples.</p></li>
<li><p>Conditional Probability Estimation:</p>
<ul>
<li><p>Categorical Features: For discrete or categorical features, the conditional probability <span class="math inline">\(P(x_i \mid y)\)</span> is estimated as:</p>
<p><span class="math display">\[
\hat{P}(x_i \mid y) = \frac{\text{count}(x_i, y)}{\text{count}(y)},
\]</span></p>
<p>where <span class="math inline">\(\text{count}(x_i, y)\)</span> is the number of samples in class <span class="math inline">\(y\)</span> that have feature <span class="math inline">\(x_i\)</span>.</p></li>
<li><p>Continuous Features: For continuous features, Naive Bayes commonly assumes a Gaussian distribution. In this case, <span class="math inline">\(P(x_i \mid y)\)</span> is modeled using the Gaussian distribution with mean <span class="math inline">\(\mu_{y,i}\)</span> and variance <span class="math inline">\(\sigma_{y,i}^2\)</span>:</p>
<p><span class="math display">\[
P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma_{y,i}^2}} \exp \left(
-\frac{(x_i - \mu_{y,i})^2}{2\sigma_{y,i}^2} \right).
\]</span></p>
<p>The parameters <span class="math inline">\(\mu_{y,i}\)</span> and <span class="math inline">\(\sigma_{y,i}^2\)</span> are estimated from the training data using the sample mean and variance for each feature in each class.</p></li>
</ul></li>
</ol>
</section>
<section id="class-prediction" class="level4" data-number="10.5.1.2">
<h4 data-number="10.5.1.2" class="anchored" data-anchor-id="class-prediction"><span class="header-section-number">10.5.1.2</span> Class Prediction</h4>
<p>The goal of the Naive Bayes classifier is to predict the class <span class="math inline">\(y\)</span> that maximizes the posterior probability <span class="math inline">\(P(y \mid X)\)</span>. After applying Bayes’ Theorem and dropping the constant denominator <span class="math inline">\(P(X)\)</span>, the decision rule becomes:</p>
<p><span class="math display">\[
y^* = \arg\max_y \, P(y) \prod_{i=1}^n P(x_i \mid y).
\]</span></p>
<p>In practice, the log of the posterior is used to prevent numerical underflow:</p>
<p><span class="math display">\[
\log P(y \mid X) = \log P(y) + \sum_{i=1}^n \log P(x_i \mid y).
\]</span></p>
<p>The predicted class is the one that maximizes this expression.</p>
</section>
<section id="surprisingly-good-performance" class="level4" data-number="10.5.1.3">
<h4 data-number="10.5.1.3" class="anchored" data-anchor-id="surprisingly-good-performance"><span class="header-section-number">10.5.1.3</span> Surprisingly Good Performance</h4>
<p>Although the assumption of conditional independence among features is often unrealistic, Naive Bayes still performs well for several reasons:</p>
<ol type="1">
<li><p>Robustness to Violations of Independence: Literature suggests that Naive Bayes can achieve good classification performance even when features are correlated, as long as the dependencies are consistent across classes <span class="citation" data-cites="domingos1997optimality">(<a href="references.html#ref-domingos1997optimality" role="doc-biblioref">Domingos &amp; Pazzani, 1997</a>)</span>. This is because the decision boundaries produced by Naive Bayes are often well-aligned with the true boundaries, despite the imprecise probability estimates.</p></li>
<li><p>Decision Rule Effectiveness: Since Naive Bayes focuses on finding the class that maximizes the posterior probability, it is less sensitive to errors in individual probability estimates, as long as the relative ordering of probabilities remains correct <span class="citation" data-cites="rish2001empirical">(<a href="references.html#ref-rish2001empirical" role="doc-biblioref">Rish, 2001</a>)</span>.</p></li>
<li><p>Zero-One Loss Minimization: Naive Bayes aims to minimize the zero-one loss, i.e., the number of misclassifications. The method benefits from the fact that exact probability estimation is not essential for accurate classification, as the correct class can still be chosen even with approximate probabilities <span class="citation" data-cites="ng2001discriminative">(<a href="references.html#ref-ng2001discriminative" role="doc-biblioref">Ng &amp; Jordan, 2001</a>)</span>.</p></li>
<li><p>High-Dimensional Settings: In high-dimensional settings, the conditional independence assumption can act as a form of implicit regularization, preventing overfitting by simplifying the probability model <span class="citation" data-cites="rish2001empirical">(<a href="references.html#ref-rish2001empirical" role="doc-biblioref">Rish, 2001</a>)</span>. This makes Naive Bayes particularly well-suited for text classification and other sparse feature spaces.</p></li>
</ol>
</section>
<section id="advantages-and-limitations" class="level4" data-number="10.5.1.4">
<h4 data-number="10.5.1.4" class="anchored" data-anchor-id="advantages-and-limitations"><span class="header-section-number">10.5.1.4</span> Advantages and Limitations</h4>
<p>Advantages:</p>
<ul>
<li>Computationally efficient, with linear time complexity in terms of the number of features and data samples.</li>
<li>Performs well on large datasets, especially when features are conditionally independent.</li>
<li>Suitable for high-dimensional data, making it popular in text classification.</li>
</ul>
<p>Limitations:</p>
<ul>
<li>Relies on the assumption of conditional independence, which may not hold in real-world datasets, potentially affecting performance.</li>
<li>It is sensitive to zero probabilities; if a feature value never appears in the training set for a given class, its likelihood becomes zero. To address this, Laplace smoothing (or add-one smoothing) is often applied.</li>
</ul>
</section>
<section id="laplace-smoothing" class="level4" data-number="10.5.1.5">
<h4 data-number="10.5.1.5" class="anchored" data-anchor-id="laplace-smoothing"><span class="header-section-number">10.5.1.5</span> Laplace Smoothing</h4>
<p>Laplace smoothing is used to handle zero probabilities in the likelihood estimation. It adds a small constant $ $ (usually 1) to the count of each feature value, preventing the probability from becoming zero:</p>
<p><span class="math display">\[
P(x_i \mid y) = \frac{\text{count}(x_i, y) + \alpha}
{\sum_{x_i'} (\text{count}(x_i', y) + \alpha)}.
\]</span></p>
<p>This adjustment ensures that even unseen features in the training data do not lead to zero probabilities, thus improving the model’s robustness.</p>
</section>
</section>
<section id="types-of-naive-bayes" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="types-of-naive-bayes"><span class="header-section-number">10.5.2</span> Types of Naive Bayes:</h3>
<p>There are 5 types of Naive Bayes classifiers:</p>
<ul>
<li><p><strong>Gaussian Naive Bayes</strong>: This type of Naive Bayes is used when the dataset consists of numerical features. It assumes that the features follow a Gaussian (normal) distribution. This model is fitted by finding the mean and standard deviation of each class <span class="citation" data-cites="ibm2024naivebayes">(<a href="references.html#ref-ibm2024naivebayes" role="doc-biblioref">IBM, 2024</a>)</span>.</p></li>
<li><p><strong>Categorical Naive Bayes</strong>: When the dataset contains categorical features, we use Categorical Naive Bayes. It assumes that each feature follows a categorical distribution.</p></li>
<li><p><strong>Bernoulli Naive Bayes</strong>: Bernoulli Naive Bayes is applied when the features are binary or follow a Bernoulli distribution. That is, variables with two values, such as True and False or 1 and 0. <span class="citation" data-cites="ibm2024naivebayes">(<a href="references.html#ref-ibm2024naivebayes" role="doc-biblioref">IBM, 2024</a>)</span>.</p></li>
<li><p><strong>Multinomial Naive Bayes</strong>: Multinomial Naive Bayes is commonly used for text classification tasks. It assumes that features represent the frequencies or occurrences of different words in the text.</p></li>
<li><p><strong>Complement Naive Bayes</strong>: Complement Naive Bayes is a variation of Naive Bayes that is designed to address imbalanced datasets. It is particularly useful when the majority class overwhelms the minority class in the dataset. It aims to correct the imbalance by considering the complement of each class when making predictions <span class="citation" data-cites="cnb2023">(<a href="references.html#ref-cnb2023" role="doc-biblioref">GeeksforGeeks, 2023</a>)</span>.</p></li>
</ul>
<p>Each type of Naive Bayes classifier is suitable for different types of datasets based on the nature of the features and their distribution. By selecting the appropriate Naive Bayes algorithm, we can effectively model and classify data based on the given features.</p>
</section>
<section id="naive-bayes-w-nyc-crash-data" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="naive-bayes-w-nyc-crash-data"><span class="header-section-number">10.5.3</span> Naive Bayes w/ NYC Crash Data</h3>
<p>Since we have an imbalanced dataset where there are more non severe crashes than severe, we will use Complement Naive Bayes classifier to predict severe crashes based on our predictors.</p>
<p>Our assumed independent predictors after feature engineering are: <code>borough</code>, <code>location</code>, <code>household_median_income</code>, <code>crash_date, crash_time</code>, <code>time_category</code>, <code>contributing_factor_vehicle_1</code>, <code>vehicle_type_code_1</code> .</p>
<p>We assume a crash is severe if there are more than 0 persons killed and/or injured.</p>
<div id="5f87edbb" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> uszipcode <span class="im">as</span> us</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable warnings</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_feather(<span class="st">'data/nyccrashes_cleaned.feather'</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Separate crash_datetime into date and time (convert datetime into numeric features)</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'crash_datetime'</span>] <span class="op">=</span> pd.to_datetime(df[<span class="st">'crash_datetime'</span>])</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'crash_date'</span>] <span class="op">=</span> df[<span class="st">'crash_datetime'</span>].dt.date</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'crash_time'</span>] <span class="op">=</span> df[<span class="st">'crash_datetime'</span>].dt.time</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract relevant features from datetime (for example: hour)</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'hour'</span>] <span class="op">=</span> df[<span class="st">'crash_datetime'</span>].dt.hour</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Create time_category column with updated time intervals</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> categorize_time(hour):</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="dv">0</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">6</span>:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'midnight'</span>  <span class="co"># 12:00 AM to 5:59 AM</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">6</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">12</span>:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'morning'</span>  <span class="co"># 6:00 AM to 11:59 AM</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">12</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">18</span>:</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'afternoon'</span>  <span class="co"># 12:00 PM to 5:59 PM</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">18</span> <span class="op">&lt;=</span> hour <span class="op">&lt;</span> <span class="dv">21</span>:</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'evening'</span>  <span class="co"># 6:00 PM to 8:59 PM</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">'night'</span>  <span class="co"># 9:00 PM to 11:59 PM</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'time_category'</span>] <span class="op">=</span> df[<span class="st">'hour'</span>].<span class="bu">apply</span>(categorize_time)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Add median household income for each zip code using the uszip package</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_median_income(zipcode):</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> us.search.by_zipcode(<span class="bu">str</span>(zipcode))</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> z:</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> z.median_income</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.nan</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.nan</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'household_median_income'</span>] <span class="op">=</span> df[<span class="st">'zip_code'</span>].<span class="bu">apply</span>(get_median_income)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="defining-predictors-and-target-variable." class="level4" data-number="10.5.3.1">
<h4 data-number="10.5.3.1" class="anchored" data-anchor-id="defining-predictors-and-target-variable."><span class="header-section-number">10.5.3.1</span> Defining predictors and target variable.</h4>
<div id="5fad1024" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> ComplementNB  <span class="co"># Complement Naive Bayes</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Target variable for severe crashes based on number of persons killed and injured</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'severe_crash'</span>] <span class="op">=</span> (df[<span class="st">'number_of_persons_killed'</span>] <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">|</span> (df[<span class="st">'number_of_persons_injured'</span>] <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Selected predictors</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>predictors <span class="op">=</span> [<span class="st">'borough'</span>, <span class="st">'location'</span>, <span class="st">'household_median_income'</span>, </span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">'crash_date'</span>, <span class="st">'crash_time'</span>, <span class="st">'time_category'</span>,</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>              <span class="st">'contributing_factor_vehicle_1'</span>, <span class="st">'vehicle_type_code_1'</span>]</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert categorical columns into dummy variables</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(df[predictors], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle NaN or missing values in numeric columns if necessary</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>X.fillna(<span class="dv">0</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Target variable</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'severe_crash'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="fitting-our-model" class="level4" data-number="10.5.3.2">
<h4 data-number="10.5.3.2" class="anchored" data-anchor-id="fitting-our-model"><span class="header-section-number">10.5.3.2</span> Fitting Our Model</h4>
<p>After accounting for imbalanced data using SMOTE, we find the k nearest neighbors in the minority class to generate synthetic points between the chosen point and its neighbors. In this case, 7 nearest neighbors are considered to generate synthetic samples. We also use alpha 0.5 for our laplace smoothing to apply an equal level of smoothing across all feature probabilities. Finally, we apply a threshold to check if the probability for class 1 (positive class) is greater than 0.4, and if so, assigns the sample to class 1, otherwise to class 0.</p>
<div id="79303718" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">3255</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Resample to balance the classes (SMOTE)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">3255</span>, k_neighbors<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>X_train_res, y_train_res <span class="op">=</span> smote.fit_resample(X_train, y_train)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Complement Naive Bayes model with Laplace smoothing</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ComplementNB(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_res, y_train_res)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the probabilities for each class</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>y_prob <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply threshold to the predicted probabilities</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> (y_prob[:, <span class="dv">1</span>] <span class="op">&gt;</span> <span class="fl">0.4</span>).astype(<span class="bu">int</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model with the new threshold</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>report <span class="op">=</span> classification_report(y_test, y_pred)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:"</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(report)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
[[111  96]
 [ 64 104]]

Classification Report:
              precision    recall  f1-score   support

       False       0.63      0.54      0.58       207
        True       0.52      0.62      0.57       168

    accuracy                           0.57       375
   macro avg       0.58      0.58      0.57       375
weighted avg       0.58      0.57      0.57       375
</code></pre>
</div>
</div>
</section>
<section id="interpreting-our-results" class="level4" data-number="10.5.3.3">
<h4 data-number="10.5.3.3" class="anchored" data-anchor-id="interpreting-our-results"><span class="header-section-number">10.5.3.3</span> Interpreting our Results:</h4>
<p>Recall vs.&nbsp;Precision: Our model is better at identifying severe crashes (higher recall of 0.62) but is less accurate when it does so (lower precision of 0.52). This means that while the model catches a good portion of the severe crashes, it also misidentifies a fair number of non-severe crashes as severe.</p>
<p>F1-Score: The F1-score of 0.57 for severe crashes is a balanced measure, showing that our model is moderately effective at distinguishing severe crashes from non-severe ones.</p>
<p>Improvement Opportunities: The overall accuracy of 58% is moderate, so there’s potential for further optimization.</p>
<p>In conclusion, our CNB model is fairly good at predicting severe crashes with an accuracy of 58% but may benefit from more tuning to improve precision and reduce false positives, especially in the context of an imbalanced dataset where we don’t have many instances of people injured or killed.</p>
</section>
</section>
</section>
<section id="handling-imbalanced-data-with-smote" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="handling-imbalanced-data-with-smote"><span class="header-section-number">10.6</span> Handling Imbalanced Data with SMOTE</h2>
<p>This section is presented by Olivia Kashalapov.</p>
<section id="introduction-1" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">10.6.1</span> Introduction</h3>
<p>This presentation on SMOTE will cover the following topics:</p>
<ul>
<li>What is class imbalance and why is it important?</li>
<li>What is SMOTE?</li>
<li>Why do we use it and how does it work?</li>
<li>Why is SMOTE better than other traditional methods for handling class imbalance?</li>
<li>Using SMOTE in data visualization, analysis, model training, and evaluation.</li>
<li>The downsides and limitations of SMOTE.</li>
</ul>
</section>
<section id="class-imbalance" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="class-imbalance"><span class="header-section-number">10.6.2</span> Class Imbalance</h3>
<p><strong>Before we can learn about SMOTE, we have to understand class imbalance and why it is important.</strong> - Class imbalance is a common issue in many datasets in which the distribution of examples within the dataset are either biased or skewed. - Let’s say there is a dataset for a rare medical diagnosis and there are two classes, with disease and without disease. The data can be taken to explore if there is a certain variable that makes it more likely for an individual to be diagnosed with this rare disease. Since the disease is rare, the class of people with the disease is going to be significantly smaller than the class of those without. In this case, the data will be skewed towards the class of people without the disease and this may skew the findings of the predictive model. - Addressing class imbalance improves the performance of models and increases model accuracy. - Unfortunately, “most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.” – this is where SMOTE comes in!</p>
</section>
<section id="synthetic-model-oversampling-technique" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="synthetic-model-oversampling-technique"><span class="header-section-number">10.6.3</span> Synthetic Model Oversampling Technique</h3>
<ul>
<li>Synthetic Model Oversampling Technique, better known as SMOTE, is an algorithm that focuses on the feature space (already existing data points) in the minority class to generate new data points to create balance between the majority and minority classes.</li>
<li>Here is how it works:
<ol type="1">
<li>It identifies imbalance in the dataset and recognizes the minority class.</li>
<li>Using an existing data point in the minority class, it takes the difference between the point and a nearest neighbor.</li>
<li>It then multiplies the difference by random number between 1 and 0.</li>
<li>This difference is then added to the sample to generate new synthetic example in the featured space (minority class).</li>
<li>This continues with next nearest neighbor up to user-defined number (k), In other words, there are k synthetic points created between the selected existing point and its k nearest neighbors.</li>
<li>This is repeated for all data points within the minority class</li>
</ol></li>
<li>In simpler terms, it creates synthetic data points in the minority class by creating points that lie in between pre-existing ones</li>
<li>SMOTE works well because it attempts to remove bias on skewed distributions and balances the data using pre-existing data points within the dataset. It uses the data already being used to create realistic randomized data points.</li>
</ul>
</section>
<section id="smote-versus-traditional-methods" class="level3" data-number="10.6.4">
<h3 data-number="10.6.4" class="anchored" data-anchor-id="smote-versus-traditional-methods"><span class="header-section-number">10.6.4</span> SMOTE versus Traditional Methods</h3>
<ul>
<li>There are other ways to handle class imbalance within data sets, other than SMOTE</li>
<li>One method of this is random under sampling the majority class where random points are chosen in the majority class to be discarded. This often leaves out too much data which could be important for training the predictive model and there are chances that the remaining sample ends up being biased.</li>
<li>Another option is random oversampling the minority class which is done by randomly duplicating points within the minority class. Although this fixes the class imbalance, it could also lead to overfitting of the model, making it less accurate to the true population.</li>
<li>SMOTE mitigates the problems of random oversampling and under sampling since the generated data points are not replications of already occurring instances and the majority class keeps all of its existing instances. It is much more unlikely for there to be a case of overfitting the model and no useful information will be left out of the model either.</li>
</ul>
</section>
<section id="installation-and-setup" class="level3" data-number="10.6.5">
<h3 data-number="10.6.5" class="anchored" data-anchor-id="installation-and-setup"><span class="header-section-number">10.6.5</span> Installation and Setup</h3>
<p>To install SMOTE, you can type one of two commands into your terminal:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>pip install imbalanced<span class="op">-</span>learn</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># OR</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>conda install imbalanced<span class="op">-</span>learn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To import SMOTE on Python, you use this command:</p>
<div id="b3b8c220" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Just like that, you are ready to use SMOTE!</p>
</section>
<section id="data-preparation-5" class="level3" data-number="10.6.6">
<h3 data-number="10.6.6" class="anchored" data-anchor-id="data-preparation-5"><span class="header-section-number">10.6.6</span> Data Preparation</h3>
<p>Here I am creating a simple data set in which there is an extremely apparent class imbalance. I am doing this rather than using past data sets so that you can truly see the work of SMOTE without other factors that can make the process confusing.</p>
<div id="408390d6" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic dataset</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>data_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>class_0_size <span class="op">=</span> <span class="bu">int</span>(data_size <span class="op">*</span> <span class="fl">0.9</span>)  <span class="co"># 90% for class 0</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>class_1_size <span class="op">=</span> data_size <span class="op">-</span> class_0_size  <span class="co"># 10% for class 1</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Class 0: Majority class</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>feature1_class_0 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, class_0_size)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>feature2_class_0 <span class="op">=</span> np.random.normal(<span class="dv">1</span>, <span class="dv">1</span>, class_0_size)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>target_class_0 <span class="op">=</span> np.zeros(class_0_size)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Class 1: Minority class</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>feature1_class_1 <span class="op">=</span> np.random.normal(<span class="dv">2</span>, <span class="dv">1</span>, class_1_size)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>feature2_class_1 <span class="op">=</span> np.random.normal(<span class="dv">2</span>, <span class="dv">1</span>, class_1_size)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>target_class_1 <span class="op">=</span> np.ones(class_1_size)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the majority and minority class</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>feature1 <span class="op">=</span> np.concatenate([feature1_class_0, feature1_class_1])</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>feature2 <span class="op">=</span> np.concatenate([feature2_class_0, feature2_class_1])</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> np.concatenate([target_class_0, target_class_1])</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature1'</span>: feature1,</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature2'</span>: feature2,</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'target'</span>: target</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.head())</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the dataset as CSV for further use</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>data.to_csv(<span class="st">'synthetic_class_imbalance_dataset.csv'</span>, index<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   feature1  feature2  target
0  0.496714  1.368673     0.0
1 -0.138264  0.606661     0.0
2  0.647689  1.028745     0.0
3  1.523030  2.278452     0.0
4 -0.234153  1.191099     0.0</code></pre>
</div>
</div>
</section>
<section id="data-visualization" class="level3" data-number="10.6.7">
<h3 data-number="10.6.7" class="anchored" data-anchor-id="data-visualization"><span class="header-section-number">10.6.7</span> Data Visualization</h3>
<p>Before using SMOTE to balance the classes, we can view the distribution of the minority and majority classes. It is quite evident that class 0 has many more instances when compared to class 1. This means any predictive models made with this exact data are likely to be skewed towards class 0.</p>
<div id="107aab74" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the class distribution</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'target'</span>].value_counts().plot(kind<span class="op">=</span><span class="st">'bar'</span>, color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'orange'</span>])</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Class Distribution'</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Class'</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="supervised_files/figure-html/cell-17-output-1.png" width="593" height="458" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="implementing-smote" class="level3" data-number="10.6.8">
<h3 data-number="10.6.8" class="anchored" data-anchor-id="implementing-smote"><span class="header-section-number">10.6.8</span> Implementing SMOTE</h3>
<p>Now we can use SMOTE on the dataset I created to handle the imbalance between the classes.</p>
<div id="ed8ca9d1" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into features (X) and target (y)</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'feature1'</span>, <span class="st">'feature2'</span>]]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'target'</span>]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the initial class distribution</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.value_counts())</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize SMOTE with custom parameters</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    sampling_strategy<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Resample minority to 100% of majority class</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    k_neighbors<span class="op">=</span><span class="dv">5</span>,          <span class="co"># Use 5 nearest neighbors to generate synthetic samples</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,        <span class="co"># Set a random state for reproducibility</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply SMOTE to the dataset</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> smote.fit_resample(X, y)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.Series(y_resampled).value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>target
0.0    900
1.0    100
Name: count, dtype: int64
target
0.0    900
1.0    900
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>In this example of SMOTE application, I am utilizing multiple customized parameters. Without these specifications, the SMOTE() command will resample the minority to have the same number of instances as the majority class and utilize the 5 nearest neighbors to generate these samples. <strong>Without a specified random state, SMOTE will choose one so it is recommended to include that paramter for reproducibility.</strong></p>
</section>
<section id="visualization-after-smote" class="level3" data-number="10.6.9">
<h3 data-number="10.6.9" class="anchored" data-anchor-id="visualization-after-smote"><span class="header-section-number">10.6.9</span> Visualization after SMOTE</h3>
<p><em>Keep in mind, the minority class will remain smaller than the majority due to the sampling_strategy parameter included in the previous slide.</em></p>
<div id="c6b78bd7" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the class distribution after SMOTE</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>pd.Series(y_resampled).value_counts().plot(kind<span class="op">=</span><span class="st">'bar'</span>, color<span class="op">=</span>[<span class="st">'blue'</span>, <span class="st">'orange'</span>])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Class Distribution After SMOTE'</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Class'</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="supervised_files/figure-html/cell-19-output-1.png" width="593" height="458" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Much better!</p>
</section>
<section id="model-training-using-smote" class="level3" data-number="10.6.10">
<h3 data-number="10.6.10" class="anchored" data-anchor-id="model-training-using-smote"><span class="header-section-number">10.6.10</span> Model Training using SMOTE</h3>
<p>Now that the dataset is balanced, we can train the machine learning model. In this case, I am using logistic regression, which works well in many binary cases.</p>
<div id="99b1c8ac" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the resampled data into training and testing sets (70% train, 30% test)</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_resampled, y_resampled, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train a Logistic Regression model</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>log_reg.fit(X_train, y_train)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> log_reg.predict(X_test)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the confusion matrix</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:"</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the classification report (includes precision, recall, F1-score)</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:"</span>)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
[[227  40]
 [ 29 244]]

Classification Report:
              precision    recall  f1-score   support

         0.0       0.89      0.85      0.87       267
         1.0       0.86      0.89      0.88       273

    accuracy                           0.87       540
   macro avg       0.87      0.87      0.87       540
weighted avg       0.87      0.87      0.87       540
</code></pre>
</div>
</div>
<p>There were 227 cases in which the model correctly predicted class 1 (true positive) and 244 cases in which the model correctly predicted class 0 (true negative). There were 40 cases in which class 1 was predicted, but it was class 0. Lastly, there were 29 cases in which class 0 was predicted, but it was class 1.</p>
<p><strong>The accuracy of this model is 87%, which means it correctly predicts the class 87% of the time.</strong></p>
</section>
<section id="model-evaluation" class="level3" data-number="10.6.11">
<h3 data-number="10.6.11" class="anchored" data-anchor-id="model-evaluation"><span class="header-section-number">10.6.11</span> Model Evaluation</h3>
<p>So how good is this model actually? Here I am going to use the ROC curve and AUC, since the last slide already touched on accuracy and confusion matrix results.</p>
<div id="c6257492" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, roc_auc_score</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute predicted probabilities for ROC curve</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>y_prob <span class="op">=</span> log_reg.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate ROC curve values</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, y_prob)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC curve</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>plt.plot(fpr, tpr, label<span class="op">=</span><span class="st">'Logistic Regression (AUC = </span><span class="sc">{:.2f}</span><span class="st">)'</span>.<span class="bu">format</span>(roc_auc_score(y_test, y_prob)))</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'Random Guessing'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'False Positive Rate'</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'True Positive Rate (Recall)'</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'ROC Curve'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute AUC</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>auc <span class="op">=</span> roc_auc_score(y_test, y_prob)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AUC:"</span>, auc)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="supervised_files/figure-html/cell-21-output-1.png" width="589" height="449" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>AUC: 0.9407197047646485</code></pre>
</div>
</div>
<p>The area under the curve (AUC) determines how good a model is at distinguishing between the positive and negative classes, with a score of 1 being perfect. In this case, with an AUC of 0.941, the model is extremely good at making these distinguishments.</p>
</section>
<section id="another-example-using-nyc-crash-data" class="level3" data-number="10.6.12">
<h3 data-number="10.6.12" class="anchored" data-anchor-id="another-example-using-nyc-crash-data"><span class="header-section-number">10.6.12</span> Another Example Using NYC Crash Data</h3>
<p>For the NYC Crash Severity Prediction homework we did, SMOTE came in helpful when it came to creating synthetic data in our model predictors. Classes like ‘contributing_factor_vehicle_4’ and ‘vehicle_type_code_5’ were missing a lot of data, making our prediction models very skewed.</p>
<p><strong>Here is how I used SMOTE to fix this issue.</strong></p>
<div id="8f06e8b9" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix, roc_auc_score, f1_score</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify merged data</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>merged_df <span class="op">=</span> pd.read_feather(<span class="st">"data/nyccrashes_merged.feather"</span>)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>merged_df[<span class="st">'Severe Crash'</span>] <span class="op">=</span> ((merged_df[<span class="st">'number_of_persons_killed'</span>] <span class="op">&gt;</span> <span class="dv">0</span>) <span class="op">|</span> </span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>                        (merged_df[<span class="st">'number_of_persons_injured'</span>] <span class="op">&gt;</span> <span class="dv">1</span>)).astype(<span class="bu">int</span>)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Select predictors</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>predictors <span class="op">=</span> [<span class="st">'borough'</span>, <span class="st">'on_street_name'</span>, <span class="st">'cross_street_name'</span>, </span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>              <span class="st">'off_street_name'</span>, <span class="st">'contributing_factor_vehicle_1'</span>,</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>              <span class="st">'contributing_factor_vehicle_2'</span>, </span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>              <span class="st">'contributing_factor_vehicle_3'</span>,</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>              <span class="st">'contributing_factor_vehicle_4'</span>, </span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>              <span class="st">'contributing_factor_vehicle_5'</span>,</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>              <span class="st">'vehicle_type_code_1'</span>, <span class="st">'vehicle_type_code_2'</span>, </span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>              <span class="st">'vehicle_type_code_3'</span>, <span class="st">'vehicle_type_code_4'</span>, </span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>              <span class="st">'vehicle_type_code_5'</span>, </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>              <span class="st">'median_home_value'</span>, <span class="st">'median_household_income'</span>]</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize data</span></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(merged_df[predictors], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> merged_df[<span class="st">'Severe Crash'</span>]</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Impute any missing values</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">'mean'</span>)</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>X_imputed <span class="op">=</span> imputer.fit_transform(X)</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_imputed, </span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>                                                    y, test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>                                                    random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply SMOTE to the training data</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>smote <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">1234</span>)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>X_train_resampled, y_train_resampled <span class="op">=</span> smote.fit_resample(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nyc-crash-data-part-2" class="level3" data-number="10.6.13">
<h3 data-number="10.6.13" class="anchored" data-anchor-id="nyc-crash-data-part-2"><span class="header-section-number">10.6.13</span> NYC Crash Data Part 2</h3>
<p>Now we can continue on to logistic regression modeling and evaluating the accuracy of our predictive model with a balanced dataset!</p>
<div id="5cd16aaa" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    accuracy_score, precision_score, recall_score, confusion_matrix,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    f1_score, roc_curve, auc</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the logistic regression model</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_resampled, y_train_resampled)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict labels on the test set</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predicted probabilities for ROC curve and AUC</span></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability for the positive class</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>y_scores <span class="op">=</span> model.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute confusion matrix</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate accuracy, precision, and recall</span></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_test, y_pred)</span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_test, y_pred)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print confusion matrix and metrics</span></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:</span><span class="ch">\n</span><span class="st">"</span>, cm)</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix:
 [[228  86]
 [ 20  16]]
Accuracy: 0.70
Precision: 0.16
Recall: 0.44</code></pre>
</div>
</div>
<p>This predictive model correctly determined whether a NYC car crash was severe or not 83% of the time, which is decently accurate. Without the use of SMOTE, this number would have been much lower.</p>
</section>
<section id="limitations-and-challenges-of-smote" class="level3" data-number="10.6.14">
<h3 data-number="10.6.14" class="anchored" data-anchor-id="limitations-and-challenges-of-smote"><span class="header-section-number">10.6.14</span> Limitations and Challenges of SMOTE</h3>
<ul>
<li>It does not take into consideration other neighboring classes, which can cause overlap between classes</li>
<li>It is also not the most effective for high-dimensional data (another reason I made a sample dataset rather than using one with many predicitors)</li>
<li>There is no consideration of the quality of the synthetic samples</li>
<li>It is only suitable for continuous variables</li>
<li>Your choice of k can severely impact the quality of the synthetic data</li>
</ul>
</section>
<section id="conclusion-1" class="level3" data-number="10.6.15">
<h3 data-number="10.6.15" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">10.6.15</span> Conclusion</h3>
<ul>
<li>SMOTE is a tool used to handle class imbalance in datasets</li>
<li>It creates synthetic data points utilizing instances already in the minority class</li>
<li>This creates a balanced data set which can improve model prediction and be used for a variety of machine learning applications</li>
</ul>
</section>
<section id="further-readings-1" class="level3" data-number="10.6.16">
<h3 data-number="10.6.16" class="anchored" data-anchor-id="further-readings-1"><span class="header-section-number">10.6.16</span> Further Readings</h3>
<p>Brownlee, J. (2020). <a href="https://www.machinelearningmastery.com/what-is-imbalanced-classification/">A gentle introduction to imbalanced classification</a></p>
<p>Brownlee, J. (2021). <a href="https://www.machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/">Smote for imbalanced classification with python</a></p>
<p>Galli, S. (2023). <a href="https://www.blog.trainindata.com/overcoming-class-imbalance-with-smote/#:~:text=Limitations%20of%20SMOTE&amp;text=No%20consideration%20for%20the%20quality,of%20the%20synthetic%20samples%20generated">Overcoming class imbalance with SMOTE: How to tackle imbalanced datasets in Machine Learning</a></p>
<p>Imbalanced data : <a href="https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/#h-approach-to-handling-imbalanced-data">How to handle imbalanced classification problems. (2023)</a></p>
<p>Maklin, C. (2022). <a href="https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c">Synthetic minority over-sampling technique (smote)</a></p>
<p>Or, D. B. (2024). <a href="https://medium.com/metaor-artificial-intelligence/solving-the-class-imbalance-problem-58cb926b5a0f#:~:text=Class%20imbalance%20is%20a%20common%20problem%20in%20machine%20learning%20that,can%20negatively%20impact%20its%20performance">Solving the class imbalance problem</a></p>
<p>Smote, <a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html">Package Imbalanced Learning Manual</a></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-breiman1984classification" class="csl-entry" role="listitem">
Breiman, L., Friedman, J. H., Olshen, R., &amp; Stone, C. J. (1984). <em>Classification and regression trees</em>. Wadsworth.
</div>
<div id="ref-domingos1997optimality" class="csl-entry" role="listitem">
Domingos, P., &amp; Pazzani, M. (1997). On the optimality of the simple <span>B</span>ayesian classifier under zero-one loss. <em>Machine Learning</em>, <em>29</em>(2-3), 103–130.
</div>
<div id="ref-cnb2023" class="csl-entry" role="listitem">
GeeksforGeeks. (2023). <em>Complement naive bayes (CNB) algorithm</em>. <a href="https://www.geeksforgeeks.org/complement-naive-bayes-cnb-algorithm/">https://www.geeksforgeeks.org/complement-naive-bayes-cnb-algorithm/</a>
</div>
<div id="ref-ibm2024naivebayes" class="csl-entry" role="listitem">
IBM. (2024). <em>What are naïve bayes classifiers?</em> <a href="https://www.ibm.com/topics/naive-bayes">https://www.ibm.com/topics/naive-bayes</a>
</div>
<div id="ref-ng2001discriminative" class="csl-entry" role="listitem">
Ng, A., &amp; Jordan, M. (2001). On discriminative vs. Generative classifiers: <span>A</span> comparison of logistic regression and naive <span>B</span>ayes. <em>Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic</em>, 841–848.
</div>
<div id="ref-rish2001empirical" class="csl-entry" role="listitem">
Rish, I. (2001). An empirical study of the naive <span>B</span>ayes classifier. <em>IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence</em>, <em>3</em>, 41–46.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./machinelearning.html" class="pagination-link" aria-label="Machine Learning: Overview">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Machine Learning: Overview</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./unsupervised.html" class="pagination-link" aria-label="Unsupervised Learning">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>